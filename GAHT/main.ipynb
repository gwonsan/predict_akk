{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1faae78f-4eaa-4045-8ec8-7443b04642f0",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee55e04-97e5-4fe6-99f5-b1f97f181a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import seaborn as sns\n",
    "import imageio\n",
    "\n",
    "from utils.dataset import load_mat_hsi, sample_gt, HSIDataset\n",
    "from utils.utils import split_info_print, metrics, show_results\n",
    "from utils.scheduler import load_scheduler\n",
    "from models.get_model import get_model\n",
    "from train import train, test\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bed360-93a7-471e-b0d7-963d4755855e",
   "metadata": {},
   "source": [
    "## argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74389dbb-520c-4b88-a145-9787d73f992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"run patch-based HSI classification\")\n",
    "parser.add_argument(\"--model\", type=str, default='speformer')\n",
    "parser.add_argument(\"--dataset_name\", type=str, default=\"gs\")\n",
    "parser.add_argument(\"--dataset_file_name\", type=str, default=\"202307_downsampled_gongsan.h5\")\n",
    "parser.add_argument(\"--dataset_dir\", type=str, default=\"/home1/jmt30269/DSNet/data/\")\n",
    "parser.add_argument(\"--device\", type=str, default=\"1\")\n",
    "parser.add_argument(\"--patch_size\", type=int, default=15)\n",
    "parser.add_argument(\"--num_run\", type=int, default=1) \n",
    "parser.add_argument(\"--epoch\", type=int, default=200)    \n",
    "parser.add_argument(\"--bs\", type=int, default=128)  # bs = batch size  \n",
    "parser.add_argument(\"--ratio\", type=float, default=0.06)\n",
    "parser.add_argument(\"--weights\", type=str, default=\"/home1/jmt30269/Group-Aware-Hierarchical-Transformer/checkpoints/speformer/gs/0.05/7_0/\")\n",
    "parser.add_argument(\"--outputs\", type=str, default=\"./results\")\n",
    "\n",
    "\n",
    "opts = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae7a583-0404-4c6e-b6db-6b32adf4d593",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e635daad-22ca-4b93-8629-4a934bea784f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "experiments will run on GPU device 1\n",
      "model = speformer\n",
      "dataset = gs\n",
      "dataset folder = /home1/jmt30269/DSNet/data/\n",
      "patch size = 15\n",
      "batch size = 128\n",
      "total epoch = 200\n",
      "0.03 for training, 0.03 for validation and 0.94 testing\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:{}\".format(opts.device))\n",
    "print(device)\n",
    "# print parameters\n",
    "print(\"experiments will run on GPU device {}\".format(opts.device))\n",
    "print(\"model = {}\".format(opts.model))    \n",
    "print(\"dataset = {}\".format(opts.dataset_name))\n",
    "print(\"dataset folder = {}\".format(opts.dataset_dir))\n",
    "print(\"patch size = {}\".format(opts.patch_size))\n",
    "print(\"batch size = {}\".format(opts.bs))\n",
    "print(\"total epoch = {}\".format(opts.epoch))\n",
    "print(\"{} for training, {} for validation and {} testing\".format(opts.ratio / 2, opts.ratio / 2, 1 - opts.ratio))\n",
    "\n",
    "# load data\n",
    "image, gt, labels = load_mat_hsi(opts.dataset_name, opts.dataset_dir,opts.dataset_file_name)\n",
    "\n",
    "num_classes = len(labels)\n",
    "num_bands = image.shape[-1]\n",
    "model_list=['ssftt','rssan','proposed','speformer']\n",
    "# random seeds\n",
    "seeds = [20250411, 20250402,20250403,20250404,20250405,20250406,20250407,20250408,20250409,20250410]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7c530-41fb-47ee-9a0b-121ddb856756",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6640f95-a52f-49c6-8e91-f4acf66046b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# empty list to storing results\n",
    "results = []\n",
    "model_list=['ssftt','rssan','proposed','speformer']\n",
    "patch_list=[9]\n",
    "for opts.patch_size in patch_list:\n",
    "    for opts.model in model_list:\n",
    "        device = torch.device(\"cuda:{}\".format(opts.device))\n",
    "        print(device)\n",
    "        # print parameters\n",
    "        print(\"experiments will run on GPU device {}\".format(opts.device))\n",
    "        print(\"model = {}\".format(opts.model))    \n",
    "        print(\"dataset = {}\".format(opts.dataset_name))\n",
    "        print(\"dataset folder = {}\".format(opts.dataset_dir))\n",
    "        print(\"patch size = {}\".format(opts.patch_size))\n",
    "        print(\"batch size = {}\".format(opts.bs))\n",
    "        print(\"total epoch = {}\".format(opts.epoch))\n",
    "        print(\"{} for training, {} for validation and {} testing\".format(opts.ratio / 2, opts.ratio / 2, 1 - opts.ratio))\n",
    "        \n",
    "        # load data\n",
    "        image, gt, labels = load_mat_hsi(opts.dataset_name, opts.dataset_dir,opts.dataset_file_name)\n",
    "        \n",
    "        num_classes = len(labels)\n",
    "        num_bands = image.shape[-1]\n",
    "        model_list=['ssftt','rssan','proposed','speformer']\n",
    "        # random seeds\n",
    "        seeds = [20250411, 20250402,20250403,20250404,20250405,20250406,20250407,20250408,20250409,20250410]\n",
    "        run=0\n",
    "        np.random.seed(seeds[0])\n",
    "        print(\"running an experiment with the {} model\".format(opts.model))\n",
    "        print(\"run {} / {}\".format(run+1, opts.num_run))\n",
    "    \n",
    "        # get train_gt, val_gt and test_gt\n",
    "        trainval_gt, test_gt = sample_gt(gt, opts.ratio, seeds[run])\n",
    "        train_gt, val_gt = sample_gt(trainval_gt, 0.5, seeds[run])\n",
    "        del trainval_gt\n",
    "    \n",
    "        train_set = HSIDataset(image, train_gt, patch_size=opts.patch_size, data_aug=True)\n",
    "        val_set = HSIDataset(image, val_gt, patch_size=opts.patch_size, data_aug=False)\n",
    "    \n",
    "        train_loader = torch.utils.data.DataLoader(train_set, opts.bs, drop_last=False, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_set, opts.bs, drop_last=False, shuffle=False)\n",
    "    \n",
    "        # load model and loss\n",
    "        model = get_model(opts.model, opts.dataset_name, opts.patch_size)\n",
    "    \n",
    "        if run == 0:\n",
    "            split_info_print(train_gt, val_gt, test_gt, labels)\n",
    "            # print(\"network information:\")\n",
    "            # with torch.no_grad():\n",
    "            #     summary(model, torch.zeros((1, 1, num_bands, opts.patch_size, opts.patch_size)))\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        optimizer, scheduler = load_scheduler(opts.model, model)\n",
    "    \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "        # where to save checkpoint model\n",
    "        model_dir = \"./checkpoints/\" + opts.model + '/' + opts.dataset_name + '_2407/'+ str(opts.ratio)+'/' + str(opts.patch_size)+'_'+str(run)+\"_1\"\n",
    "        print(f'model save dir : {model_dir}')\n",
    "        try:\n",
    "            train(model, optimizer, criterion, train_loader, val_loader, opts.epoch, model_dir, device, scheduler)\n",
    "        except KeyboardInterrupt:\n",
    "            print('\"ctrl+c\" is pused, the training is over')\n",
    "    \n",
    "        # test the model\n",
    "        probabilities = test(model, model_dir, image, opts.patch_size, num_classes, device)\n",
    "        \n",
    "        prediction = np.argmax(probabilities, axis=-1)\n",
    "    \n",
    "        # computing metrics\n",
    "        run_results = metrics(prediction, test_gt, n_classes=num_classes)  # only for test set\n",
    "        results.append(run_results)\n",
    "        show_results(run_results, label_values=labels)\n",
    "    \n",
    "        del model, train_set, train_loader, val_set, val_loader\n",
    "    \n",
    "    if opts.num_run > 1:\n",
    "        show_results(results, label_values=labels, agregated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63172da7-cc09-459c-9737-492870ddafd3",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d04f801b-9016-4102-be84-463de3ff5cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_results(arr2d, palette):\n",
    "    arr_3d = np.zeros((arr2d.shape[0], arr2d.shape[1], 3), dtype=np.uint8)\n",
    "    for c, i in palette.items():\n",
    "        m = arr2d == c\n",
    "        arr_3d[m] = i\n",
    "    return arr_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81553b-7a41-4d27-90f9-16b738d4ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list=['ssftt','rssan','proposed','speformer']\n",
    "for opts.model in model_list:\n",
    "    palette = {0: (0, 0, 0)}\n",
    "    for k, color in enumerate(sns.color_palette(\"hls\", num_classes + 1)):\n",
    "        palette[k + 1] = tuple(np.asarray(255 * np.array(color), dtype='uint8'))\n",
    "    opts.weights= f'/home1/jmt30269/Group-Aware-Hierarchical-Transformer/checkpoints/{opts.model}/gs_2407/0.03/{opts.patch_size}_0_1/'\n",
    "# load model and weights\n",
    "    model = get_model(opts.model, opts.dataset_name, opts.patch_size)\n",
    "    print('loading weights from %s' % opts.weights + '/model_best.pth')\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(opts.weights, 'model_best.pth')))\n",
    "    model.eval()\n",
    "    \n",
    "    # testing model: metric for the whole HSI, including train, val, and test\n",
    "    probabilities = test(model, opts.weights, image, opts.patch_size, num_classes, device=device)\n",
    "    prediction = np.argmax(probabilities, axis=-1)\n",
    "    \n",
    "    run_results = metrics(prediction, gt, n_classes=num_classes)\n",
    "    \n",
    "    prediction[gt < 0] = -1\n",
    "    \n",
    "    # color results\n",
    "    colored_gt = color_results(gt+1, palette)\n",
    "    colored_pred = color_results(prediction+1, palette)\n",
    "    \n",
    "    outfile = os.path.join(opts.outputs, opts.dataset_name,  opts.model)\n",
    "    os.makedirs(outfile, exist_ok=True)\n",
    "    \n",
    "    # imageio.imsave(os.path.join(outfile, opts.dataset_name + '_gt.png'), colored_gt)  # eps or png\n",
    "    # imageio.imsave(os.path.join(outfile, opts.dataset_name+'_' + opts.model + '_out.png'), colored_pred)  # or png\n",
    "    \n",
    "    prod = probabilities.reshape(-1,2)\n",
    "    gtt= gt.reshape(-1)\n",
    "    predd = prediction.reshape(-1)\n",
    "    gtt=gtt +1\n",
    "    predd=predd +1\n",
    "    df = pd.DataFrame({\n",
    "        'gt': gtt,\n",
    "        'prob_akk': prod[:, 0],  # 첫 번째 열\n",
    "        'prob_back': prod[:, 1],  # 두 번째 열\n",
    "        'pred': predd\n",
    "    })\n",
    "    df.to_csv(os.path.join(outfile, opts.dataset_name+'_2307_'+str(opts.patch_size)+'_' + opts.model+\"_result.csv\"),index=False)\n",
    "    \n",
    "    show_results(run_results, label_values=labels)\n",
    "    del model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e0ba6-58ea-4d85-8e5a-646d4b57288a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## train size test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777c269-ef29-4983-a0f0-a2c36cb6c9ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:{}\".format(opts.device))\n",
    "print(\"model = {}\".format(opts.model))    \n",
    "print(\"dataset = {}\".format(opts.dataset_name))\n",
    "print(\"dataset folder = {}\".format(opts.dataset_dir))\n",
    "print(\"patch size = {}\".format(opts.patch_size))\n",
    "print(\"batch size = {}\".format(opts.bs))\n",
    "print(\"total epoch = {}\".format(opts.epoch))\n",
    "opts.epoch=100\n",
    "for i in np.arange(0.05, 0.2, 0.02):\n",
    "    opts.ratio=i\n",
    "    print(\"{} for training, {} for validation and {} testing\".format(opts.ratio / 2, opts.ratio / 2, 1 - opts.ratio))\n",
    "\n",
    "    # load data\n",
    "    image, gt, labels = load_mat_hsi(opts.dataset_name, opts.dataset_dir,opts.dataset_file_name)\n",
    "    \n",
    "    num_classes = len(labels)\n",
    "    num_bands = image.shape[-1]\n",
    "    \n",
    "    # random seeds\n",
    "    seeds = [202201, 202202, 202203, 202204, 202205]\n",
    "    \n",
    "    # empty list to storing results\n",
    "    results = []\n",
    "    \n",
    "    for run in range(opts.num_run):\n",
    "        np.random.seed(seeds[run])\n",
    "        print(\"running an experiment with the {} model\".format(opts.model))\n",
    "        print(\"run {} / {}\".format(run+1, opts.num_run))\n",
    "    \n",
    "        # get train_gt, val_gt and test_gt\n",
    "        trainval_gt, test_gt = sample_gt(gt, opts.ratio, seeds[run])\n",
    "        train_gt, val_gt = sample_gt(trainval_gt, 0.5, seeds[run])\n",
    "        del trainval_gt\n",
    "    \n",
    "        train_set = HSIDataset(image, train_gt, patch_size=opts.patch_size, data_aug=True)\n",
    "        val_set = HSIDataset(image, val_gt, patch_size=opts.patch_size, data_aug=False)\n",
    "    \n",
    "        train_loader = torch.utils.data.DataLoader(train_set, opts.bs, drop_last=False, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_set, opts.bs, drop_last=False, shuffle=False)\n",
    "    \n",
    "        # load model and loss\n",
    "        model = get_model(opts.model, opts.dataset_name, opts.patch_size)\n",
    "    \n",
    "        if run == 0:\n",
    "            split_info_print(train_gt, val_gt, test_gt, labels)\n",
    "            # print(\"network information:\")\n",
    "            # with torch.no_grad():\n",
    "            #     summary(model, torch.zeros((1, 1, num_bands, opts.patch_size, opts.patch_size)))\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        optimizer, scheduler = load_scheduler(opts.model, model)\n",
    "    \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "        # where to save checkpoint model\n",
    "        model_dir = \"./checkpoints/\" + opts.model + '/' + opts.dataset_name + '/'+ str(opts.ratio)+'/' + str(opts.patch_size)+'_'+str(run)\n",
    "        print(f'model save dir : {model_dir}')\n",
    "        try:\n",
    "            train(model, optimizer, criterion, train_loader, val_loader, opts.epoch, model_dir, device, scheduler)\n",
    "        except KeyboardInterrupt:\n",
    "            print('\"ctrl+c\" is pused, the training is over')\n",
    "    \n",
    "        # test the model\n",
    "        probabilities = test(model, model_dir, image, opts.patch_size, num_classes, device)\n",
    "        \n",
    "        prediction = np.argmax(probabilities, axis=-1)\n",
    "    \n",
    "        # computing metrics\n",
    "        run_results = metrics(prediction, test_gt, n_classes=num_classes)  # only for test set\n",
    "        results.append(run_results)\n",
    "        show_results(run_results, label_values=labels)\n",
    "    \n",
    "        del model, train_set, train_loader, val_set, val_loader\n",
    "    \n",
    "    if opts.num_run > 1:\n",
    "        show_results(results, label_values=labels, agregated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a46e937-b4b8-4b64-a1f3-cb3a33519e84",
   "metadata": {},
   "source": [
    "## test per size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57cd972d-ef52-4e6c-a19a-6fc89e27c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_results(arr2d, palette):\n",
    "    arr_3d = np.zeros((arr2d.shape[0], arr2d.shape[1], 3), dtype=np.uint8)\n",
    "    for c, i in palette.items():\n",
    "        m = arr2d == c\n",
    "        arr_3d[m] = i\n",
    "    return arr_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48352ce9-934d-4070-a780-0f28a59294c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7_0', '7_1', '7_2', '7_3', '7_4', '7_5', '7_6', '7_7', '7_8', '7_9']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "base_path='/home1/jmt30269/Group-Aware-Hierarchical-Transformer/checkpoints/rssan/bs/0.06/'\n",
    "list_dir=os.listdir(base_path)\n",
    "list_dir.sort()\n",
    "print(list_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a13a86-01cb-436b-bb41-5bf610bb8559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ratio in list_dir:\n",
    "    opts.weights=base_path+str(ratio)\n",
    "    print(opts.weights)\n",
    "    # load data\n",
    "    image, gt, labels = load_mat_hsi(opts.dataset_name, opts.dataset_dir,opts.dataset_file_name)\n",
    "    \n",
    "    num_classes = len(labels)\n",
    "    num_bands = image.shape[-1]\n",
    "\n",
    "    # empty list to storing results\n",
    "    results = []\n",
    "    \n",
    "    palette = {0: (0, 0, 0)}\n",
    "    for k, color in enumerate(sns.color_palette(\"hls\", num_classes + 1)):\n",
    "        palette[k + 1] = tuple(np.asarray(255 * np.array(color), dtype='uint8'))\n",
    "    \n",
    "    # load model and weights\n",
    "    model = get_model(opts.model, opts.dataset_name, opts.patch_size)\n",
    "    print('loading weights from %s' % opts.weights + '/model_best.pth')\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(opts.weights, 'model_best.pth')))\n",
    "    model.eval()\n",
    "    \n",
    "    # testing model: metric for the whole HSI, including train, val, and test\n",
    "    probabilities = test(model, opts.weights, image, opts.patch_size, num_classes, device=device)\n",
    "    prediction = np.argmax(probabilities, axis=-1)\n",
    "    \n",
    "    run_results = metrics(prediction, gt, n_classes=num_classes)\n",
    "    \n",
    "    prediction[gt < 0] = -1\n",
    "    \n",
    "    # color results\n",
    "    colored_gt = color_results(gt+1, palette)\n",
    "    colored_pred = color_results(prediction+1, palette)\n",
    "    \n",
    "    outfile = os.path.join(opts.outputs, opts.dataset_name,  opts.model)\n",
    "    os.makedirs(outfile, exist_ok=True)\n",
    "    \n",
    "    # imageio.imsave(os.path.join(outfile, opts.dataset_name + '_gt.png'), colored_gt)  # eps or png\n",
    "    imageio.imsave(os.path.join(outfile, opts.dataset_name+'_' + opts.model+\"_\"+str(round(float(ratio),2))+ '_out.png'), colored_pred)  # or png\n",
    "    \n",
    "    prod = probabilities.reshape(-1,2)\n",
    "    gtt= gt.reshape(-1)\n",
    "    predd = prediction.reshape(-1)\n",
    "    gtt=gtt +1\n",
    "    predd=predd +1\n",
    "    df = pd.DataFrame({\n",
    "        'gt': gtt,\n",
    "        'prob_akk': prod[:, 0],  # 첫 번째 열\n",
    "        'prob_back': prod[:, 1],  # 두 번째 열\n",
    "        'pred': predd\n",
    "    })\n",
    "    df.to_csv(os.path.join(outfile, opts.dataset_name+'_' + opts.model+\"_\"+str(ratio)+\"_result.csv\"),index=False)\n",
    "    \n",
    "    show_results(run_results, label_values=labels)\n",
    "    del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c887e-b8df-4c5c-9e06-65b8464e94d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
