{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f248973-324f-4549-a4f4-ab5c163604c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import pandas as pd\n",
    "from scipy.io import savemat\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "from torch import optim\n",
    "from model import DSNet\n",
    "from utils import AvgrageMeter, accuracy, output_metric, NonZeroClipper, print_args\n",
    "from dataset import prepare_dataset\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd39823a-d990-403c-84f3-fdb14094391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"HSI\")\n",
    "parser.add_argument('--fix_random', action='store_true', default=True, help='fix randomness')\n",
    "parser.add_argument('--gpu_id', default='0', help='gpu id')\n",
    "parser.add_argument('--seed', type=int, default=0, help='number of seed')\n",
    "parser.add_argument('--dataset', choices=['Indian', 'Berlin', 'Augsburg','GS','BS'], default='BS', help='dataset to use')\n",
    "parser.add_argument('--flag_test', choices=['test', 'train'], default='train', help='testing mark')\n",
    "parser.add_argument('--model_name', choices=['conv2d_unmix'], default='conv2d_unmix', help='DSNet')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='number of batch size')\n",
    "parser.add_argument('--test_freq', type=int, default=10, help='number of evaluation')\n",
    "parser.add_argument('--patches', type=int, default=5, help='number of patches')\n",
    "parser.add_argument('--epoches', type=int, default=100, help='epoch number')\n",
    "parser.add_argument('--inference_model', type=str, default='./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl', help='inference model')\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-4, help='learning rate')\n",
    "parser.add_argument('--gamma', type=float, default=0.9, help='gamma')\n",
    "parser.add_argument('--weight_decay', type=float, default=0, help='weight_decay')\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ef8e2b9-25ad-4d9c-91ac-7ada288c2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer,lambda_value=0.5):\n",
    "    model.train()\n",
    "    objs = AvgrageMeter()\n",
    "    top1 = AvgrageMeter()\n",
    "    tar = np.array([])\n",
    "    pre = np.array([])\n",
    "\n",
    "    eps = 1e-7\n",
    "    # add tqdm for progress bar\n",
    "    for batch_idx, (batch_data, batch_target) in enumerate(train_loader):\n",
    "        # Ensure batch_data is a Tensor\n",
    "        if isinstance(batch_data, np.ndarray):\n",
    "            batch_data = torch.from_numpy(batch_data)\n",
    "\n",
    "        batch_data = batch_data.cuda()\n",
    "        batch_target = batch_target.cuda()\n",
    "        batch_target = batch_target.long().cuda()\n",
    "\n",
    "        # 입력 차원을 조정합니다. (batch_size, channels, sequence_length)\n",
    "        if batch_data.dim() == 2:\n",
    "            batch_data = batch_data.unsqueeze(-1)  # (batch_size, channels) -> (batch_size, channels, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if 'unmix' in args.model_name:\n",
    "            re_unmix_nonlinear, re_unmix, batch_pred = model(batch_data)\n",
    "\n",
    "            band = re_unmix.shape[1]//2  # 2 represents the number of layer\n",
    "            output_linear = re_unmix[:,0:band] + re_unmix[:,band:band*2]\n",
    "            re_unmix = re_unmix_nonlinear + output_linear\n",
    "\n",
    "            cosine_similarity = torch.sum(batch_data * re_unmix, dim=1) / (\n",
    "                torch.norm(re_unmix, dim=1, p=2) * torch.norm(batch_data, dim=1, p=2) + eps\n",
    "            )\n",
    "            # 범위 클램핑\n",
    "            cosine_similarity = torch.clamp(cosine_similarity, -0.999, 0.999)\n",
    "\n",
    "            sad_loss = torch.mean(torch.acos(cosine_similarity))\n",
    "            loss = lambda_value * sad_loss + (1 - lambda_value) * criterion(batch_pred, batch_target)\n",
    "            \n",
    "        else:\n",
    "            batch_pred = model(batch_data)\n",
    "            loss = criterion(batch_pred, batch_target)\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"NaN or Inf detected in loss at batch {batch_idx}. Skipping this batch.\")\n",
    "            continue\n",
    "                \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        prec1, t, p = accuracy(batch_pred, batch_target, topk=(1,))\n",
    "        n = batch_data.shape[0]\n",
    "        objs.update(loss.item(), n)  # loss.item()으로 접근\n",
    "        top1.update(prec1[0].item(), n)  # prec1[0].item()으로 접근\n",
    "\n",
    "        tar = np.append(tar, t.data.cpu().numpy())\n",
    "        pre = np.append(pre, p.data.cpu().numpy())\n",
    "    return top1.avg, objs.avg, tar, pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "689bb484-f7e0-4003-b307-d884fd413761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, valid_loader, criterion, optimizer):\n",
    "    objs = AvgrageMeter()\n",
    "    top1 = AvgrageMeter()\n",
    "    tar = np.array([])\n",
    "    pre = np.array([])\n",
    "    for batch_idx, (batch_data, batch_target) in enumerate(valid_loader):\n",
    "        batch_data = batch_data.cuda()\n",
    "        batch_target = batch_target.cuda()\n",
    "\n",
    "        if 'unmix' in args.model_name:\n",
    "            re_unmix_nonlinear, re_unmix, batch_pred = model(batch_data)\n",
    "\n",
    "            band = re_unmix.shape[1]//2\n",
    "            output_linear = re_unmix[:,0:band] + re_unmix[:,band:band*2]\n",
    "            re_unmix = re_unmix_nonlinear + output_linear\n",
    "\n",
    "            sad_loss = torch.mean(torch.acos(torch.sum(batch_data * re_unmix, dim=1)/\n",
    "                        (torch.norm(re_unmix, dim=1, p=2) * torch.norm(batch_data, dim=1, p=2))))\n",
    "            loss = criterion(batch_pred, batch_target) + sad_loss\n",
    "        else:\n",
    "            batch_pred = model(batch_data)\n",
    "            loss = criterion(batch_pred, batch_target)\n",
    "\n",
    "        prec1, t, p = accuracy(batch_pred, batch_target, topk=(1,))\n",
    "        n = batch_data.shape[0]\n",
    "        objs.update(loss.data, n)\n",
    "        top1.update(prec1[0].data, n)\n",
    "        tar = np.append(tar, t.data.cpu().numpy())\n",
    "        pre = np.append(pre, p.data.cpu().numpy())\n",
    "    return tar, pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66f1b888-9f4c-4a4a-b202-34d1d8d120fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, test_loader):\n",
    "    pre = np.array([])\n",
    "    for batch_idx, (batch_data, batch_target) in enumerate(test_loader):\n",
    "        batch_data = batch_data.cuda()\n",
    "        batch_target = batch_target.cuda()\n",
    "\n",
    "        if 'unmix' in args.model_name:\n",
    "            re_unmix_nonlinear, re_unmix, batch_pred = model(batch_data)\n",
    "        else:\n",
    "            batch_pred = model(batch_data)\n",
    "\n",
    "        _, pred = batch_pred.topk(1, 1, True, True)\n",
    "        pp = pred.squeeze()\n",
    "        pre = np.append(pre, pp.data.cpu().numpy())\n",
    "\n",
    "    return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "379c175f-41f2-4dd1-ad1d-f415ec99c251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data change to the ratio of train samples: 0.03\n",
      "height=2715,width=1843,band=150\n",
      "**************************************************\n",
      "patch is : 5\n",
      "mirror_image shape : [2719,1847,150]\n",
      "**************************************************\n",
      "x_train shape = (116561, 5, 5, 150), type = float32\n",
      "x_test  shape = (3768786, 5, 5, 150), type = float32\n",
      "x_true  shape = (5003745, 5, 5, 150), type = float32\n",
      "**************************************************\n",
      "y_train: shape = (116561,) ,type = int64\n",
      "y_test: shape = (3768786,) ,type = int64\n",
      "y_true: shape = (5003745,) ,type = int64\n",
      "**************************************************\n",
      "Model Name: conv2d_unmix\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akk: 5764, gt: 110797\n",
      "Epoch: 001 train_loss: 1.0209 train_acc: 94.5711 Precision: 0.0330 Recall: 0.2013 F1-Score: 0.0566\n",
      "akk: 49244, gt: 3719542\n",
      "OA: 0.9845 AA: 0.6685 Kappa: 0.2583 Precision: 0.2152 Recall: 0.3473 F1-Score: 0.2658\n",
      "*************************\n",
      "akk: 3274, gt: 113287\n",
      "Epoch: 002 train_loss: 0.1851 train_acc: 96.7245 Precision: 0.0611 Recall: 0.2119 F1-Score: 0.0948\n",
      "akk: 1174, gt: 115387\n",
      "Epoch: 003 train_loss: 0.0732 train_acc: 98.4386 Precision: 0.1269 Recall: 0.1578 F1-Score: 0.1407\n",
      "akk: 471, gt: 116090\n",
      "Epoch: 004 train_loss: 0.0512 train_acc: 99.1618 Precision: 0.4650 Recall: 0.2320 F1-Score: 0.3095\n",
      "akk: 473, gt: 116088\n",
      "Epoch: 005 train_loss: 0.0530 train_acc: 99.1944 Precision: 0.5053 Recall: 0.2532 F1-Score: 0.3373\n",
      "akk: 556, gt: 116005\n",
      "Epoch: 006 train_loss: 0.0421 train_acc: 99.3137 Precision: 0.6295 Recall: 0.3708 F1-Score: 0.4667\n",
      "akk: 556, gt: 116005\n",
      "Epoch: 007 train_loss: 0.0410 train_acc: 99.3274 Precision: 0.6439 Recall: 0.3792 F1-Score: 0.4773\n",
      "akk: 582, gt: 115979\n",
      "Epoch: 008 train_loss: 0.0394 train_acc: 99.3548 Precision: 0.6649 Recall: 0.4100 F1-Score: 0.5072\n",
      "akk: 580, gt: 115981\n",
      "Epoch: 009 train_loss: 0.0392 train_acc: 99.3823 Precision: 0.6931 Recall: 0.4258 F1-Score: 0.5276\n",
      "akk: 578, gt: 115983\n",
      "Epoch: 010 train_loss: 0.0387 train_acc: 99.4338 Precision: 0.7457 Recall: 0.4566 F1-Score: 0.5664\n",
      "akk: 608, gt: 115953\n",
      "Epoch: 011 train_loss: 0.0382 train_acc: 99.4098 Precision: 0.7105 Recall: 0.4576 F1-Score: 0.5567\n",
      "akk: 22324, gt: 3746462\n",
      "OA: 0.9949 AA: 0.7744 Kappa: 0.6330 Precision: 0.7521 Recall: 0.5502 F1-Score: 0.6355\n",
      "*************************\n",
      "akk: 589, gt: 115972\n",
      "Epoch: 012 train_loss: 0.0419 train_acc: 99.4106 Precision: 0.7182 Recall: 0.4481 F1-Score: 0.5519\n",
      "akk: 641, gt: 115920\n",
      "Epoch: 013 train_loss: 0.0375 train_acc: 99.4432 Precision: 0.7301 Recall: 0.4958 F1-Score: 0.5905\n",
      "akk: 651, gt: 115910\n",
      "Epoch: 014 train_loss: 0.0371 train_acc: 99.4587 Precision: 0.7404 Recall: 0.5106 F1-Score: 0.6044\n",
      "akk: 616, gt: 115945\n",
      "Epoch: 015 train_loss: 0.0370 train_acc: 99.4578 Precision: 0.7532 Recall: 0.4915 F1-Score: 0.5949\n",
      "akk: 677, gt: 115884\n",
      "Epoch: 016 train_loss: 0.0367 train_acc: 99.4707 Precision: 0.7415 Recall: 0.5318 F1-Score: 0.6194\n",
      "akk: 674, gt: 115887\n",
      "Epoch: 017 train_loss: 0.0374 train_acc: 99.4595 Precision: 0.7329 Recall: 0.5233 F1-Score: 0.6106\n",
      "akk: 659, gt: 115902\n",
      "Epoch: 018 train_loss: 0.0368 train_acc: 99.4638 Precision: 0.7420 Recall: 0.5180 F1-Score: 0.6101\n",
      "akk: 685, gt: 115876\n",
      "Epoch: 019 train_loss: 0.0363 train_acc: 99.4947 Precision: 0.7591 Recall: 0.5508 F1-Score: 0.6384\n",
      "akk: 670, gt: 115891\n",
      "Epoch: 020 train_loss: 0.0358 train_acc: 99.5007 Precision: 0.7701 Recall: 0.5466 F1-Score: 0.6394\n",
      "akk: 669, gt: 115892\n",
      "Epoch: 021 train_loss: 0.0360 train_acc: 99.4964 Precision: 0.7668 Recall: 0.5434 F1-Score: 0.6361\n",
      "akk: 18566, gt: 3750220\n",
      "OA: 0.9951 AA: 0.7499 Kappa: 0.6202 Precision: 0.8228 Recall: 0.5006 F1-Score: 0.6225\n",
      "*************************\n",
      "akk: 690, gt: 115871\n",
      "Epoch: 022 train_loss: 0.0358 train_acc: 99.5093 Precision: 0.7696 Recall: 0.5625 F1-Score: 0.6499\n",
      "akk: 699, gt: 115862\n",
      "Epoch: 023 train_loss: 0.0371 train_acc: 99.4964 Precision: 0.7554 Recall: 0.5593 F1-Score: 0.6427\n",
      "akk: 713, gt: 115848\n",
      "Epoch: 024 train_loss: 0.0360 train_acc: 99.4913 Precision: 0.7461 Recall: 0.5636 F1-Score: 0.6421\n",
      "akk: 704, gt: 115857\n",
      "Epoch: 025 train_loss: 0.0361 train_acc: 99.5024 Precision: 0.7585 Recall: 0.5657 F1-Score: 0.6481\n",
      "akk: 715, gt: 115846\n",
      "Epoch: 026 train_loss: 0.0354 train_acc: 99.5136 Precision: 0.7636 Recall: 0.5784 F1-Score: 0.6582\n",
      "akk: 694, gt: 115867\n",
      "Epoch: 027 train_loss: 0.0357 train_acc: 99.5230 Precision: 0.7795 Recall: 0.5731 F1-Score: 0.6606\n",
      "akk: 711, gt: 115850\n",
      "Epoch: 028 train_loss: 0.0354 train_acc: 99.5050 Precision: 0.7581 Recall: 0.5710 F1-Score: 0.6514\n",
      "akk: 707, gt: 115854\n",
      "Epoch: 029 train_loss: 0.0352 train_acc: 99.5153 Precision: 0.7680 Recall: 0.5752 F1-Score: 0.6578\n",
      "akk: 695, gt: 115866\n",
      "Epoch: 030 train_loss: 0.0358 train_acc: 99.5290 Precision: 0.7842 Recall: 0.5773 F1-Score: 0.6650\n",
      "akk: 721, gt: 115840\n",
      "Epoch: 031 train_loss: 0.0355 train_acc: 99.5256 Precision: 0.7712 Recall: 0.5890 F1-Score: 0.6679\n",
      "akk: 12275, gt: 3756511\n",
      "OA: 0.9944 AA: 0.6771 Kappa: 0.5035 Precision: 0.8816 Recall: 0.3546 F1-Score: 0.5058\n",
      "*************************\n",
      "akk: 709, gt: 115852\n",
      "Epoch: 032 train_loss: 0.0353 train_acc: 99.5153 Precision: 0.7673 Recall: 0.5763 F1-Score: 0.6582\n",
      "akk: 718, gt: 115843\n",
      "Epoch: 033 train_loss: 0.0358 train_acc: 99.5367 Precision: 0.7813 Recall: 0.5943 F1-Score: 0.6751\n",
      "akk: 730, gt: 115831\n",
      "Epoch: 034 train_loss: 0.0350 train_acc: 99.5350 Precision: 0.7753 Recall: 0.5996 F1-Score: 0.6762\n",
      "akk: 725, gt: 115836\n",
      "Epoch: 035 train_loss: 0.0353 train_acc: 99.5324 Precision: 0.7752 Recall: 0.5953 F1-Score: 0.6735\n",
      "akk: 739, gt: 115822\n",
      "Epoch: 036 train_loss: 0.0350 train_acc: 99.5376 Precision: 0.7740 Recall: 0.6059 F1-Score: 0.6797\n",
      "akk: 734, gt: 115827\n",
      "Epoch: 037 train_loss: 0.0354 train_acc: 99.5230 Precision: 0.7643 Recall: 0.5943 F1-Score: 0.6687\n",
      "akk: 724, gt: 115837\n",
      "Epoch: 038 train_loss: 0.0349 train_acc: 99.5419 Precision: 0.7831 Recall: 0.6006 F1-Score: 0.6799\n",
      "akk: 717, gt: 115844\n",
      "Epoch: 039 train_loss: 0.0352 train_acc: 99.5273 Precision: 0.7741 Recall: 0.5879 F1-Score: 0.6683\n",
      "akk: 750, gt: 115811\n",
      "Epoch: 040 train_loss: 0.0343 train_acc: 99.5367 Precision: 0.7693 Recall: 0.6112 F1-Score: 0.6812\n",
      "akk: 736, gt: 115825\n",
      "Epoch: 041 train_loss: 0.0343 train_acc: 99.5436 Precision: 0.7799 Recall: 0.6081 F1-Score: 0.6833\n",
      "akk: 33638, gt: 3735148\n",
      "OA: 0.9947 AA: 0.8592 Kappa: 0.6837 Precision: 0.6545 Recall: 0.7215 F1-Score: 0.6864\n",
      "*************************\n",
      "akk: 747, gt: 115814\n",
      "Epoch: 042 train_loss: 0.0347 train_acc: 99.5393 Precision: 0.7724 Recall: 0.6112 F1-Score: 0.6824\n",
      "akk: 741, gt: 115820\n",
      "Epoch: 043 train_loss: 0.0350 train_acc: 99.5565 Precision: 0.7881 Recall: 0.6186 F1-Score: 0.6932\n",
      "akk: 746, gt: 115815\n",
      "Epoch: 044 train_loss: 0.0344 train_acc: 99.5642 Precision: 0.7922 Recall: 0.6261 F1-Score: 0.6994\n",
      "akk: 718, gt: 115843\n",
      "Epoch: 045 train_loss: 0.0348 train_acc: 99.5470 Precision: 0.7897 Recall: 0.6006 F1-Score: 0.6823\n",
      "akk: 740, gt: 115821\n",
      "Epoch: 046 train_loss: 0.0346 train_acc: 99.5419 Precision: 0.7770 Recall: 0.6091 F1-Score: 0.6829\n",
      "akk: 725, gt: 115836\n",
      "Epoch: 047 train_loss: 0.0342 train_acc: 99.5462 Precision: 0.7862 Recall: 0.6038 F1-Score: 0.6830\n",
      "akk: 732, gt: 115829\n",
      "Epoch: 048 train_loss: 0.0344 train_acc: 99.5693 Precision: 0.8019 Recall: 0.6218 F1-Score: 0.7005\n",
      "akk: 751, gt: 115810\n",
      "Epoch: 049 train_loss: 0.0342 train_acc: 99.5668 Precision: 0.7923 Recall: 0.6303 F1-Score: 0.7021\n",
      "akk: 734, gt: 115827\n",
      "Epoch: 050 train_loss: 0.0342 train_acc: 99.5693 Precision: 0.8011 Recall: 0.6229 F1-Score: 0.7008\n",
      "akk: 762, gt: 115799\n",
      "Epoch: 051 train_loss: 0.0341 train_acc: 99.5728 Precision: 0.7927 Recall: 0.6398 F1-Score: 0.7081\n",
      "akk: 19388, gt: 3749398\n",
      "OA: 0.9954 AA: 0.7674 Kappa: 0.6528 Precision: 0.8429 Recall: 0.5355 F1-Score: 0.6550\n",
      "*************************\n",
      "akk: 740, gt: 115821\n",
      "Epoch: 052 train_loss: 0.0338 train_acc: 99.5522 Precision: 0.7851 Recall: 0.6155 F1-Score: 0.6900\n",
      "akk: 752, gt: 115809\n",
      "Epoch: 053 train_loss: 0.0337 train_acc: 99.5693 Precision: 0.7939 Recall: 0.6324 F1-Score: 0.7040\n",
      "akk: 738, gt: 115823\n",
      "Epoch: 054 train_loss: 0.0340 train_acc: 99.5590 Precision: 0.7913 Recall: 0.6186 F1-Score: 0.6944\n",
      "akk: 745, gt: 115816\n",
      "Epoch: 055 train_loss: 0.0347 train_acc: 99.5770 Precision: 0.8027 Recall: 0.6335 F1-Score: 0.7081\n",
      "akk: 738, gt: 115823\n",
      "Epoch: 056 train_loss: 0.0338 train_acc: 99.5556 Precision: 0.7886 Recall: 0.6165 F1-Score: 0.6920\n",
      "akk: 732, gt: 115829\n",
      "Epoch: 057 train_loss: 0.0341 train_acc: 99.5676 Precision: 0.8005 Recall: 0.6208 F1-Score: 0.6993\n",
      "akk: 760, gt: 115801\n",
      "Epoch: 058 train_loss: 0.0335 train_acc: 99.5796 Precision: 0.7987 Recall: 0.6430 F1-Score: 0.7124\n",
      "akk: 737, gt: 115824\n",
      "Epoch: 059 train_loss: 0.0335 train_acc: 99.5736 Precision: 0.8033 Recall: 0.6271 F1-Score: 0.7043\n",
      "akk: 759, gt: 115802\n",
      "Epoch: 060 train_loss: 0.0343 train_acc: 99.5788 Precision: 0.7984 Recall: 0.6419 F1-Score: 0.7117\n",
      "akk: 740, gt: 115821\n",
      "Epoch: 061 train_loss: 0.0336 train_acc: 99.5710 Precision: 0.8000 Recall: 0.6271 F1-Score: 0.7031\n",
      "akk: 29138, gt: 3739648\n",
      "OA: 0.9954 AA: 0.8442 Kappa: 0.7042 Precision: 0.7232 Recall: 0.6906 F1-Score: 0.7065\n",
      "*************************\n",
      "akk: 767, gt: 115794\n",
      "Epoch: 062 train_loss: 0.0332 train_acc: 99.5788 Precision: 0.7953 Recall: 0.6462 F1-Score: 0.7130\n",
      "akk: 751, gt: 115810\n",
      "Epoch: 063 train_loss: 0.0340 train_acc: 99.5942 Precision: 0.8136 Recall: 0.6472 F1-Score: 0.7209\n",
      "akk: 770, gt: 115791\n",
      "Epoch: 064 train_loss: 0.0334 train_acc: 99.5916 Precision: 0.8039 Recall: 0.6557 F1-Score: 0.7223\n",
      "akk: 745, gt: 115816\n",
      "Epoch: 065 train_loss: 0.0334 train_acc: 99.5873 Precision: 0.8107 Recall: 0.6398 F1-Score: 0.7152\n",
      "akk: 756, gt: 115805\n",
      "Epoch: 066 train_loss: 0.0330 train_acc: 99.5968 Precision: 0.8135 Recall: 0.6515 F1-Score: 0.7235\n",
      "akk: 735, gt: 115826\n",
      "Epoch: 067 train_loss: 0.0333 train_acc: 99.5908 Precision: 0.8177 Recall: 0.6367 F1-Score: 0.7159\n",
      "akk: 749, gt: 115812\n",
      "Epoch: 068 train_loss: 0.0330 train_acc: 99.5873 Precision: 0.8091 Recall: 0.6419 F1-Score: 0.7159\n",
      "akk: 754, gt: 115807\n",
      "Epoch: 069 train_loss: 0.0336 train_acc: 99.5848 Precision: 0.8050 Recall: 0.6430 F1-Score: 0.7150\n",
      "akk: 767, gt: 115794\n",
      "Epoch: 070 train_loss: 0.0334 train_acc: 99.5873 Precision: 0.8018 Recall: 0.6515 F1-Score: 0.7189\n",
      "akk: 776, gt: 115785\n",
      "Epoch: 071 train_loss: 0.0328 train_acc: 99.6036 Precision: 0.8106 Recall: 0.6663 F1-Score: 0.7314\n",
      "akk: 17383, gt: 3751403\n",
      "OA: 0.9952 AA: 0.7439 Kappa: 0.6201 Precision: 0.8574 Recall: 0.4884 F1-Score: 0.6223\n",
      "*************************\n",
      "akk: 754, gt: 115807\n",
      "Epoch: 072 train_loss: 0.0329 train_acc: 99.5933 Precision: 0.8117 Recall: 0.6483 F1-Score: 0.7208\n",
      "akk: 769, gt: 115792\n",
      "Epoch: 073 train_loss: 0.0331 train_acc: 99.6062 Precision: 0.8153 Recall: 0.6642 F1-Score: 0.7320\n",
      "akk: 748, gt: 115813\n",
      "Epoch: 074 train_loss: 0.0329 train_acc: 99.6157 Precision: 0.8316 Recall: 0.6589 F1-Score: 0.7352\n",
      "akk: 742, gt: 115819\n",
      "Epoch: 075 train_loss: 0.0330 train_acc: 99.5916 Precision: 0.8154 Recall: 0.6409 F1-Score: 0.7177\n",
      "akk: 771, gt: 115790\n",
      "Epoch: 076 train_loss: 0.0326 train_acc: 99.6337 Precision: 0.8353 Recall: 0.6822 F1-Score: 0.7510\n",
      "akk: 766, gt: 115795\n",
      "Epoch: 077 train_loss: 0.0324 train_acc: 99.6242 Precision: 0.8303 Recall: 0.6737 F1-Score: 0.7439\n",
      "akk: 762, gt: 115799\n",
      "Epoch: 078 train_loss: 0.0327 train_acc: 99.6019 Precision: 0.8150 Recall: 0.6578 F1-Score: 0.7280\n",
      "akk: 742, gt: 115819\n",
      "Epoch: 079 train_loss: 0.0328 train_acc: 99.6174 Precision: 0.8356 Recall: 0.6568 F1-Score: 0.7355\n",
      "akk: 775, gt: 115786\n",
      "Epoch: 080 train_loss: 0.0325 train_acc: 99.6182 Precision: 0.8219 Recall: 0.6748 F1-Score: 0.7411\n",
      "akk: 793, gt: 115768\n",
      "Epoch: 081 train_loss: 0.0325 train_acc: 99.6354 Precision: 0.8272 Recall: 0.6949 F1-Score: 0.7553\n",
      "akk: 17334, gt: 3751452\n",
      "OA: 0.9951 AA: 0.7414 Kappa: 0.6144 Precision: 0.8511 Recall: 0.4835 F1-Score: 0.6166\n",
      "*************************\n",
      "akk: 790, gt: 115771\n",
      "Epoch: 082 train_loss: 0.0323 train_acc: 99.6311 Precision: 0.8253 Recall: 0.6907 F1-Score: 0.7520\n",
      "akk: 765, gt: 115796\n",
      "Epoch: 083 train_loss: 0.0322 train_acc: 99.6217 Precision: 0.8288 Recall: 0.6716 F1-Score: 0.7420\n",
      "akk: 778, gt: 115783\n",
      "Epoch: 084 train_loss: 0.0322 train_acc: 99.6345 Precision: 0.8329 Recall: 0.6864 F1-Score: 0.7526\n",
      "akk: 789, gt: 115772\n",
      "Epoch: 085 train_loss: 0.0322 train_acc: 99.6131 Precision: 0.8124 Recall: 0.6790 F1-Score: 0.7398\n",
      "akk: 778, gt: 115783\n",
      "Epoch: 086 train_loss: 0.0322 train_acc: 99.6242 Precision: 0.8252 Recall: 0.6801 F1-Score: 0.7456\n",
      "akk: 764, gt: 115797\n",
      "Epoch: 087 train_loss: 0.0324 train_acc: 99.6294 Precision: 0.8351 Recall: 0.6758 F1-Score: 0.7471\n",
      "akk: 790, gt: 115771\n",
      "Epoch: 088 train_loss: 0.0323 train_acc: 99.6431 Precision: 0.8342 Recall: 0.6981 F1-Score: 0.7601\n",
      "akk: 796, gt: 115765\n",
      "Epoch: 089 train_loss: 0.0324 train_acc: 99.6345 Precision: 0.8254 Recall: 0.6960 F1-Score: 0.7552\n",
      "akk: 801, gt: 115760\n",
      "Epoch: 090 train_loss: 0.0323 train_acc: 99.6491 Precision: 0.8340 Recall: 0.7076 F1-Score: 0.7656\n",
      "akk: 804, gt: 115757\n",
      "Epoch: 091 train_loss: 0.0319 train_acc: 99.6671 Precision: 0.8458 Recall: 0.7203 F1-Score: 0.7780\n",
      "akk: 25246, gt: 3743540\n",
      "OA: 0.9956 AA: 0.8197 Kappa: 0.6993 Precision: 0.7747 Recall: 0.6410 F1-Score: 0.7015\n",
      "*************************\n",
      "akk: 801, gt: 115760\n",
      "Epoch: 092 train_loss: 0.0320 train_acc: 99.6422 Precision: 0.8290 Recall: 0.7034 F1-Score: 0.7610\n",
      "akk: 785, gt: 115776\n",
      "Epoch: 093 train_loss: 0.0320 train_acc: 99.6302 Precision: 0.8268 Recall: 0.6875 F1-Score: 0.7507\n",
      "akk: 784, gt: 115777\n",
      "Epoch: 094 train_loss: 0.0321 train_acc: 99.6620 Precision: 0.8508 Recall: 0.7066 F1-Score: 0.7720\n",
      "akk: 776, gt: 115785\n",
      "Epoch: 095 train_loss: 0.0324 train_acc: 99.6483 Precision: 0.8441 Recall: 0.6939 F1-Score: 0.7616\n",
      "akk: 770, gt: 115791\n",
      "Epoch: 096 train_loss: 0.0321 train_acc: 99.6603 Precision: 0.8558 Recall: 0.6981 F1-Score: 0.7690\n",
      "akk: 810, gt: 115751\n",
      "Epoch: 097 train_loss: 0.0323 train_acc: 99.6517 Precision: 0.8321 Recall: 0.7140 F1-Score: 0.7685\n",
      "akk: 804, gt: 115757\n",
      "Epoch: 098 train_loss: 0.0318 train_acc: 99.6843 Precision: 0.8582 Recall: 0.7309 F1-Score: 0.7895\n",
      "akk: 812, gt: 115749\n",
      "Epoch: 099 train_loss: 0.0318 train_acc: 99.6585 Precision: 0.8362 Recall: 0.7193 F1-Score: 0.7733\n",
      "akk: 800, gt: 115761\n",
      "Epoch: 100 train_loss: 0.0321 train_acc: 99.6534 Precision: 0.8375 Recall: 0.7097 F1-Score: 0.7683\n",
      "akk: 22223, gt: 3746563\n",
      "OA: 0.9955 AA: 0.7930 Kappa: 0.6773 Precision: 0.8063 Recall: 0.5872 F1-Score: 0.6795\n",
      "*************************\n",
      "Running Time: 2871.85\n",
      "**************************************************\n",
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [53:07<7:58:11, 3187.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "F1 score: 0.6830271858927259\n",
      "Precision: 0.8090161437709412\n",
      "Recall: 0.5909914491878318\n",
      "max_label: 1 max_pred: 1\n",
      "Accuracy: 0.9965513830141224\n",
      "Confusion Matrix:\n",
      "[[4967897    4389]\n",
      " [  12867   18592]]\n",
      "**************************************************\n",
      "Parameter:\n",
      "fix_random: True\n",
      "gpu_id: 0\n",
      "seed: 0\n",
      "dataset: BS\n",
      "flag_test: train\n",
      "model_name: conv2d_unmix\n",
      "batch_size: 128\n",
      "test_freq: 10\n",
      "patches: 5\n",
      "epoches: 100\n",
      "inference_model: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "learning_rate: 0.0001\n",
      "gamma: 0.9\n",
      "weight_decay: 0\n",
      "Train data change to the ratio of train samples: 0.03\n",
      "height=2715,width=1843,band=150\n",
      "**************************************************\n",
      "patch is : 5\n",
      "mirror_image shape : [2719,1847,150]\n",
      "**************************************************\n",
      "x_train shape = (116561, 5, 5, 150), type = float32\n",
      "x_test  shape = (3768786, 5, 5, 150), type = float32\n",
      "x_true  shape = (5003745, 5, 5, 150), type = float32\n",
      "**************************************************\n",
      "y_train: shape = (116561,) ,type = int64\n",
      "y_test: shape = (3768786,) ,type = int64\n",
      "y_true: shape = (5003745,) ,type = int64\n",
      "**************************************************\n",
      "Model Name: conv2d_unmix\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akk: 6037, gt: 110524\n",
      "Epoch: 001 train_loss: 1.4232 train_acc: 94.4261 Precision: 0.0401 Recall: 0.2564 F1-Score: 0.0693\n",
      "akk: 15532, gt: 3753254\n",
      "OA: 0.9907 AA: 0.5900 Kappa: 0.2380 Precision: 0.3590 Recall: 0.1827 F1-Score: 0.2422\n",
      "*************************\n",
      "akk: 5088, gt: 111473\n",
      "Epoch: 002 train_loss: 0.6723 train_acc: 95.2746 Precision: 0.0515 Recall: 0.2775 F1-Score: 0.0869\n",
      "akk: 3724, gt: 112837\n",
      "Epoch: 003 train_loss: 0.3505 train_acc: 96.5083 Precision: 0.0803 Recall: 0.3167 F1-Score: 0.1281\n",
      "akk: 2510, gt: 114051\n",
      "Epoch: 004 train_loss: 0.1617 train_acc: 97.4983 Precision: 0.1072 Recall: 0.2850 F1-Score: 0.1558\n",
      "akk: 2536, gt: 114025\n",
      "Epoch: 005 train_loss: 0.1375 train_acc: 97.5086 Precision: 0.1136 Recall: 0.3051 F1-Score: 0.1655\n",
      "akk: 1077, gt: 115484\n",
      "Epoch: 006 train_loss: 0.0823 train_acc: 98.4652 Precision: 0.1077 Recall: 0.1229 F1-Score: 0.1148\n",
      "akk: 10, gt: 116551\n",
      "Epoch: 007 train_loss: 0.0469 train_acc: 99.1815 Precision: 0.0000 Recall: 0.0000 F1-Score: 0.0000\n",
      "akk: 1, gt: 116560\n",
      "Epoch: 008 train_loss: 0.0443 train_acc: 99.1893 Precision: 0.0000 Recall: 0.0000 F1-Score: 0.0000\n",
      "akk: 3, gt: 116558\n",
      "Epoch: 009 train_loss: 0.0425 train_acc: 99.1893 Precision: 0.3333 Recall: 0.0011 F1-Score: 0.0021\n",
      "akk: 5, gt: 116556\n",
      "Epoch: 010 train_loss: 0.0411 train_acc: 99.1893 Precision: 0.4000 Recall: 0.0021 F1-Score: 0.0042\n",
      "akk: 39, gt: 116522\n",
      "Epoch: 011 train_loss: 0.0413 train_acc: 99.1841 Precision: 0.4103 Recall: 0.0169 F1-Score: 0.0326\n",
      "akk: 172, gt: 3768614\n",
      "OA: 0.9919 AA: 0.5021 Kappa: 0.0084 Precision: 0.7616 Recall: 0.0043 F1-Score: 0.0085\n",
      "*************************\n",
      "akk: 34, gt: 116527\n",
      "Epoch: 012 train_loss: 0.0398 train_acc: 99.1850 Precision: 0.4118 Recall: 0.0148 F1-Score: 0.0286\n",
      "akk: 23, gt: 116538\n",
      "Epoch: 013 train_loss: 0.0403 train_acc: 99.1875 Precision: 0.4348 Recall: 0.0106 F1-Score: 0.0207\n",
      "akk: 69, gt: 116492\n",
      "Epoch: 014 train_loss: 0.0398 train_acc: 99.2133 Precision: 0.6957 Recall: 0.0508 F1-Score: 0.0948\n",
      "akk: 317, gt: 116244\n",
      "Epoch: 015 train_loss: 0.0399 train_acc: 99.3248 Precision: 0.7476 Recall: 0.2511 F1-Score: 0.3759\n",
      "akk: 426, gt: 116135\n",
      "Epoch: 016 train_loss: 0.0389 train_acc: 99.3583 Precision: 0.7300 Recall: 0.3294 F1-Score: 0.4540\n",
      "akk: 400, gt: 116161\n",
      "Epoch: 017 train_loss: 0.0396 train_acc: 99.3377 Precision: 0.7150 Recall: 0.3030 F1-Score: 0.4256\n",
      "akk: 438, gt: 116123\n",
      "Epoch: 018 train_loss: 0.0383 train_acc: 99.3772 Precision: 0.7489 Recall: 0.3475 F1-Score: 0.4747\n",
      "akk: 429, gt: 116132\n",
      "Epoch: 019 train_loss: 0.0380 train_acc: 99.3900 Precision: 0.7716 Recall: 0.3506 F1-Score: 0.4822\n",
      "akk: 469, gt: 116092\n",
      "Epoch: 020 train_loss: 0.0375 train_acc: 99.3935 Precision: 0.7527 Recall: 0.3739 F1-Score: 0.4996\n",
      "akk: 500, gt: 116061\n",
      "Epoch: 021 train_loss: 0.0382 train_acc: 99.3995 Precision: 0.7440 Recall: 0.3941 F1-Score: 0.5152\n",
      "akk: 9443, gt: 3759343\n",
      "OA: 0.9936 AA: 0.6300 Kappa: 0.3954 Precision: 0.8414 Recall: 0.2604 F1-Score: 0.3977\n",
      "*************************\n",
      "akk: 493, gt: 116068\n",
      "Epoch: 022 train_loss: 0.0372 train_acc: 99.4278 Precision: 0.7809 Recall: 0.4078 F1-Score: 0.5358\n",
      "akk: 507, gt: 116054\n",
      "Epoch: 023 train_loss: 0.0376 train_acc: 99.4449 Precision: 0.7929 Recall: 0.4258 F1-Score: 0.5541\n",
      "akk: 508, gt: 116053\n",
      "Epoch: 024 train_loss: 0.0374 train_acc: 99.4046 Precision: 0.7461 Recall: 0.4015 F1-Score: 0.5220\n",
      "akk: 549, gt: 116012\n",
      "Epoch: 025 train_loss: 0.0376 train_acc: 99.4072 Precision: 0.7304 Recall: 0.4248 F1-Score: 0.5372\n",
      "akk: 525, gt: 116036\n",
      "Epoch: 026 train_loss: 0.0368 train_acc: 99.4226 Precision: 0.7581 Recall: 0.4216 F1-Score: 0.5419\n",
      "akk: 542, gt: 116019\n",
      "Epoch: 027 train_loss: 0.0375 train_acc: 99.4321 Precision: 0.7601 Recall: 0.4364 F1-Score: 0.5545\n",
      "akk: 567, gt: 115994\n",
      "Epoch: 028 train_loss: 0.0380 train_acc: 99.4055 Precision: 0.7213 Recall: 0.4333 F1-Score: 0.5414\n",
      "akk: 548, gt: 116013\n",
      "Epoch: 029 train_loss: 0.0367 train_acc: 99.4235 Precision: 0.7482 Recall: 0.4343 F1-Score: 0.5496\n",
      "akk: 570, gt: 115991\n",
      "Epoch: 030 train_loss: 0.0374 train_acc: 99.4063 Precision: 0.7211 Recall: 0.4354 F1-Score: 0.5429\n",
      "akk: 578, gt: 115983\n",
      "Epoch: 031 train_loss: 0.0364 train_acc: 99.4286 Precision: 0.7405 Recall: 0.4534 F1-Score: 0.5624\n",
      "akk: 2126, gt: 3766660\n",
      "OA: 0.9923 AA: 0.5290 Kappa: 0.1075 Precision: 0.8325 Recall: 0.0580 F1-Score: 0.1085\n",
      "*************************\n",
      "akk: 556, gt: 116005\n",
      "Epoch: 032 train_loss: 0.0365 train_acc: 99.4132 Precision: 0.7338 Recall: 0.4322 F1-Score: 0.5440\n",
      "akk: 586, gt: 115975\n",
      "Epoch: 033 train_loss: 0.0361 train_acc: 99.4595 Precision: 0.7679 Recall: 0.4767 F1-Score: 0.5882\n",
      "akk: 611, gt: 115950\n",
      "Epoch: 034 train_loss: 0.0361 train_acc: 99.4621 Precision: 0.7594 Recall: 0.4915 F1-Score: 0.5968\n",
      "akk: 593, gt: 115968\n",
      "Epoch: 035 train_loss: 0.0363 train_acc: 99.4569 Precision: 0.7622 Recall: 0.4788 F1-Score: 0.5882\n",
      "akk: 562, gt: 115999\n",
      "Epoch: 036 train_loss: 0.0359 train_acc: 99.4595 Precision: 0.7794 Recall: 0.4640 F1-Score: 0.5817\n",
      "akk: 603, gt: 115958\n",
      "Epoch: 037 train_loss: 0.0357 train_acc: 99.4638 Precision: 0.7645 Recall: 0.4883 F1-Score: 0.5960\n",
      "akk: 609, gt: 115952\n",
      "Epoch: 038 train_loss: 0.0356 train_acc: 99.4741 Precision: 0.7718 Recall: 0.4979 F1-Score: 0.6053\n",
      "akk: 609, gt: 115952\n",
      "Epoch: 039 train_loss: 0.0356 train_acc: 99.4552 Precision: 0.7537 Recall: 0.4862 F1-Score: 0.5911\n",
      "akk: 621, gt: 115940\n",
      "Epoch: 040 train_loss: 0.0350 train_acc: 99.5015 Precision: 0.7923 Recall: 0.5212 F1-Score: 0.6288\n",
      "akk: 597, gt: 115964\n",
      "Epoch: 041 train_loss: 0.0353 train_acc: 99.4604 Precision: 0.7638 Recall: 0.4831 F1-Score: 0.5918\n",
      "akk: 13256, gt: 3755530\n",
      "OA: 0.9944 AA: 0.6850 Kappa: 0.5142 Precision: 0.8528 Recall: 0.3705 F1-Score: 0.5166\n",
      "*************************\n",
      "akk: 649, gt: 115912\n",
      "Epoch: 042 train_loss: 0.0351 train_acc: 99.4689 Precision: 0.7504 Recall: 0.5159 F1-Score: 0.6114\n",
      "akk: 623, gt: 115938\n",
      "Epoch: 043 train_loss: 0.0356 train_acc: 99.4604 Precision: 0.7528 Recall: 0.4968 F1-Score: 0.5986\n",
      "akk: 635, gt: 115926\n",
      "Epoch: 044 train_loss: 0.0353 train_acc: 99.4810 Precision: 0.7669 Recall: 0.5159 F1-Score: 0.6168\n",
      "akk: 608, gt: 115953\n",
      "Epoch: 045 train_loss: 0.0355 train_acc: 99.4732 Precision: 0.7714 Recall: 0.4968 F1-Score: 0.6044\n",
      "akk: 600, gt: 115961\n",
      "Epoch: 046 train_loss: 0.0352 train_acc: 99.4561 Precision: 0.7583 Recall: 0.4820 F1-Score: 0.5894\n",
      "akk: 616, gt: 115945\n",
      "Epoch: 047 train_loss: 0.0350 train_acc: 99.4767 Precision: 0.7711 Recall: 0.5032 F1-Score: 0.6090\n",
      "akk: 623, gt: 115938\n",
      "Epoch: 048 train_loss: 0.0347 train_acc: 99.4895 Precision: 0.7801 Recall: 0.5148 F1-Score: 0.6203\n",
      "akk: 593, gt: 115968\n",
      "Epoch: 049 train_loss: 0.0349 train_acc: 99.4878 Precision: 0.7926 Recall: 0.4979 F1-Score: 0.6116\n",
      "akk: 639, gt: 115922\n",
      "Epoch: 050 train_loss: 0.0347 train_acc: 99.4981 Precision: 0.7809 Recall: 0.5286 F1-Score: 0.6304\n",
      "akk: 606, gt: 115955\n",
      "Epoch: 051 train_loss: 0.0346 train_acc: 99.4921 Precision: 0.7904 Recall: 0.5074 F1-Score: 0.6181\n",
      "akk: 21352, gt: 3747434\n",
      "OA: 0.9950 AA: 0.7707 Kappa: 0.6361 Precision: 0.7756 Recall: 0.5427 F1-Score: 0.6386\n",
      "*************************\n",
      "akk: 600, gt: 115961\n",
      "Epoch: 052 train_loss: 0.0345 train_acc: 99.4835 Precision: 0.7850 Recall: 0.4989 F1-Score: 0.6101\n",
      "akk: 631, gt: 115930\n",
      "Epoch: 053 train_loss: 0.0345 train_acc: 99.4895 Precision: 0.7765 Recall: 0.5191 F1-Score: 0.6222\n",
      "akk: 646, gt: 115915\n",
      "Epoch: 054 train_loss: 0.0344 train_acc: 99.5076 Precision: 0.7864 Recall: 0.5381 F1-Score: 0.6390\n",
      "akk: 654, gt: 115907\n",
      "Epoch: 055 train_loss: 0.0344 train_acc: 99.5007 Precision: 0.7768 Recall: 0.5381 F1-Score: 0.6358\n",
      "akk: 616, gt: 115945\n",
      "Epoch: 056 train_loss: 0.0343 train_acc: 99.5076 Precision: 0.8003 Recall: 0.5222 F1-Score: 0.6321\n",
      "akk: 577, gt: 115984\n",
      "Epoch: 057 train_loss: 0.0346 train_acc: 99.5033 Precision: 0.8163 Recall: 0.4989 F1-Score: 0.6193\n",
      "akk: 628, gt: 115933\n",
      "Epoch: 058 train_loss: 0.0344 train_acc: 99.5041 Precision: 0.7914 Recall: 0.5265 F1-Score: 0.6323\n",
      "akk: 582, gt: 115979\n",
      "Epoch: 059 train_loss: 0.0345 train_acc: 99.4904 Precision: 0.8007 Recall: 0.4936 F1-Score: 0.6107\n",
      "akk: 646, gt: 115915\n",
      "Epoch: 060 train_loss: 0.0337 train_acc: 99.5127 Precision: 0.7910 Recall: 0.5413 F1-Score: 0.6428\n",
      "akk: 682, gt: 115879\n",
      "Epoch: 061 train_loss: 0.0339 train_acc: 99.5058 Precision: 0.7698 Recall: 0.5561 F1-Score: 0.6458\n",
      "akk: 14971, gt: 3753815\n",
      "OA: 0.9945 AA: 0.7017 Kappa: 0.5398 Precision: 0.8237 Recall: 0.4041 F1-Score: 0.5422\n",
      "*************************\n",
      "akk: 640, gt: 115921\n",
      "Epoch: 062 train_loss: 0.0339 train_acc: 99.4973 Precision: 0.7797 Recall: 0.5286 F1-Score: 0.6301\n",
      "akk: 687, gt: 115874\n",
      "Epoch: 063 train_loss: 0.0338 train_acc: 99.5239 Precision: 0.7831 Recall: 0.5699 F1-Score: 0.6597\n",
      "akk: 672, gt: 115889\n",
      "Epoch: 064 train_loss: 0.0340 train_acc: 99.5247 Precision: 0.7902 Recall: 0.5625 F1-Score: 0.6572\n",
      "akk: 695, gt: 115866\n",
      "Epoch: 065 train_loss: 0.0339 train_acc: 99.5136 Precision: 0.7712 Recall: 0.5678 F1-Score: 0.6541\n",
      "akk: 627, gt: 115934\n",
      "Epoch: 066 train_loss: 0.0339 train_acc: 99.5221 Precision: 0.8086 Recall: 0.5371 F1-Score: 0.6454\n",
      "akk: 654, gt: 115907\n",
      "Epoch: 067 train_loss: 0.0340 train_acc: 99.5367 Precision: 0.8089 Recall: 0.5604 F1-Score: 0.6621\n",
      "akk: 647, gt: 115914\n",
      "Epoch: 068 train_loss: 0.0337 train_acc: 99.5324 Precision: 0.8083 Recall: 0.5540 F1-Score: 0.6574\n",
      "akk: 639, gt: 115922\n",
      "Epoch: 069 train_loss: 0.0340 train_acc: 99.5204 Precision: 0.8013 Recall: 0.5424 F1-Score: 0.6469\n",
      "akk: 663, gt: 115898\n",
      "Epoch: 070 train_loss: 0.0334 train_acc: 99.5239 Precision: 0.7934 Recall: 0.5572 F1-Score: 0.6546\n",
      "akk: 651, gt: 115910\n",
      "Epoch: 071 train_loss: 0.0337 train_acc: 99.5118 Precision: 0.7880 Recall: 0.5434 F1-Score: 0.6433\n",
      "akk: 23019, gt: 3745767\n",
      "OA: 0.9951 AA: 0.7854 Kappa: 0.6500 Precision: 0.7587 Recall: 0.5723 F1-Score: 0.6524\n",
      "*************************\n",
      "akk: 689, gt: 115872\n",
      "Epoch: 072 train_loss: 0.0337 train_acc: 99.5341 Precision: 0.7910 Recall: 0.5773 F1-Score: 0.6675\n",
      "akk: 662, gt: 115899\n",
      "Epoch: 073 train_loss: 0.0334 train_acc: 99.5367 Precision: 0.8051 Recall: 0.5646 F1-Score: 0.6638\n",
      "akk: 683, gt: 115878\n",
      "Epoch: 074 train_loss: 0.0333 train_acc: 99.5427 Precision: 0.8009 Recall: 0.5794 F1-Score: 0.6724\n",
      "akk: 664, gt: 115897\n",
      "Epoch: 075 train_loss: 0.0335 train_acc: 99.5487 Precision: 0.8148 Recall: 0.5731 F1-Score: 0.6729\n",
      "akk: 691, gt: 115870\n",
      "Epoch: 076 train_loss: 0.0335 train_acc: 99.5324 Precision: 0.7887 Recall: 0.5773 F1-Score: 0.6667\n",
      "akk: 715, gt: 115846\n",
      "Epoch: 077 train_loss: 0.0337 train_acc: 99.5273 Precision: 0.7748 Recall: 0.5869 F1-Score: 0.6679\n",
      "akk: 673, gt: 115888\n",
      "Epoch: 078 train_loss: 0.0333 train_acc: 99.5410 Precision: 0.8039 Recall: 0.5731 F1-Score: 0.6691\n",
      "akk: 671, gt: 115890\n",
      "Epoch: 079 train_loss: 0.0333 train_acc: 99.5444 Precision: 0.8077 Recall: 0.5742 F1-Score: 0.6712\n",
      "akk: 671, gt: 115890\n",
      "Epoch: 080 train_loss: 0.0330 train_acc: 99.5479 Precision: 0.8107 Recall: 0.5763 F1-Score: 0.6737\n",
      "akk: 663, gt: 115898\n",
      "Epoch: 081 train_loss: 0.0335 train_acc: 99.5462 Precision: 0.8130 Recall: 0.5710 F1-Score: 0.6708\n",
      "akk: 23266, gt: 3745520\n",
      "OA: 0.9953 AA: 0.7937 Kappa: 0.6659 Precision: 0.7724 Recall: 0.5889 F1-Score: 0.6683\n",
      "*************************\n",
      "akk: 690, gt: 115871\n",
      "Epoch: 082 train_loss: 0.0331 train_acc: 99.5539 Precision: 0.8072 Recall: 0.5900 F1-Score: 0.6818\n",
      "akk: 688, gt: 115873\n",
      "Epoch: 083 train_loss: 0.0330 train_acc: 99.5367 Precision: 0.7936 Recall: 0.5784 F1-Score: 0.6691\n",
      "akk: 688, gt: 115873\n",
      "Epoch: 084 train_loss: 0.0331 train_acc: 99.5436 Precision: 0.7994 Recall: 0.5826 F1-Score: 0.6740\n",
      "akk: 715, gt: 115846\n",
      "Epoch: 085 train_loss: 0.0330 train_acc: 99.5393 Precision: 0.7846 Recall: 0.5943 F1-Score: 0.6763\n",
      "akk: 703, gt: 115858\n",
      "Epoch: 086 train_loss: 0.0330 train_acc: 99.5290 Precision: 0.7809 Recall: 0.5816 F1-Score: 0.6667\n",
      "akk: 706, gt: 115855\n",
      "Epoch: 087 train_loss: 0.0328 train_acc: 99.5710 Precision: 0.8144 Recall: 0.6091 F1-Score: 0.6970\n",
      "akk: 705, gt: 115856\n",
      "Epoch: 088 train_loss: 0.0331 train_acc: 99.5410 Precision: 0.7901 Recall: 0.5900 F1-Score: 0.6756\n",
      "akk: 714, gt: 115847\n",
      "Epoch: 089 train_loss: 0.0330 train_acc: 99.5590 Precision: 0.8011 Recall: 0.6059 F1-Score: 0.6900\n",
      "akk: 718, gt: 115843\n",
      "Epoch: 090 train_loss: 0.0331 train_acc: 99.5625 Precision: 0.8022 Recall: 0.6102 F1-Score: 0.6931\n",
      "akk: 705, gt: 115856\n",
      "Epoch: 091 train_loss: 0.0331 train_acc: 99.5444 Precision: 0.7929 Recall: 0.5922 F1-Score: 0.6780\n",
      "akk: 24200, gt: 3744586\n",
      "OA: 0.9952 AA: 0.8002 Kappa: 0.6691 Precision: 0.7590 Recall: 0.6020 F1-Score: 0.6714\n",
      "*************************\n",
      "akk: 714, gt: 115847\n",
      "Epoch: 092 train_loss: 0.0328 train_acc: 99.5607 Precision: 0.8025 Recall: 0.6070 F1-Score: 0.6912\n",
      "akk: 690, gt: 115871\n",
      "Epoch: 093 train_loss: 0.0333 train_acc: 99.5436 Precision: 0.7986 Recall: 0.5837 F1-Score: 0.6744\n",
      "akk: 716, gt: 115845\n",
      "Epoch: 094 train_loss: 0.0327 train_acc: 99.5625 Precision: 0.8031 Recall: 0.6091 F1-Score: 0.6928\n",
      "akk: 705, gt: 115856\n",
      "Epoch: 095 train_loss: 0.0329 train_acc: 99.5359 Precision: 0.7858 Recall: 0.5869 F1-Score: 0.6719\n",
      "akk: 681, gt: 115880\n",
      "Epoch: 096 train_loss: 0.0329 train_acc: 99.5479 Precision: 0.8062 Recall: 0.5816 F1-Score: 0.6757\n",
      "akk: 709, gt: 115852\n",
      "Epoch: 097 train_loss: 0.0329 train_acc: 99.5753 Precision: 0.8166 Recall: 0.6133 F1-Score: 0.7005\n",
      "akk: 721, gt: 115840\n",
      "Epoch: 098 train_loss: 0.0328 train_acc: 99.5565 Precision: 0.7961 Recall: 0.6081 F1-Score: 0.6895\n",
      "akk: 704, gt: 115857\n",
      "Epoch: 099 train_loss: 0.0327 train_acc: 99.5642 Precision: 0.8097 Recall: 0.6038 F1-Score: 0.6917\n",
      "akk: 712, gt: 115849\n",
      "Epoch: 100 train_loss: 0.0326 train_acc: 99.5762 Precision: 0.8160 Recall: 0.6155 F1-Score: 0.7017\n",
      "akk: 21861, gt: 3746925\n",
      "OA: 0.9952 AA: 0.7811 Kappa: 0.6541 Precision: 0.7864 Recall: 0.5634 F1-Score: 0.6564\n",
      "*************************\n",
      "Running Time: 2998.27\n",
      "**************************************************\n",
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [1:48:54<7:17:27, 3280.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "F1 score: 0.594124229619361\n",
      "Precision: 0.6259590342788766\n",
      "Recall: 0.5653708000890048\n",
      "max_label: 1 max_pred: 1\n",
      "Accuracy: 0.9951434375652636\n",
      "Confusion Matrix:\n",
      "[[4961658   10628]\n",
      " [  13673   17786]]\n",
      "**************************************************\n",
      "Parameter:\n",
      "fix_random: True\n",
      "gpu_id: 0\n",
      "seed: 0\n",
      "dataset: BS\n",
      "flag_test: train\n",
      "model_name: conv2d_unmix\n",
      "batch_size: 128\n",
      "test_freq: 10\n",
      "patches: 5\n",
      "epoches: 100\n",
      "inference_model: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "learning_rate: 0.0001\n",
      "gamma: 0.9\n",
      "weight_decay: 0\n",
      "Train data change to the ratio of train samples: 0.03\n",
      "height=2715,width=1843,band=150\n",
      "**************************************************\n",
      "patch is : 5\n",
      "mirror_image shape : [2719,1847,150]\n",
      "**************************************************\n",
      "x_train shape = (116561, 5, 5, 150), type = float32\n",
      "x_test  shape = (3768786, 5, 5, 150), type = float32\n",
      "x_true  shape = (5003745, 5, 5, 150), type = float32\n",
      "**************************************************\n",
      "y_train: shape = (116561,) ,type = int64\n",
      "y_test: shape = (3768786,) ,type = int64\n",
      "y_true: shape = (5003745,) ,type = int64\n",
      "**************************************************\n",
      "Model Name: conv2d_unmix\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akk: 3986, gt: 112575\n",
      "Epoch: 001 train_loss: 1.0670 train_acc: 96.0501 Precision: 0.0409 Recall: 0.1727 F1-Score: 0.0661\n",
      "akk: 202740, gt: 3566046\n",
      "OA: 0.9446 AA: 0.6742 Kappa: 0.0917 Precision: 0.0601 Recall: 0.3994 F1-Score: 0.1045\n",
      "*************************\n",
      "akk: 3677, gt: 112884\n",
      "Epoch: 002 train_loss: 0.4195 train_acc: 96.4525 Precision: 0.0661 Recall: 0.2574 F1-Score: 0.1052\n",
      "akk: 2624, gt: 113937\n",
      "Epoch: 003 train_loss: 0.2126 train_acc: 97.2324 Precision: 0.0652 Recall: 0.1811 F1-Score: 0.0959\n",
      "akk: 422, gt: 116139\n",
      "Epoch: 004 train_loss: 0.0658 train_acc: 98.8349 Precision: 0.0095 Recall: 0.0042 F1-Score: 0.0059\n",
      "akk: 92, gt: 116469\n",
      "Epoch: 005 train_loss: 0.0500 train_acc: 99.1112 Precision: 0.0000 Recall: 0.0000 F1-Score: 0.0000\n",
      "akk: 6, gt: 116555\n",
      "Epoch: 006 train_loss: 0.0443 train_acc: 99.1867 Precision: 0.1667 Recall: 0.0011 F1-Score: 0.0021\n",
      "akk: 2, gt: 116559\n",
      "Epoch: 007 train_loss: 0.0415 train_acc: 99.1884 Precision: 0.0000 Recall: 0.0000 F1-Score: 0.0000\n",
      "akk: 2, gt: 116559\n",
      "Epoch: 008 train_loss: 0.0403 train_acc: 99.1884 Precision: 0.0000 Recall: 0.0000 F1-Score: 0.0000\n",
      "akk: 2, gt: 116559\n",
      "Epoch: 009 train_loss: 0.0404 train_acc: 99.1884 Precision: 0.0000 Recall: 0.0000 F1-Score: 0.0000\n",
      "akk: 169, gt: 116392\n",
      "Epoch: 010 train_loss: 0.0385 train_acc: 99.2922 Precision: 0.8521 Recall: 0.1525 F1-Score: 0.2588\n",
      "akk: 388, gt: 116173\n",
      "Epoch: 011 train_loss: 0.0381 train_acc: 99.4046 Precision: 0.8222 Recall: 0.3379 F1-Score: 0.4790\n",
      "akk: 52264, gt: 3716522\n",
      "OA: 0.9872 AA: 0.7798 Kappa: 0.4135 Precision: 0.3322 Recall: 0.5690 F1-Score: 0.4195\n",
      "*************************\n",
      "akk: 471, gt: 116090\n",
      "Epoch: 012 train_loss: 0.0382 train_acc: 99.4261 Precision: 0.7919 Recall: 0.3951 F1-Score: 0.5272\n",
      "akk: 482, gt: 116079\n",
      "Epoch: 013 train_loss: 0.0373 train_acc: 99.4526 Precision: 0.8174 Recall: 0.4174 F1-Score: 0.5526\n",
      "akk: 497, gt: 116064\n",
      "Epoch: 014 train_loss: 0.0371 train_acc: 99.4415 Precision: 0.7948 Recall: 0.4184 F1-Score: 0.5482\n",
      "akk: 528, gt: 116033\n",
      "Epoch: 015 train_loss: 0.0393 train_acc: 99.4544 Precision: 0.7917 Recall: 0.4428 F1-Score: 0.5679\n",
      "akk: 568, gt: 115993\n",
      "Epoch: 016 train_loss: 0.0360 train_acc: 99.4835 Precision: 0.8011 Recall: 0.4820 F1-Score: 0.6019\n",
      "akk: 579, gt: 115982\n",
      "Epoch: 017 train_loss: 0.0360 train_acc: 99.4655 Precision: 0.7772 Recall: 0.4767 F1-Score: 0.5909\n",
      "akk: 597, gt: 115964\n",
      "Epoch: 018 train_loss: 0.0363 train_acc: 99.4741 Precision: 0.7772 Recall: 0.4915 F1-Score: 0.6022\n",
      "akk: 633, gt: 115928\n",
      "Epoch: 019 train_loss: 0.0360 train_acc: 99.4878 Precision: 0.7741 Recall: 0.5191 F1-Score: 0.6214\n",
      "akk: 660, gt: 115901\n",
      "Epoch: 020 train_loss: 0.0354 train_acc: 99.5007 Precision: 0.7742 Recall: 0.5413 F1-Score: 0.6372\n",
      "akk: 654, gt: 115907\n",
      "Epoch: 021 train_loss: 0.0356 train_acc: 99.5007 Precision: 0.7768 Recall: 0.5381 F1-Score: 0.6358\n",
      "akk: 27711, gt: 3741075\n",
      "OA: 0.9943 AA: 0.7994 Kappa: 0.6273 Precision: 0.6621 Recall: 0.6012 F1-Score: 0.6302\n",
      "*************************\n",
      "akk: 672, gt: 115889\n",
      "Epoch: 022 train_loss: 0.0353 train_acc: 99.4955 Precision: 0.7649 Recall: 0.5445 F1-Score: 0.6361\n",
      "akk: 659, gt: 115902\n",
      "Epoch: 023 train_loss: 0.0353 train_acc: 99.5084 Precision: 0.7815 Recall: 0.5456 F1-Score: 0.6425\n",
      "akk: 672, gt: 115889\n",
      "Epoch: 024 train_loss: 0.0351 train_acc: 99.4938 Precision: 0.7634 Recall: 0.5434 F1-Score: 0.6349\n",
      "akk: 688, gt: 115873\n",
      "Epoch: 025 train_loss: 0.0355 train_acc: 99.5058 Precision: 0.7674 Recall: 0.5593 F1-Score: 0.6471\n",
      "akk: 667, gt: 115894\n",
      "Epoch: 026 train_loss: 0.0349 train_acc: 99.5136 Precision: 0.7826 Recall: 0.5530 F1-Score: 0.6480\n",
      "akk: 698, gt: 115863\n",
      "Epoch: 027 train_loss: 0.0346 train_acc: 99.5196 Precision: 0.7751 Recall: 0.5731 F1-Score: 0.6590\n",
      "akk: 670, gt: 115891\n",
      "Epoch: 028 train_loss: 0.0348 train_acc: 99.5058 Precision: 0.7746 Recall: 0.5498 F1-Score: 0.6431\n",
      "akk: 702, gt: 115859\n",
      "Epoch: 029 train_loss: 0.0346 train_acc: 99.5144 Precision: 0.7692 Recall: 0.5720 F1-Score: 0.6561\n",
      "akk: 688, gt: 115873\n",
      "Epoch: 030 train_loss: 0.0345 train_acc: 99.5110 Precision: 0.7718 Recall: 0.5625 F1-Score: 0.6507\n",
      "akk: 705, gt: 115856\n",
      "Epoch: 031 train_loss: 0.0344 train_acc: 99.5341 Precision: 0.7844 Recall: 0.5858 F1-Score: 0.6707\n",
      "akk: 20440, gt: 3748346\n",
      "OA: 0.9953 AA: 0.7723 Kappa: 0.6513 Precision: 0.8146 Recall: 0.5456 F1-Score: 0.6535\n",
      "*************************\n",
      "akk: 702, gt: 115859\n",
      "Epoch: 032 train_loss: 0.0344 train_acc: 99.5127 Precision: 0.7678 Recall: 0.5710 F1-Score: 0.6549\n",
      "akk: 718, gt: 115843\n",
      "Epoch: 033 train_loss: 0.0341 train_acc: 99.5196 Precision: 0.7674 Recall: 0.5837 F1-Score: 0.6631\n",
      "akk: 733, gt: 115828\n",
      "Epoch: 034 train_loss: 0.0341 train_acc: 99.5359 Precision: 0.7749 Recall: 0.6017 F1-Score: 0.6774\n",
      "akk: 731, gt: 115830\n",
      "Epoch: 035 train_loss: 0.0343 train_acc: 99.5324 Precision: 0.7729 Recall: 0.5985 F1-Score: 0.6746\n",
      "akk: 739, gt: 115822\n",
      "Epoch: 036 train_loss: 0.0344 train_acc: 99.5359 Precision: 0.7727 Recall: 0.6049 F1-Score: 0.6786\n",
      "akk: 726, gt: 115835\n",
      "Epoch: 037 train_loss: 0.0341 train_acc: 99.5316 Precision: 0.7741 Recall: 0.5953 F1-Score: 0.6731\n",
      "akk: 734, gt: 115827\n",
      "Epoch: 038 train_loss: 0.0340 train_acc: 99.5333 Precision: 0.7725 Recall: 0.6006 F1-Score: 0.6758\n",
      "akk: 727, gt: 115834\n",
      "Epoch: 039 train_loss: 0.0341 train_acc: 99.5376 Precision: 0.7785 Recall: 0.5996 F1-Score: 0.6774\n",
      "akk: 768, gt: 115793\n",
      "Epoch: 040 train_loss: 0.0338 train_acc: 99.5453 Precision: 0.7695 Recall: 0.6261 F1-Score: 0.6904\n",
      "akk: 752, gt: 115809\n",
      "Epoch: 041 train_loss: 0.0338 train_acc: 99.5161 Precision: 0.7527 Recall: 0.5996 F1-Score: 0.6675\n",
      "akk: 26837, gt: 3741949\n",
      "OA: 0.9951 AA: 0.8181 Kappa: 0.6767 Precision: 0.7256 Recall: 0.6382 F1-Score: 0.6791\n",
      "*************************\n",
      "akk: 754, gt: 115807\n",
      "Epoch: 042 train_loss: 0.0337 train_acc: 99.5367 Precision: 0.7679 Recall: 0.6133 F1-Score: 0.6820\n",
      "akk: 742, gt: 115819\n",
      "Epoch: 043 train_loss: 0.0338 train_acc: 99.5419 Precision: 0.7763 Recall: 0.6102 F1-Score: 0.6833\n",
      "akk: 775, gt: 115786\n",
      "Epoch: 044 train_loss: 0.0338 train_acc: 99.5221 Precision: 0.7497 Recall: 0.6155 F1-Score: 0.6760\n",
      "akk: 765, gt: 115796\n",
      "Epoch: 045 train_loss: 0.0336 train_acc: 99.5462 Precision: 0.7712 Recall: 0.6250 F1-Score: 0.6905\n",
      "akk: 759, gt: 115802\n",
      "Epoch: 046 train_loss: 0.0336 train_acc: 99.5479 Precision: 0.7747 Recall: 0.6229 F1-Score: 0.6905\n",
      "akk: 753, gt: 115808\n",
      "Epoch: 047 train_loss: 0.0337 train_acc: 99.5513 Precision: 0.7795 Recall: 0.6218 F1-Score: 0.6918\n",
      "akk: 774, gt: 115787\n",
      "Epoch: 048 train_loss: 0.0337 train_acc: 99.5384 Precision: 0.7623 Recall: 0.6250 F1-Score: 0.6868\n",
      "akk: 765, gt: 115796\n",
      "Epoch: 049 train_loss: 0.0336 train_acc: 99.5444 Precision: 0.7699 Recall: 0.6239 F1-Score: 0.6893\n",
      "akk: 752, gt: 115809\n",
      "Epoch: 050 train_loss: 0.0336 train_acc: 99.5333 Precision: 0.7660 Recall: 0.6102 F1-Score: 0.6792\n",
      "akk: 785, gt: 115776\n",
      "Epoch: 051 train_loss: 0.0334 train_acc: 99.5496 Precision: 0.7669 Recall: 0.6377 F1-Score: 0.6964\n",
      "akk: 13280, gt: 3755506\n",
      "OA: 0.9943 AA: 0.6839 Kappa: 0.5110 Precision: 0.8465 Recall: 0.3684 F1-Score: 0.5134\n",
      "*************************\n",
      "akk: 783, gt: 115778\n",
      "Epoch: 052 train_loss: 0.0335 train_acc: 99.5290 Precision: 0.7522 Recall: 0.6239 F1-Score: 0.6821\n",
      "akk: 773, gt: 115788\n",
      "Epoch: 053 train_loss: 0.0334 train_acc: 99.5410 Precision: 0.7646 Recall: 0.6261 F1-Score: 0.6884\n",
      "akk: 785, gt: 115776\n",
      "Epoch: 054 train_loss: 0.0333 train_acc: 99.5565 Precision: 0.7720 Recall: 0.6419 F1-Score: 0.7010\n",
      "akk: 801, gt: 115760\n",
      "Epoch: 055 train_loss: 0.0333 train_acc: 99.5513 Precision: 0.7628 Recall: 0.6472 F1-Score: 0.7003\n",
      "akk: 785, gt: 115776\n",
      "Epoch: 056 train_loss: 0.0333 train_acc: 99.5359 Precision: 0.7567 Recall: 0.6292 F1-Score: 0.6871\n",
      "akk: 787, gt: 115774\n",
      "Epoch: 057 train_loss: 0.0334 train_acc: 99.5565 Precision: 0.7713 Recall: 0.6430 F1-Score: 0.7013\n",
      "akk: 773, gt: 115788\n",
      "Epoch: 058 train_loss: 0.0334 train_acc: 99.5496 Precision: 0.7710 Recall: 0.6314 F1-Score: 0.6942\n",
      "akk: 773, gt: 115788\n",
      "Epoch: 059 train_loss: 0.0332 train_acc: 99.5444 Precision: 0.7671 Recall: 0.6282 F1-Score: 0.6907\n",
      "akk: 810, gt: 115751\n",
      "Epoch: 060 train_loss: 0.0331 train_acc: 99.5522 Precision: 0.7605 Recall: 0.6525 F1-Score: 0.7024\n",
      "akk: 761, gt: 115800\n",
      "Epoch: 061 train_loss: 0.0331 train_acc: 99.5719 Precision: 0.7924 Recall: 0.6388 F1-Score: 0.7073\n",
      "akk: 19642, gt: 3749144\n",
      "OA: 0.9952 AA: 0.7634 Kappa: 0.6399 Precision: 0.8200 Recall: 0.5278 F1-Score: 0.6422\n",
      "*************************\n",
      "akk: 769, gt: 115792\n",
      "Epoch: 062 train_loss: 0.0331 train_acc: 99.5616 Precision: 0.7815 Recall: 0.6367 F1-Score: 0.7017\n",
      "akk: 789, gt: 115772\n",
      "Epoch: 063 train_loss: 0.0330 train_acc: 99.5668 Precision: 0.7782 Recall: 0.6504 F1-Score: 0.7086\n",
      "akk: 774, gt: 115787\n",
      "Epoch: 064 train_loss: 0.0331 train_acc: 99.5556 Precision: 0.7752 Recall: 0.6356 F1-Score: 0.6985\n",
      "akk: 781, gt: 115780\n",
      "Epoch: 065 train_loss: 0.0330 train_acc: 99.5565 Precision: 0.7734 Recall: 0.6398 F1-Score: 0.7003\n",
      "akk: 781, gt: 115780\n",
      "Epoch: 066 train_loss: 0.0329 train_acc: 99.5685 Precision: 0.7823 Recall: 0.6472 F1-Score: 0.7084\n",
      "akk: 768, gt: 115793\n",
      "Epoch: 067 train_loss: 0.0329 train_acc: 99.5539 Precision: 0.7760 Recall: 0.6314 F1-Score: 0.6963\n",
      "akk: 785, gt: 115776\n",
      "Epoch: 068 train_loss: 0.0328 train_acc: 99.5685 Precision: 0.7809 Recall: 0.6494 F1-Score: 0.7091\n",
      "akk: 783, gt: 115778\n",
      "Epoch: 069 train_loss: 0.0328 train_acc: 99.5633 Precision: 0.7778 Recall: 0.6451 F1-Score: 0.7053\n",
      "akk: 789, gt: 115772\n",
      "Epoch: 070 train_loss: 0.0326 train_acc: 99.5702 Precision: 0.7807 Recall: 0.6525 F1-Score: 0.7109\n",
      "akk: 781, gt: 115780\n",
      "Epoch: 071 train_loss: 0.0330 train_acc: 99.5616 Precision: 0.7772 Recall: 0.6430 F1-Score: 0.7038\n",
      "akk: 23784, gt: 3745002\n",
      "OA: 0.9954 AA: 0.8017 Kappa: 0.6775 Precision: 0.7760 Recall: 0.6048 F1-Score: 0.6798\n",
      "*************************\n",
      "akk: 749, gt: 115812\n",
      "Epoch: 072 train_loss: 0.0329 train_acc: 99.5805 Precision: 0.8037 Recall: 0.6377 F1-Score: 0.7112\n",
      "akk: 786, gt: 115775\n",
      "Epoch: 073 train_loss: 0.0327 train_acc: 99.5659 Precision: 0.7786 Recall: 0.6483 F1-Score: 0.7075\n",
      "akk: 799, gt: 115762\n",
      "Epoch: 074 train_loss: 0.0328 train_acc: 99.5547 Precision: 0.7660 Recall: 0.6483 F1-Score: 0.7022\n",
      "akk: 784, gt: 115777\n",
      "Epoch: 075 train_loss: 0.0326 train_acc: 99.5625 Precision: 0.7768 Recall: 0.6451 F1-Score: 0.7049\n",
      "akk: 791, gt: 115770\n",
      "Epoch: 076 train_loss: 0.0328 train_acc: 99.5462 Precision: 0.7623 Recall: 0.6388 F1-Score: 0.6951\n",
      "akk: 785, gt: 115776\n",
      "Epoch: 077 train_loss: 0.0329 train_acc: 99.5444 Precision: 0.7631 Recall: 0.6345 F1-Score: 0.6929\n",
      "akk: 783, gt: 115778\n",
      "Epoch: 078 train_loss: 0.0328 train_acc: 99.5839 Precision: 0.7931 Recall: 0.6578 F1-Score: 0.7192\n",
      "akk: 800, gt: 115761\n",
      "Epoch: 079 train_loss: 0.0327 train_acc: 99.5899 Precision: 0.7913 Recall: 0.6706 F1-Score: 0.7259\n",
      "akk: 802, gt: 115759\n",
      "Epoch: 080 train_loss: 0.0327 train_acc: 99.5831 Precision: 0.7855 Recall: 0.6674 F1-Score: 0.7216\n",
      "akk: 776, gt: 115785\n",
      "Epoch: 081 train_loss: 0.0325 train_acc: 99.5779 Precision: 0.7912 Recall: 0.6504 F1-Score: 0.7140\n",
      "akk: 29010, gt: 3739776\n",
      "OA: 0.9947 AA: 0.8230 Kappa: 0.6622 Precision: 0.6821 Recall: 0.6485 F1-Score: 0.6649\n",
      "*************************\n",
      "akk: 801, gt: 115760\n",
      "Epoch: 082 train_loss: 0.0324 train_acc: 99.5702 Precision: 0.7765 Recall: 0.6589 F1-Score: 0.7129\n",
      "akk: 823, gt: 115738\n",
      "Epoch: 083 train_loss: 0.0325 train_acc: 99.5839 Precision: 0.7789 Recall: 0.6790 F1-Score: 0.7255\n",
      "akk: 792, gt: 115769\n",
      "Epoch: 084 train_loss: 0.0328 train_acc: 99.5779 Precision: 0.7854 Recall: 0.6589 F1-Score: 0.7166\n",
      "akk: 778, gt: 115783\n",
      "Epoch: 085 train_loss: 0.0325 train_acc: 99.5779 Precision: 0.7905 Recall: 0.6515 F1-Score: 0.7143\n",
      "akk: 800, gt: 115761\n",
      "Epoch: 086 train_loss: 0.0325 train_acc: 99.5848 Precision: 0.7875 Recall: 0.6674 F1-Score: 0.7225\n",
      "akk: 820, gt: 115741\n",
      "Epoch: 087 train_loss: 0.0324 train_acc: 99.5779 Precision: 0.7756 Recall: 0.6737 F1-Score: 0.7211\n",
      "akk: 803, gt: 115758\n",
      "Epoch: 088 train_loss: 0.0325 train_acc: 99.5668 Precision: 0.7733 Recall: 0.6578 F1-Score: 0.7109\n",
      "akk: 811, gt: 115750\n",
      "Epoch: 089 train_loss: 0.0325 train_acc: 99.5925 Precision: 0.7891 Recall: 0.6780 F1-Score: 0.7293\n",
      "akk: 795, gt: 115766\n",
      "Epoch: 090 train_loss: 0.0325 train_acc: 99.5942 Precision: 0.7962 Recall: 0.6706 F1-Score: 0.7280\n",
      "akk: 796, gt: 115765\n",
      "Epoch: 091 train_loss: 0.0322 train_acc: 99.6002 Precision: 0.8003 Recall: 0.6748 F1-Score: 0.7322\n",
      "akk: 16308, gt: 3752478\n",
      "OA: 0.9949 AA: 0.7262 Kappa: 0.5882 Precision: 0.8477 Recall: 0.4531 F1-Score: 0.5905\n",
      "*************************\n",
      "akk: 785, gt: 115776\n",
      "Epoch: 092 train_loss: 0.0324 train_acc: 99.5839 Precision: 0.7924 Recall: 0.6589 F1-Score: 0.7195\n",
      "akk: 802, gt: 115759\n",
      "Epoch: 093 train_loss: 0.0325 train_acc: 99.5848 Precision: 0.7868 Recall: 0.6684 F1-Score: 0.7228\n",
      "akk: 802, gt: 115759\n",
      "Epoch: 094 train_loss: 0.0324 train_acc: 99.5659 Precision: 0.7731 Recall: 0.6568 F1-Score: 0.7102\n",
      "akk: 797, gt: 115764\n",
      "Epoch: 095 train_loss: 0.0322 train_acc: 99.5891 Precision: 0.7917 Recall: 0.6684 F1-Score: 0.7249\n",
      "akk: 772, gt: 115789\n",
      "Epoch: 096 train_loss: 0.0322 train_acc: 99.5831 Precision: 0.7966 Recall: 0.6515 F1-Score: 0.7168\n",
      "akk: 787, gt: 115774\n",
      "Epoch: 097 train_loss: 0.0322 train_acc: 99.5976 Precision: 0.8018 Recall: 0.6684 F1-Score: 0.7291\n",
      "akk: 812, gt: 115749\n",
      "Epoch: 098 train_loss: 0.0321 train_acc: 99.5865 Precision: 0.7845 Recall: 0.6748 F1-Score: 0.7255\n",
      "akk: 788, gt: 115773\n",
      "Epoch: 099 train_loss: 0.0321 train_acc: 99.6054 Precision: 0.8071 Recall: 0.6737 F1-Score: 0.7344\n",
      "akk: 810, gt: 115751\n",
      "Epoch: 100 train_loss: 0.0320 train_acc: 99.5899 Precision: 0.7877 Recall: 0.6758 F1-Score: 0.7275\n",
      "akk: 33590, gt: 3735196\n",
      "OA: 0.9948 AA: 0.8639 Kappa: 0.6932 Precision: 0.6640 Recall: 0.7309 F1-Score: 0.6958\n",
      "*************************\n",
      "Running Time: 2978.93\n",
      "**************************************************\n",
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [2:44:43<6:26:25, 3312.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "F1 score: 0.6908883792736484\n",
      "Precision: 0.6536672528220546\n",
      "Recall: 0.7326043421596363\n",
      "max_label: 1 max_pred: 1\n",
      "Accuracy: 0.9958784870132271\n",
      "Confusion Matrix:\n",
      "[[4960075   12211]\n",
      " [   8412   23047]]\n",
      "**************************************************\n",
      "Parameter:\n",
      "fix_random: True\n",
      "gpu_id: 0\n",
      "seed: 0\n",
      "dataset: BS\n",
      "flag_test: train\n",
      "model_name: conv2d_unmix\n",
      "batch_size: 128\n",
      "test_freq: 10\n",
      "patches: 5\n",
      "epoches: 100\n",
      "inference_model: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "learning_rate: 0.0001\n",
      "gamma: 0.9\n",
      "weight_decay: 0\n",
      "Train data change to the ratio of train samples: 0.03\n",
      "height=2715,width=1843,band=150\n",
      "**************************************************\n",
      "patch is : 5\n",
      "mirror_image shape : [2719,1847,150]\n",
      "**************************************************\n",
      "x_train shape = (116561, 5, 5, 150), type = float32\n",
      "x_test  shape = (3768786, 5, 5, 150), type = float32\n",
      "x_true  shape = (5003745, 5, 5, 150), type = float32\n",
      "**************************************************\n",
      "y_train: shape = (116561,) ,type = int64\n",
      "y_test: shape = (3768786,) ,type = int64\n",
      "y_true: shape = (5003745,) ,type = int64\n",
      "**************************************************\n",
      "Model Name: conv2d_unmix\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akk: 5766, gt: 110795\n",
      "Epoch: 001 train_loss: 3.1593 train_acc: 94.4836 Precision: 0.0243 Recall: 0.1483 F1-Score: 0.0417\n",
      "akk: 187, gt: 3768599\n",
      "OA: 0.9919 AA: 0.5000 Kappa: -0.0000 Precision: 0.0053 Recall: 0.0000 F1-Score: 0.0001\n",
      "*************************\n",
      "akk: 3293, gt: 113268\n",
      "Epoch: 002 train_loss: 0.1408 train_acc: 96.7270 Precision: 0.0641 Recall: 0.2235 F1-Score: 0.0996\n",
      "akk: 1885, gt: 114676\n",
      "Epoch: 003 train_loss: 0.0897 train_acc: 97.9710 Precision: 0.1231 Recall: 0.2458 F1-Score: 0.1640\n",
      "akk: 1294, gt: 115267\n",
      "Epoch: 004 train_loss: 0.0648 train_acc: 98.5398 Precision: 0.2071 Recall: 0.2839 F1-Score: 0.2395\n",
      "akk: 1029, gt: 115532\n",
      "Epoch: 005 train_loss: 0.0577 train_acc: 98.7517 Precision: 0.2517 Recall: 0.2744 F1-Score: 0.2625\n",
      "akk: 1007, gt: 115554\n",
      "Epoch: 006 train_loss: 0.0555 train_acc: 98.7552 Precision: 0.2483 Recall: 0.2648 F1-Score: 0.2563\n",
      "akk: 646, gt: 115915\n",
      "Epoch: 007 train_loss: 0.0454 train_acc: 99.1901 Precision: 0.5000 Recall: 0.3422 F1-Score: 0.4063\n",
      "akk: 278, gt: 116283\n",
      "Epoch: 008 train_loss: 0.0436 train_acc: 99.2073 Precision: 0.5360 Recall: 0.1578 F1-Score: 0.2439\n",
      "akk: 7, gt: 116554\n",
      "Epoch: 009 train_loss: 0.0412 train_acc: 99.1893 Precision: 0.4286 Recall: 0.0032 F1-Score: 0.0063\n",
      "akk: 2, gt: 116559\n",
      "Epoch: 010 train_loss: 0.0405 train_acc: 99.1918 Precision: 1.0000 Recall: 0.0021 F1-Score: 0.0042\n",
      "akk: 2, gt: 116559\n",
      "Epoch: 011 train_loss: 0.0393 train_acc: 99.1901 Precision: 0.5000 Recall: 0.0011 F1-Score: 0.0021\n",
      "akk: 79, gt: 3768707\n",
      "OA: 0.9919 AA: 0.5004 Kappa: 0.0015 Precision: 0.2911 Recall: 0.0008 F1-Score: 0.0015\n",
      "*************************\n",
      "akk: 3, gt: 116558\n",
      "Epoch: 012 train_loss: 0.0392 train_acc: 99.1927 Precision: 1.0000 Recall: 0.0032 F1-Score: 0.0063\n",
      "akk: 153, gt: 116408\n",
      "Epoch: 013 train_loss: 0.0418 train_acc: 99.1275 Precision: 0.2614 Recall: 0.0424 F1-Score: 0.0729\n",
      "akk: 95, gt: 116466\n",
      "Epoch: 014 train_loss: 0.0388 train_acc: 99.2528 Precision: 0.8842 Recall: 0.0890 F1-Score: 0.1617\n",
      "akk: 143, gt: 116418\n",
      "Epoch: 015 train_loss: 0.0383 train_acc: 99.2751 Precision: 0.8462 Recall: 0.1282 F1-Score: 0.2226\n",
      "akk: 213, gt: 116348\n",
      "Epoch: 016 train_loss: 0.0385 train_acc: 99.3111 Precision: 0.8310 Recall: 0.1875 F1-Score: 0.3060\n",
      "akk: 267, gt: 116294\n",
      "Epoch: 017 train_loss: 0.0383 train_acc: 99.3162 Precision: 0.7753 Recall: 0.2193 F1-Score: 0.3419\n",
      "akk: 279, gt: 116282\n",
      "Epoch: 018 train_loss: 0.0377 train_acc: 99.3231 Precision: 0.7778 Recall: 0.2299 F1-Score: 0.3549\n",
      "akk: 307, gt: 116254\n",
      "Epoch: 019 train_loss: 0.0373 train_acc: 99.3540 Precision: 0.8111 Recall: 0.2638 F1-Score: 0.3981\n",
      "akk: 348, gt: 116213\n",
      "Epoch: 020 train_loss: 0.0372 train_acc: 99.3840 Precision: 0.8247 Recall: 0.3040 F1-Score: 0.4443\n",
      "akk: 389, gt: 116172\n",
      "Epoch: 021 train_loss: 0.0368 train_acc: 99.3935 Precision: 0.8046 Recall: 0.3316 F1-Score: 0.4696\n",
      "akk: 4004, gt: 3764782\n",
      "OA: 0.9926 AA: 0.5533 Kappa: 0.1874 Precision: 0.8144 Recall: 0.1069 F1-Score: 0.1889\n",
      "*************************\n",
      "akk: 425, gt: 116136\n",
      "Epoch: 022 train_loss: 0.0369 train_acc: 99.4072 Precision: 0.7976 Recall: 0.3591 F1-Score: 0.4953\n",
      "akk: 443, gt: 116118\n",
      "Epoch: 023 train_loss: 0.0371 train_acc: 99.3986 Precision: 0.7743 Recall: 0.3633 F1-Score: 0.4946\n",
      "akk: 476, gt: 116085\n",
      "Epoch: 024 train_loss: 0.0370 train_acc: 99.4115 Precision: 0.7710 Recall: 0.3888 F1-Score: 0.5169\n",
      "akk: 495, gt: 116066\n",
      "Epoch: 025 train_loss: 0.0366 train_acc: 99.4363 Precision: 0.7899 Recall: 0.4142 F1-Score: 0.5434\n",
      "akk: 486, gt: 116075\n",
      "Epoch: 026 train_loss: 0.0363 train_acc: 99.4338 Precision: 0.7922 Recall: 0.4078 F1-Score: 0.5385\n",
      "akk: 536, gt: 116025\n",
      "Epoch: 027 train_loss: 0.0361 train_acc: 99.4389 Precision: 0.7705 Recall: 0.4375 F1-Score: 0.5581\n",
      "akk: 496, gt: 116065\n",
      "Epoch: 028 train_loss: 0.0364 train_acc: 99.4149 Precision: 0.7641 Recall: 0.4015 F1-Score: 0.5264\n",
      "akk: 543, gt: 116018\n",
      "Epoch: 029 train_loss: 0.0358 train_acc: 99.4432 Precision: 0.7716 Recall: 0.4439 F1-Score: 0.5636\n",
      "akk: 540, gt: 116021\n",
      "Epoch: 030 train_loss: 0.0357 train_acc: 99.4441 Precision: 0.7741 Recall: 0.4428 F1-Score: 0.5633\n",
      "akk: 574, gt: 115987\n",
      "Epoch: 031 train_loss: 0.0355 train_acc: 99.4544 Precision: 0.7683 Recall: 0.4672 F1-Score: 0.5810\n",
      "akk: 26526, gt: 3742260\n",
      "OA: 0.9941 AA: 0.7825 Kappa: 0.6041 Precision: 0.6527 Recall: 0.5674 F1-Score: 0.6071\n",
      "*************************\n",
      "akk: 559, gt: 116002\n",
      "Epoch: 032 train_loss: 0.0352 train_acc: 99.4878 Precision: 0.8104 Recall: 0.4799 F1-Score: 0.6028\n",
      "akk: 529, gt: 116032\n",
      "Epoch: 033 train_loss: 0.0356 train_acc: 99.4415 Precision: 0.7769 Recall: 0.4354 F1-Score: 0.5580\n",
      "akk: 538, gt: 116023\n",
      "Epoch: 034 train_loss: 0.0357 train_acc: 99.4492 Precision: 0.7807 Recall: 0.4449 F1-Score: 0.5668\n",
      "akk: 566, gt: 115995\n",
      "Epoch: 035 train_loss: 0.0352 train_acc: 99.4578 Precision: 0.7756 Recall: 0.4650 F1-Score: 0.5815\n",
      "akk: 579, gt: 115982\n",
      "Epoch: 036 train_loss: 0.0353 train_acc: 99.4381 Precision: 0.7496 Recall: 0.4597 F1-Score: 0.5699\n",
      "akk: 571, gt: 115990\n",
      "Epoch: 037 train_loss: 0.0353 train_acc: 99.4484 Precision: 0.7636 Recall: 0.4619 F1-Score: 0.5756\n",
      "akk: 552, gt: 116009\n",
      "Epoch: 038 train_loss: 0.0351 train_acc: 99.4492 Precision: 0.7736 Recall: 0.4523 F1-Score: 0.5709\n",
      "akk: 608, gt: 115953\n",
      "Epoch: 039 train_loss: 0.0350 train_acc: 99.4681 Precision: 0.7664 Recall: 0.4936 F1-Score: 0.6005\n",
      "akk: 588, gt: 115973\n",
      "Epoch: 040 train_loss: 0.0347 train_acc: 99.4750 Precision: 0.7823 Recall: 0.4873 F1-Score: 0.6005\n",
      "akk: 593, gt: 115968\n",
      "Epoch: 041 train_loss: 0.0346 train_acc: 99.4810 Precision: 0.7858 Recall: 0.4936 F1-Score: 0.6064\n",
      "akk: 11658, gt: 3757128\n",
      "OA: 0.9941 AA: 0.6617 Kappa: 0.4663 Precision: 0.8478 Recall: 0.3239 F1-Score: 0.4687\n",
      "*************************\n",
      "akk: 609, gt: 115952\n",
      "Epoch: 042 train_loss: 0.0349 train_acc: 99.4689 Precision: 0.7668 Recall: 0.4947 F1-Score: 0.6014\n",
      "akk: 602, gt: 115959\n",
      "Epoch: 043 train_loss: 0.0349 train_acc: 99.4612 Precision: 0.7625 Recall: 0.4862 F1-Score: 0.5938\n",
      "akk: 609, gt: 115952\n",
      "Epoch: 044 train_loss: 0.0348 train_acc: 99.4775 Precision: 0.7750 Recall: 0.5000 F1-Score: 0.6079\n",
      "akk: 620, gt: 115941\n",
      "Epoch: 045 train_loss: 0.0351 train_acc: 99.4835 Precision: 0.7758 Recall: 0.5095 F1-Score: 0.6151\n",
      "akk: 572, gt: 115989\n",
      "Epoch: 046 train_loss: 0.0347 train_acc: 99.4647 Precision: 0.7797 Recall: 0.4725 F1-Score: 0.5884\n",
      "akk: 608, gt: 115953\n",
      "Epoch: 047 train_loss: 0.0345 train_acc: 99.4870 Precision: 0.7845 Recall: 0.5053 F1-Score: 0.6147\n",
      "akk: 644, gt: 115917\n",
      "Epoch: 048 train_loss: 0.0343 train_acc: 99.5093 Precision: 0.7888 Recall: 0.5381 F1-Score: 0.6398\n",
      "akk: 594, gt: 115967\n",
      "Epoch: 049 train_loss: 0.0344 train_acc: 99.4921 Precision: 0.7963 Recall: 0.5011 F1-Score: 0.6151\n",
      "akk: 613, gt: 115948\n",
      "Epoch: 050 train_loss: 0.0345 train_acc: 99.4810 Precision: 0.7765 Recall: 0.5042 F1-Score: 0.6114\n",
      "akk: 641, gt: 115920\n",
      "Epoch: 051 train_loss: 0.0344 train_acc: 99.4775 Precision: 0.7613 Recall: 0.5169 F1-Score: 0.6158\n",
      "akk: 25844, gt: 3742942\n",
      "OA: 0.9948 AA: 0.8008 Kappa: 0.6510 Precision: 0.7127 Recall: 0.6036 F1-Score: 0.6536\n",
      "*************************\n",
      "akk: 625, gt: 115936\n",
      "Epoch: 052 train_loss: 0.0345 train_acc: 99.4638 Precision: 0.7552 Recall: 0.5000 F1-Score: 0.6017\n",
      "akk: 634, gt: 115927\n",
      "Epoch: 053 train_loss: 0.0343 train_acc: 99.5041 Precision: 0.7886 Recall: 0.5297 F1-Score: 0.6337\n",
      "akk: 635, gt: 115926\n",
      "Epoch: 054 train_loss: 0.0342 train_acc: 99.4964 Precision: 0.7811 Recall: 0.5254 F1-Score: 0.6282\n",
      "akk: 634, gt: 115927\n",
      "Epoch: 055 train_loss: 0.0340 train_acc: 99.5024 Precision: 0.7871 Recall: 0.5286 F1-Score: 0.6324\n",
      "akk: 605, gt: 115956\n",
      "Epoch: 056 train_loss: 0.0343 train_acc: 99.4724 Precision: 0.7719 Recall: 0.4947 F1-Score: 0.6030\n",
      "akk: 609, gt: 115952\n",
      "Epoch: 057 train_loss: 0.0342 train_acc: 99.4947 Precision: 0.7915 Recall: 0.5106 F1-Score: 0.6207\n",
      "akk: 653, gt: 115908\n",
      "Epoch: 058 train_loss: 0.0341 train_acc: 99.4895 Precision: 0.7672 Recall: 0.5307 F1-Score: 0.6274\n",
      "akk: 615, gt: 115946\n",
      "Epoch: 059 train_loss: 0.0342 train_acc: 99.4518 Precision: 0.7480 Recall: 0.4873 F1-Score: 0.5901\n",
      "akk: 642, gt: 115919\n",
      "Epoch: 060 train_loss: 0.0340 train_acc: 99.4887 Precision: 0.7710 Recall: 0.5244 F1-Score: 0.6242\n",
      "akk: 644, gt: 115917\n",
      "Epoch: 061 train_loss: 0.0339 train_acc: 99.4955 Precision: 0.7764 Recall: 0.5297 F1-Score: 0.6297\n",
      "akk: 16575, gt: 3752211\n",
      "OA: 0.9946 AA: 0.7189 Kappa: 0.5660 Precision: 0.8075 Recall: 0.4386 F1-Score: 0.5685\n",
      "*************************\n",
      "akk: 615, gt: 115946\n",
      "Epoch: 062 train_loss: 0.0338 train_acc: 99.4964 Precision: 0.7902 Recall: 0.5148 F1-Score: 0.6235\n",
      "akk: 624, gt: 115937\n",
      "Epoch: 063 train_loss: 0.0341 train_acc: 99.5058 Precision: 0.7949 Recall: 0.5254 F1-Score: 0.6327\n",
      "akk: 623, gt: 115938\n",
      "Epoch: 064 train_loss: 0.0338 train_acc: 99.4913 Precision: 0.7817 Recall: 0.5159 F1-Score: 0.6216\n",
      "akk: 648, gt: 115913\n",
      "Epoch: 065 train_loss: 0.0339 train_acc: 99.4750 Precision: 0.7562 Recall: 0.5191 F1-Score: 0.6156\n",
      "akk: 653, gt: 115908\n",
      "Epoch: 066 train_loss: 0.0340 train_acc: 99.5033 Precision: 0.7795 Recall: 0.5392 F1-Score: 0.6374\n",
      "akk: 656, gt: 115905\n",
      "Epoch: 067 train_loss: 0.0338 train_acc: 99.4990 Precision: 0.7744 Recall: 0.5381 F1-Score: 0.6350\n",
      "akk: 609, gt: 115952\n",
      "Epoch: 068 train_loss: 0.0339 train_acc: 99.5101 Precision: 0.8062 Recall: 0.5201 F1-Score: 0.6323\n",
      "akk: 673, gt: 115888\n",
      "Epoch: 069 train_loss: 0.0336 train_acc: 99.5084 Precision: 0.7756 Recall: 0.5530 F1-Score: 0.6456\n",
      "akk: 671, gt: 115890\n",
      "Epoch: 070 train_loss: 0.0337 train_acc: 99.5170 Precision: 0.7839 Recall: 0.5572 F1-Score: 0.6514\n",
      "akk: 654, gt: 115907\n",
      "Epoch: 071 train_loss: 0.0334 train_acc: 99.5299 Precision: 0.8028 Recall: 0.5561 F1-Score: 0.6571\n",
      "akk: 19936, gt: 3748850\n",
      "OA: 0.9949 AA: 0.7545 Kappa: 0.6147 Precision: 0.7809 Recall: 0.5102 F1-Score: 0.6172\n",
      "*************************\n",
      "akk: 686, gt: 115875\n",
      "Epoch: 072 train_loss: 0.0337 train_acc: 99.5127 Precision: 0.7741 Recall: 0.5625 F1-Score: 0.6515\n",
      "akk: 648, gt: 115913\n",
      "Epoch: 073 train_loss: 0.0334 train_acc: 99.5299 Precision: 0.8056 Recall: 0.5530 F1-Score: 0.6558\n",
      "akk: 651, gt: 115910\n",
      "Epoch: 074 train_loss: 0.0335 train_acc: 99.5118 Precision: 0.7880 Recall: 0.5434 F1-Score: 0.6433\n",
      "akk: 637, gt: 115924\n",
      "Epoch: 075 train_loss: 0.0337 train_acc: 99.5033 Precision: 0.7865 Recall: 0.5307 F1-Score: 0.6338\n",
      "akk: 668, gt: 115893\n",
      "Epoch: 076 train_loss: 0.0334 train_acc: 99.5076 Precision: 0.7769 Recall: 0.5498 F1-Score: 0.6439\n",
      "akk: 635, gt: 115926\n",
      "Epoch: 077 train_loss: 0.0335 train_acc: 99.5153 Precision: 0.7984 Recall: 0.5371 F1-Score: 0.6422\n",
      "akk: 667, gt: 115894\n",
      "Epoch: 078 train_loss: 0.0336 train_acc: 99.4998 Precision: 0.7706 Recall: 0.5445 F1-Score: 0.6381\n",
      "akk: 679, gt: 115882\n",
      "Epoch: 079 train_loss: 0.0333 train_acc: 99.5170 Precision: 0.7806 Recall: 0.5614 F1-Score: 0.6531\n",
      "akk: 675, gt: 115886\n",
      "Epoch: 080 train_loss: 0.0336 train_acc: 99.5170 Precision: 0.7822 Recall: 0.5593 F1-Score: 0.6523\n",
      "akk: 659, gt: 115902\n",
      "Epoch: 081 train_loss: 0.0332 train_acc: 99.5239 Precision: 0.7951 Recall: 0.5551 F1-Score: 0.6538\n",
      "akk: 22919, gt: 3745867\n",
      "OA: 0.9950 AA: 0.7841 Kappa: 0.6483 Precision: 0.7585 Recall: 0.5697 F1-Score: 0.6507\n",
      "*************************\n",
      "akk: 654, gt: 115907\n",
      "Epoch: 082 train_loss: 0.0336 train_acc: 99.5041 Precision: 0.7798 Recall: 0.5403 F1-Score: 0.6383\n",
      "akk: 679, gt: 115882\n",
      "Epoch: 083 train_loss: 0.0332 train_acc: 99.5239 Precision: 0.7865 Recall: 0.5657 F1-Score: 0.6580\n",
      "akk: 620, gt: 115941\n",
      "Epoch: 084 train_loss: 0.0333 train_acc: 99.5178 Precision: 0.8081 Recall: 0.5307 F1-Score: 0.6407\n",
      "akk: 678, gt: 115883\n",
      "Epoch: 085 train_loss: 0.0330 train_acc: 99.5196 Precision: 0.7832 Recall: 0.5625 F1-Score: 0.6547\n",
      "akk: 696, gt: 115865\n",
      "Epoch: 086 train_loss: 0.0334 train_acc: 99.4818 Precision: 0.7443 Recall: 0.5487 F1-Score: 0.6317\n",
      "akk: 672, gt: 115889\n",
      "Epoch: 087 train_loss: 0.0331 train_acc: 99.5264 Precision: 0.7917 Recall: 0.5636 F1-Score: 0.6584\n",
      "akk: 671, gt: 115890\n",
      "Epoch: 088 train_loss: 0.0335 train_acc: 99.5170 Precision: 0.7839 Recall: 0.5572 F1-Score: 0.6514\n",
      "akk: 679, gt: 115882\n",
      "Epoch: 089 train_loss: 0.0334 train_acc: 99.5101 Precision: 0.7747 Recall: 0.5572 F1-Score: 0.6482\n",
      "akk: 664, gt: 115897\n",
      "Epoch: 090 train_loss: 0.0331 train_acc: 99.5213 Precision: 0.7907 Recall: 0.5561 F1-Score: 0.6530\n",
      "akk: 695, gt: 115866\n",
      "Epoch: 091 train_loss: 0.0328 train_acc: 99.5359 Precision: 0.7899 Recall: 0.5816 F1-Score: 0.6699\n",
      "akk: 22304, gt: 3746482\n",
      "OA: 0.9950 AA: 0.7789 Kappa: 0.6437 Precision: 0.7650 Recall: 0.5592 F1-Score: 0.6461\n",
      "*************************\n",
      "akk: 673, gt: 115888\n",
      "Epoch: 092 train_loss: 0.0329 train_acc: 99.5273 Precision: 0.7920 Recall: 0.5646 F1-Score: 0.6592\n",
      "akk: 687, gt: 115874\n",
      "Epoch: 093 train_loss: 0.0328 train_acc: 99.5307 Precision: 0.7889 Recall: 0.5742 F1-Score: 0.6646\n",
      "akk: 692, gt: 115869\n",
      "Epoch: 094 train_loss: 0.0329 train_acc: 99.5281 Precision: 0.7847 Recall: 0.5752 F1-Score: 0.6638\n",
      "akk: 671, gt: 115890\n",
      "Epoch: 095 train_loss: 0.0329 train_acc: 99.5084 Precision: 0.7765 Recall: 0.5519 F1-Score: 0.6452\n",
      "akk: 665, gt: 115896\n",
      "Epoch: 096 train_loss: 0.0329 train_acc: 99.5256 Precision: 0.7940 Recall: 0.5593 F1-Score: 0.6563\n",
      "akk: 693, gt: 115868\n",
      "Epoch: 097 train_loss: 0.0328 train_acc: 99.5341 Precision: 0.7893 Recall: 0.5794 F1-Score: 0.6683\n",
      "akk: 658, gt: 115903\n",
      "Epoch: 098 train_loss: 0.0328 train_acc: 99.5384 Precision: 0.8085 Recall: 0.5636 F1-Score: 0.6642\n",
      "akk: 674, gt: 115887\n",
      "Epoch: 099 train_loss: 0.0328 train_acc: 99.5350 Precision: 0.7982 Recall: 0.5699 F1-Score: 0.6650\n",
      "akk: 697, gt: 115864\n",
      "Epoch: 100 train_loss: 0.0327 train_acc: 99.5341 Precision: 0.7877 Recall: 0.5816 F1-Score: 0.6691\n",
      "akk: 22627, gt: 3746159\n",
      "OA: 0.9951 AA: 0.7828 Kappa: 0.6488 Precision: 0.7648 Recall: 0.5671 F1-Score: 0.6512\n",
      "*************************\n",
      "Running Time: 2909.38\n",
      "**************************************************\n",
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [3:39:01<5:29:06, 3291.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "F1 score: 0.6369492490568723\n",
      "Precision: 0.723491126652383\n",
      "Recall: 0.5688992021361137\n",
      "max_label: 1 max_pred: 1\n",
      "Accuracy: 0.9959226539322048\n",
      "Confusion Matrix:\n",
      "[[4965446    6840]\n",
      " [  13562   17897]]\n",
      "**************************************************\n",
      "Parameter:\n",
      "fix_random: True\n",
      "gpu_id: 0\n",
      "seed: 0\n",
      "dataset: BS\n",
      "flag_test: train\n",
      "model_name: conv2d_unmix\n",
      "batch_size: 128\n",
      "test_freq: 10\n",
      "patches: 5\n",
      "epoches: 100\n",
      "inference_model: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "learning_rate: 0.0001\n",
      "gamma: 0.9\n",
      "weight_decay: 0\n",
      "Train data change to the ratio of train samples: 0.03\n",
      "height=2715,width=1843,band=150\n",
      "**************************************************\n",
      "patch is : 5\n",
      "mirror_image shape : [2719,1847,150]\n",
      "**************************************************\n",
      "x_train shape = (116561, 5, 5, 150), type = float32\n",
      "x_test  shape = (3768786, 5, 5, 150), type = float32\n",
      "x_true  shape = (5003745, 5, 5, 150), type = float32\n",
      "**************************************************\n",
      "y_train: shape = (116561,) ,type = int64\n",
      "y_test: shape = (3768786,) ,type = int64\n",
      "y_true: shape = (5003745,) ,type = int64\n",
      "**************************************************\n",
      "Model Name: conv2d_unmix\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akk: 8027, gt: 108534\n",
      "Epoch: 001 train_loss: 3.3251 train_acc: 92.6828 Precision: 0.0275 Recall: 0.2341 F1-Score: 0.0493\n",
      "akk: 9, gt: 3768777\n",
      "OA: 0.9919 AA: 0.5000 Kappa: -0.0000 Precision: 0.0000 Recall: 0.0000 F1-Score: 0.0000\n",
      "*************************\n",
      "akk: 5879, gt: 110682\n",
      "Epoch: 002 train_loss: 1.0379 train_acc: 94.6303 Precision: 0.0480 Recall: 0.2987 F1-Score: 0.0827\n",
      "akk: 3571, gt: 112990\n",
      "Epoch: 003 train_loss: 0.3589 train_acc: 96.5503 Precision: 0.0692 Recall: 0.2617 F1-Score: 0.1094\n",
      "akk: 2223, gt: 114338\n",
      "Epoch: 004 train_loss: 0.1370 train_acc: 97.7308 Precision: 0.1174 Recall: 0.2765 F1-Score: 0.1648\n",
      "akk: 1547, gt: 115014\n",
      "Epoch: 005 train_loss: 0.0836 train_acc: 98.3674 Precision: 0.1900 Recall: 0.3114 F1-Score: 0.2360\n",
      "akk: 1171, gt: 115390\n",
      "Epoch: 006 train_loss: 0.0719 train_acc: 98.5184 Precision: 0.1657 Recall: 0.2055 F1-Score: 0.1835\n",
      "akk: 45, gt: 116516\n",
      "Epoch: 007 train_loss: 0.0530 train_acc: 99.1515 Precision: 0.0000 Recall: 0.0000 F1-Score: 0.0000\n",
      "akk: 1, gt: 116560\n",
      "Epoch: 008 train_loss: 0.0433 train_acc: 99.1893 Precision: 0.0000 Recall: 0.0000 F1-Score: 0.0000\n",
      "akk: 0, gt: 116561\n",
      "Epoch: 009 train_loss: 0.0430 train_acc: 99.1901 Precision: 1.0000 Recall: 0.0000 F1-Score: 0.0000\n",
      "akk: 0, gt: 116561\n",
      "Epoch: 010 train_loss: 0.0410 train_acc: 99.1901 Precision: 1.0000 Recall: 0.0000 F1-Score: 0.0000\n",
      "akk: 4, gt: 116557\n",
      "Epoch: 011 train_loss: 0.0410 train_acc: 99.1884 Precision: 0.2500 Recall: 0.0011 F1-Score: 0.0021\n",
      "akk: 21, gt: 3768765\n",
      "OA: 0.9919 AA: 0.5001 Kappa: 0.0005 Precision: 0.3810 Recall: 0.0003 F1-Score: 0.0005\n",
      "*************************\n",
      "akk: 2, gt: 116559\n",
      "Epoch: 012 train_loss: 0.0420 train_acc: 99.1901 Precision: 0.5000 Recall: 0.0011 F1-Score: 0.0021\n",
      "akk: 3, gt: 116558\n",
      "Epoch: 013 train_loss: 0.0406 train_acc: 99.1910 Precision: 0.6667 Recall: 0.0021 F1-Score: 0.0042\n",
      "akk: 7, gt: 116554\n",
      "Epoch: 014 train_loss: 0.0393 train_acc: 99.1944 Precision: 0.8571 Recall: 0.0064 F1-Score: 0.0126\n",
      "akk: 24, gt: 116537\n",
      "Epoch: 015 train_loss: 0.0399 train_acc: 99.2004 Precision: 0.7500 Recall: 0.0191 F1-Score: 0.0372\n",
      "akk: 334, gt: 116227\n",
      "Epoch: 016 train_loss: 0.0390 train_acc: 99.3669 Precision: 0.8084 Recall: 0.2860 F1-Score: 0.4225\n",
      "akk: 368, gt: 116193\n",
      "Epoch: 017 train_loss: 0.0391 train_acc: 99.3291 Precision: 0.7201 Recall: 0.2807 F1-Score: 0.4040\n",
      "akk: 336, gt: 116225\n",
      "Epoch: 018 train_loss: 0.0393 train_acc: 99.3497 Precision: 0.7768 Recall: 0.2765 F1-Score: 0.4078\n",
      "akk: 374, gt: 116187\n",
      "Epoch: 019 train_loss: 0.0390 train_acc: 99.3531 Precision: 0.7540 Recall: 0.2987 F1-Score: 0.4279\n",
      "akk: 407, gt: 116154\n",
      "Epoch: 020 train_loss: 0.0384 train_acc: 99.3883 Precision: 0.7838 Recall: 0.3379 F1-Score: 0.4722\n",
      "akk: 422, gt: 116139\n",
      "Epoch: 021 train_loss: 0.0384 train_acc: 99.3463 Precision: 0.7156 Recall: 0.3199 F1-Score: 0.4422\n",
      "akk: 30328, gt: 3738458\n",
      "OA: 0.9929 AA: 0.7770 Kappa: 0.5556 Precision: 0.5609 Recall: 0.5575 F1-Score: 0.5592\n",
      "*************************\n",
      "akk: 402, gt: 116159\n",
      "Epoch: 022 train_loss: 0.0382 train_acc: 99.3720 Precision: 0.7637 Recall: 0.3252 F1-Score: 0.4562\n",
      "akk: 442, gt: 116119\n",
      "Epoch: 023 train_loss: 0.0376 train_acc: 99.4012 Precision: 0.7783 Recall: 0.3644 F1-Score: 0.4964\n",
      "akk: 470, gt: 116091\n",
      "Epoch: 024 train_loss: 0.0379 train_acc: 99.3857 Precision: 0.7426 Recall: 0.3697 F1-Score: 0.4936\n",
      "akk: 463, gt: 116098\n",
      "Epoch: 025 train_loss: 0.0373 train_acc: 99.4175 Precision: 0.7862 Recall: 0.3856 F1-Score: 0.5174\n",
      "akk: 494, gt: 116067\n",
      "Epoch: 026 train_loss: 0.0373 train_acc: 99.3995 Precision: 0.7470 Recall: 0.3909 F1-Score: 0.5132\n",
      "akk: 550, gt: 116011\n",
      "Epoch: 027 train_loss: 0.0369 train_acc: 99.4166 Precision: 0.7400 Recall: 0.4311 F1-Score: 0.5448\n",
      "akk: 530, gt: 116031\n",
      "Epoch: 028 train_loss: 0.0370 train_acc: 99.4526 Precision: 0.7887 Recall: 0.4428 F1-Score: 0.5672\n",
      "akk: 534, gt: 116027\n",
      "Epoch: 029 train_loss: 0.0368 train_acc: 99.4321 Precision: 0.7640 Recall: 0.4322 F1-Score: 0.5521\n",
      "akk: 538, gt: 116023\n",
      "Epoch: 030 train_loss: 0.0369 train_acc: 99.4338 Precision: 0.7639 Recall: 0.4354 F1-Score: 0.5547\n",
      "akk: 574, gt: 115987\n",
      "Epoch: 031 train_loss: 0.0368 train_acc: 99.4424 Precision: 0.7561 Recall: 0.4597 F1-Score: 0.5718\n",
      "akk: 9890, gt: 3758896\n",
      "OA: 0.9937 AA: 0.6365 Kappa: 0.4106 Precision: 0.8436 Recall: 0.2734 F1-Score: 0.4130\n",
      "*************************\n",
      "akk: 557, gt: 116004\n",
      "Epoch: 032 train_loss: 0.0369 train_acc: 99.4432 Precision: 0.7648 Recall: 0.4513 F1-Score: 0.5676\n",
      "akk: 521, gt: 116040\n",
      "Epoch: 033 train_loss: 0.0364 train_acc: 99.4415 Precision: 0.7812 Recall: 0.4311 F1-Score: 0.5556\n",
      "akk: 559, gt: 116002\n",
      "Epoch: 034 train_loss: 0.0360 train_acc: 99.4295 Precision: 0.7496 Recall: 0.4439 F1-Score: 0.5576\n",
      "akk: 553, gt: 116008\n",
      "Epoch: 035 train_loss: 0.0358 train_acc: 99.4501 Precision: 0.7740 Recall: 0.4534 F1-Score: 0.5718\n",
      "akk: 572, gt: 115989\n",
      "Epoch: 036 train_loss: 0.0365 train_acc: 99.4561 Precision: 0.7710 Recall: 0.4672 F1-Score: 0.5818\n",
      "akk: 544, gt: 116017\n",
      "Epoch: 037 train_loss: 0.0362 train_acc: 99.4475 Precision: 0.7757 Recall: 0.4470 F1-Score: 0.5672\n",
      "akk: 607, gt: 115954\n",
      "Epoch: 038 train_loss: 0.0360 train_acc: 99.4638 Precision: 0.7628 Recall: 0.4905 F1-Score: 0.5970\n",
      "akk: 560, gt: 116001\n",
      "Epoch: 039 train_loss: 0.0361 train_acc: 99.4372 Precision: 0.7571 Recall: 0.4492 F1-Score: 0.5638\n",
      "akk: 611, gt: 115950\n",
      "Epoch: 040 train_loss: 0.0355 train_acc: 99.4998 Precision: 0.7954 Recall: 0.5148 F1-Score: 0.6251\n",
      "akk: 564, gt: 115997\n",
      "Epoch: 041 train_loss: 0.0357 train_acc: 99.4647 Precision: 0.7837 Recall: 0.4682 F1-Score: 0.5862\n",
      "akk: 16560, gt: 3752226\n",
      "OA: 0.9946 AA: 0.7192 Kappa: 0.5670 Precision: 0.8094 Recall: 0.4393 F1-Score: 0.5695\n",
      "*************************\n",
      "akk: 572, gt: 115989\n",
      "Epoch: 042 train_loss: 0.0353 train_acc: 99.4441 Precision: 0.7587 Recall: 0.4597 F1-Score: 0.5726\n",
      "akk: 622, gt: 115939\n",
      "Epoch: 043 train_loss: 0.0361 train_acc: 99.4269 Precision: 0.7219 Recall: 0.4756 F1-Score: 0.5734\n",
      "akk: 606, gt: 115955\n",
      "Epoch: 044 train_loss: 0.0353 train_acc: 99.4938 Precision: 0.7921 Recall: 0.5085 F1-Score: 0.6194\n",
      "akk: 571, gt: 115990\n",
      "Epoch: 045 train_loss: 0.0351 train_acc: 99.4775 Precision: 0.7933 Recall: 0.4799 F1-Score: 0.5980\n",
      "akk: 603, gt: 115958\n",
      "Epoch: 046 train_loss: 0.0350 train_acc: 99.4827 Precision: 0.7828 Recall: 0.5000 F1-Score: 0.6102\n",
      "akk: 621, gt: 115940\n",
      "Epoch: 047 train_loss: 0.0351 train_acc: 99.4655 Precision: 0.7585 Recall: 0.4989 F1-Score: 0.6019\n",
      "akk: 607, gt: 115954\n",
      "Epoch: 048 train_loss: 0.0351 train_acc: 99.4913 Precision: 0.7891 Recall: 0.5074 F1-Score: 0.6177\n",
      "akk: 598, gt: 115963\n",
      "Epoch: 049 train_loss: 0.0348 train_acc: 99.4835 Precision: 0.7860 Recall: 0.4979 F1-Score: 0.6096\n",
      "akk: 592, gt: 115969\n",
      "Epoch: 050 train_loss: 0.0346 train_acc: 99.4938 Precision: 0.7990 Recall: 0.5011 F1-Score: 0.6159\n",
      "akk: 619, gt: 115942\n",
      "Epoch: 051 train_loss: 0.0347 train_acc: 99.4930 Precision: 0.7851 Recall: 0.5148 F1-Score: 0.6219\n",
      "akk: 18594, gt: 3750192\n",
      "OA: 0.9947 AA: 0.7388 Kappa: 0.5924 Precision: 0.7856 Recall: 0.4787 F1-Score: 0.5949\n",
      "*************************\n",
      "akk: 610, gt: 115951\n",
      "Epoch: 052 train_loss: 0.0343 train_acc: 99.4887 Precision: 0.7852 Recall: 0.5074 F1-Score: 0.6165\n",
      "akk: 647, gt: 115914\n",
      "Epoch: 053 train_loss: 0.0345 train_acc: 99.5170 Precision: 0.7944 Recall: 0.5445 F1-Score: 0.6461\n",
      "akk: 617, gt: 115944\n",
      "Epoch: 054 train_loss: 0.0343 train_acc: 99.4913 Precision: 0.7844 Recall: 0.5127 F1-Score: 0.6201\n",
      "akk: 658, gt: 115903\n",
      "Epoch: 055 train_loss: 0.0342 train_acc: 99.5024 Precision: 0.7766 Recall: 0.5413 F1-Score: 0.6380\n",
      "akk: 648, gt: 115913\n",
      "Epoch: 056 train_loss: 0.0343 train_acc: 99.5076 Precision: 0.7855 Recall: 0.5392 F1-Score: 0.6394\n",
      "akk: 638, gt: 115923\n",
      "Epoch: 057 train_loss: 0.0345 train_acc: 99.4801 Precision: 0.7649 Recall: 0.5169 F1-Score: 0.6169\n",
      "akk: 648, gt: 115913\n",
      "Epoch: 058 train_loss: 0.0344 train_acc: 99.4870 Precision: 0.7670 Recall: 0.5265 F1-Score: 0.6244\n",
      "akk: 657, gt: 115904\n",
      "Epoch: 059 train_loss: 0.0342 train_acc: 99.5033 Precision: 0.7778 Recall: 0.5413 F1-Score: 0.6384\n",
      "akk: 633, gt: 115928\n",
      "Epoch: 060 train_loss: 0.0342 train_acc: 99.5084 Precision: 0.7930 Recall: 0.5318 F1-Score: 0.6367\n",
      "akk: 648, gt: 115913\n",
      "Epoch: 061 train_loss: 0.0340 train_acc: 99.5161 Precision: 0.7932 Recall: 0.5445 F1-Score: 0.6457\n",
      "akk: 30371, gt: 3738415\n",
      "OA: 0.9945 AA: 0.8287 Kappa: 0.6590 Precision: 0.6633 Recall: 0.6602 F1-Score: 0.6618\n",
      "*************************\n",
      "akk: 664, gt: 115897\n",
      "Epoch: 062 train_loss: 0.0340 train_acc: 99.5110 Precision: 0.7816 Recall: 0.5498 F1-Score: 0.6455\n",
      "akk: 659, gt: 115902\n",
      "Epoch: 063 train_loss: 0.0346 train_acc: 99.5084 Precision: 0.7815 Recall: 0.5456 F1-Score: 0.6425\n",
      "akk: 648, gt: 115913\n",
      "Epoch: 064 train_loss: 0.0341 train_acc: 99.5076 Precision: 0.7855 Recall: 0.5392 F1-Score: 0.6394\n",
      "akk: 616, gt: 115945\n",
      "Epoch: 065 train_loss: 0.0339 train_acc: 99.4990 Precision: 0.7922 Recall: 0.5169 F1-Score: 0.6256\n",
      "akk: 672, gt: 115889\n",
      "Epoch: 066 train_loss: 0.0338 train_acc: 99.5350 Precision: 0.7991 Recall: 0.5689 F1-Score: 0.6646\n",
      "akk: 658, gt: 115903\n",
      "Epoch: 067 train_loss: 0.0349 train_acc: 99.4990 Precision: 0.7736 Recall: 0.5392 F1-Score: 0.6355\n",
      "akk: 620, gt: 115941\n",
      "Epoch: 068 train_loss: 0.0339 train_acc: 99.5058 Precision: 0.7968 Recall: 0.5233 F1-Score: 0.6317\n",
      "akk: 633, gt: 115928\n",
      "Epoch: 069 train_loss: 0.0336 train_acc: 99.5273 Precision: 0.8104 Recall: 0.5434 F1-Score: 0.6506\n",
      "akk: 673, gt: 115888\n",
      "Epoch: 070 train_loss: 0.0333 train_acc: 99.5444 Precision: 0.8068 Recall: 0.5752 F1-Score: 0.6716\n",
      "akk: 663, gt: 115898\n",
      "Epoch: 071 train_loss: 0.0336 train_acc: 99.5204 Precision: 0.7903 Recall: 0.5551 F1-Score: 0.6521\n",
      "akk: 27307, gt: 3741479\n",
      "OA: 0.9947 AA: 0.8088 Kappa: 0.6515 Precision: 0.6926 Recall: 0.6198 F1-Score: 0.6542\n",
      "*************************\n",
      "akk: 634, gt: 115927\n",
      "Epoch: 072 train_loss: 0.0354 train_acc: 99.5161 Precision: 0.7997 Recall: 0.5371 F1-Score: 0.6426\n",
      "akk: 641, gt: 115920\n",
      "Epoch: 073 train_loss: 0.0335 train_acc: 99.5187 Precision: 0.7988 Recall: 0.5424 F1-Score: 0.6461\n",
      "akk: 674, gt: 115887\n",
      "Epoch: 074 train_loss: 0.0334 train_acc: 99.5316 Precision: 0.7953 Recall: 0.5678 F1-Score: 0.6625\n",
      "akk: 665, gt: 115896\n",
      "Epoch: 075 train_loss: 0.0334 train_acc: 99.5136 Precision: 0.7835 Recall: 0.5519 F1-Score: 0.6476\n",
      "akk: 670, gt: 115891\n",
      "Epoch: 076 train_loss: 0.0333 train_acc: 99.5350 Precision: 0.8000 Recall: 0.5678 F1-Score: 0.6642\n",
      "akk: 695, gt: 115866\n",
      "Epoch: 077 train_loss: 0.0334 train_acc: 99.5290 Precision: 0.7842 Recall: 0.5773 F1-Score: 0.6650\n",
      "akk: 684, gt: 115877\n",
      "Epoch: 078 train_loss: 0.0335 train_acc: 99.5178 Precision: 0.7792 Recall: 0.5646 F1-Score: 0.6548\n",
      "akk: 706, gt: 115855\n",
      "Epoch: 079 train_loss: 0.0331 train_acc: 99.5402 Precision: 0.7890 Recall: 0.5900 F1-Score: 0.6752\n",
      "akk: 695, gt: 115866\n",
      "Epoch: 080 train_loss: 0.0331 train_acc: 99.5427 Precision: 0.7957 Recall: 0.5858 F1-Score: 0.6748\n",
      "akk: 690, gt: 115871\n",
      "Epoch: 081 train_loss: 0.0331 train_acc: 99.5419 Precision: 0.7971 Recall: 0.5826 F1-Score: 0.6732\n",
      "akk: 22278, gt: 3746508\n",
      "OA: 0.9952 AA: 0.7826 Kappa: 0.6526 Precision: 0.7760 Recall: 0.5665 F1-Score: 0.6549\n",
      "*************************\n",
      "akk: 691, gt: 115870\n",
      "Epoch: 082 train_loss: 0.0329 train_acc: 99.5393 Precision: 0.7945 Recall: 0.5816 F1-Score: 0.6716\n",
      "akk: 679, gt: 115882\n",
      "Epoch: 083 train_loss: 0.0334 train_acc: 99.5273 Precision: 0.7894 Recall: 0.5678 F1-Score: 0.6605\n",
      "akk: 715, gt: 115846\n",
      "Epoch: 084 train_loss: 0.0331 train_acc: 99.5530 Precision: 0.7958 Recall: 0.6028 F1-Score: 0.6860\n",
      "akk: 683, gt: 115878\n",
      "Epoch: 085 train_loss: 0.0330 train_acc: 99.5599 Precision: 0.8155 Recall: 0.5900 F1-Score: 0.6847\n",
      "akk: 690, gt: 115871\n",
      "Epoch: 086 train_loss: 0.0335 train_acc: 99.5316 Precision: 0.7884 Recall: 0.5763 F1-Score: 0.6659\n",
      "akk: 638, gt: 115923\n",
      "Epoch: 087 train_loss: 0.0331 train_acc: 99.5247 Precision: 0.8056 Recall: 0.5445 F1-Score: 0.6498\n",
      "akk: 672, gt: 115889\n",
      "Epoch: 088 train_loss: 0.0331 train_acc: 99.5367 Precision: 0.8006 Recall: 0.5699 F1-Score: 0.6658\n",
      "akk: 692, gt: 115869\n",
      "Epoch: 089 train_loss: 0.0328 train_acc: 99.5504 Precision: 0.8035 Recall: 0.5890 F1-Score: 0.6797\n",
      "akk: 707, gt: 115854\n",
      "Epoch: 090 train_loss: 0.0328 train_acc: 99.5565 Precision: 0.8020 Recall: 0.6006 F1-Score: 0.6869\n",
      "akk: 674, gt: 115887\n",
      "Epoch: 091 train_loss: 0.0329 train_acc: 99.5402 Precision: 0.8027 Recall: 0.5731 F1-Score: 0.6687\n",
      "akk: 16909, gt: 3751877\n",
      "OA: 0.9947 AA: 0.7248 Kappa: 0.5773 Precision: 0.8129 Recall: 0.4505 F1-Score: 0.5797\n",
      "*************************\n",
      "akk: 721, gt: 115840\n",
      "Epoch: 092 train_loss: 0.0326 train_acc: 99.5565 Precision: 0.7961 Recall: 0.6081 F1-Score: 0.6895\n",
      "akk: 698, gt: 115863\n",
      "Epoch: 093 train_loss: 0.0327 train_acc: 99.5762 Precision: 0.8223 Recall: 0.6081 F1-Score: 0.6991\n",
      "akk: 721, gt: 115840\n",
      "Epoch: 094 train_loss: 0.0325 train_acc: 99.5444 Precision: 0.7864 Recall: 0.6006 F1-Score: 0.6811\n",
      "akk: 695, gt: 115866\n",
      "Epoch: 095 train_loss: 0.0327 train_acc: 99.5616 Precision: 0.8115 Recall: 0.5975 F1-Score: 0.6882\n",
      "akk: 715, gt: 115846\n",
      "Epoch: 096 train_loss: 0.0324 train_acc: 99.5736 Precision: 0.8126 Recall: 0.6155 F1-Score: 0.7004\n",
      "akk: 697, gt: 115864\n",
      "Epoch: 097 train_loss: 0.0328 train_acc: 99.5633 Precision: 0.8121 Recall: 0.5996 F1-Score: 0.6898\n",
      "akk: 692, gt: 115869\n",
      "Epoch: 098 train_loss: 0.0328 train_acc: 99.5556 Precision: 0.8078 Recall: 0.5922 F1-Score: 0.6834\n",
      "akk: 724, gt: 115837\n",
      "Epoch: 099 train_loss: 0.0326 train_acc: 99.5676 Precision: 0.8039 Recall: 0.6165 F1-Score: 0.6978\n",
      "akk: 730, gt: 115831\n",
      "Epoch: 100 train_loss: 0.0324 train_acc: 99.5659 Precision: 0.8000 Recall: 0.6186 F1-Score: 0.6977\n",
      "akk: 25445, gt: 3743341\n",
      "OA: 0.9952 AA: 0.8088 Kappa: 0.6730 Precision: 0.7427 Recall: 0.6193 F1-Score: 0.6754\n",
      "*************************\n",
      "Running Time: 2968.63\n",
      "**************************************************\n",
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [4:34:56<4:36:10, 3314.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "F1 score: 0.6561309913265221\n",
      "Precision: 0.6947207616882194\n",
      "Recall: 0.621602721001939\n",
      "max_label: 1 max_pred: 1\n",
      "Accuracy: 0.9959036681525537\n",
      "Confusion Matrix:\n",
      "[[4963693    8593]\n",
      " [  11904   19555]]\n",
      "**************************************************\n",
      "Parameter:\n",
      "fix_random: True\n",
      "gpu_id: 0\n",
      "seed: 0\n",
      "dataset: BS\n",
      "flag_test: train\n",
      "model_name: conv2d_unmix\n",
      "batch_size: 128\n",
      "test_freq: 10\n",
      "patches: 5\n",
      "epoches: 100\n",
      "inference_model: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "learning_rate: 0.0001\n",
      "gamma: 0.9\n",
      "weight_decay: 0\n",
      "Train data change to the ratio of train samples: 0.03\n",
      "height=2715,width=1843,band=150\n",
      "**************************************************\n",
      "patch is : 5\n",
      "mirror_image shape : [2719,1847,150]\n",
      "**************************************************\n",
      "x_train shape = (116561, 5, 5, 150), type = float32\n",
      "x_test  shape = (3768786, 5, 5, 150), type = float32\n",
      "x_true  shape = (5003745, 5, 5, 150), type = float32\n",
      "**************************************************\n",
      "y_train: shape = (116561,) ,type = int64\n",
      "y_test: shape = (3768786,) ,type = int64\n",
      "y_true: shape = (5003745,) ,type = int64\n",
      "**************************************************\n",
      "Model Name: conv2d_unmix\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akk: 5429, gt: 111132\n",
      "Epoch: 001 train_loss: 1.4352 train_acc: 94.8293 Precision: 0.0319 Recall: 0.1833 F1-Score: 0.0543\n",
      "akk: 1536612, gt: 2232174\n",
      "OA: 0.5994 AA: 0.7673 Kappa: 0.0210 Precision: 0.0186 Recall: 0.9380 F1-Score: 0.0365\n",
      "*************************\n",
      "akk: 5122, gt: 111439\n",
      "Epoch: 002 train_loss: 0.7191 train_acc: 95.2694 Precision: 0.0539 Recall: 0.2924 F1-Score: 0.0910\n",
      "akk: 2802, gt: 113759\n",
      "Epoch: 003 train_loss: 0.3933 train_acc: 97.1997 Precision: 0.0860 Recall: 0.2553 F1-Score: 0.1287\n",
      "akk: 2509, gt: 114052\n",
      "Epoch: 004 train_loss: 0.2477 train_acc: 97.3670 Precision: 0.0765 Recall: 0.2034 F1-Score: 0.1112\n",
      "akk: 269, gt: 116292\n",
      "Epoch: 005 train_loss: 0.0662 train_acc: 98.9662 Precision: 0.0149 Recall: 0.0042 F1-Score: 0.0066\n",
      "akk: 76, gt: 116485\n",
      "Epoch: 006 train_loss: 0.0548 train_acc: 99.1301 Precision: 0.0395 Recall: 0.0032 F1-Score: 0.0059\n",
      "akk: 46, gt: 116515\n",
      "Epoch: 007 train_loss: 0.0509 train_acc: 99.2090 Precision: 0.7391 Recall: 0.0360 F1-Score: 0.0687\n",
      "akk: 274, gt: 116287\n",
      "Epoch: 008 train_loss: 0.0443 train_acc: 99.2793 Precision: 0.6898 Recall: 0.2002 F1-Score: 0.3103\n",
      "akk: 448, gt: 116113\n",
      "Epoch: 009 train_loss: 0.0421 train_acc: 99.3497 Precision: 0.7076 Recall: 0.3358 F1-Score: 0.4555\n",
      "akk: 529, gt: 116032\n",
      "Epoch: 010 train_loss: 0.0403 train_acc: 99.3986 Precision: 0.7297 Recall: 0.4089 F1-Score: 0.5241\n",
      "akk: 544, gt: 116017\n",
      "Epoch: 011 train_loss: 0.0400 train_acc: 99.4029 Precision: 0.7279 Recall: 0.4195 F1-Score: 0.5323\n",
      "akk: 11003, gt: 3757783\n",
      "OA: 0.9941 AA: 0.6576 Kappa: 0.4615 Precision: 0.8751 Recall: 0.3155 F1-Score: 0.4638\n",
      "*************************\n",
      "akk: 554, gt: 116007\n",
      "Epoch: 012 train_loss: 0.0398 train_acc: 99.4509 Precision: 0.7744 Recall: 0.4544 F1-Score: 0.5728\n",
      "akk: 587, gt: 115974\n",
      "Epoch: 013 train_loss: 0.0387 train_acc: 99.4415 Precision: 0.7496 Recall: 0.4661 F1-Score: 0.5748\n",
      "akk: 535, gt: 116026\n",
      "Epoch: 014 train_loss: 0.0393 train_acc: 99.4295 Precision: 0.7607 Recall: 0.4311 F1-Score: 0.5504\n",
      "akk: 603, gt: 115958\n",
      "Epoch: 015 train_loss: 0.0388 train_acc: 99.4226 Precision: 0.7247 Recall: 0.4629 F1-Score: 0.5650\n",
      "akk: 583, gt: 115978\n",
      "Epoch: 016 train_loss: 0.0383 train_acc: 99.4449 Precision: 0.7547 Recall: 0.4661 F1-Score: 0.5763\n",
      "akk: 611, gt: 115950\n",
      "Epoch: 017 train_loss: 0.0385 train_acc: 99.4724 Precision: 0.7692 Recall: 0.4979 F1-Score: 0.6045\n",
      "akk: 623, gt: 115938\n",
      "Epoch: 018 train_loss: 0.0374 train_acc: 99.4827 Precision: 0.7737 Recall: 0.5106 F1-Score: 0.6152\n",
      "akk: 609, gt: 115952\n",
      "Epoch: 019 train_loss: 0.0389 train_acc: 99.4689 Precision: 0.7668 Recall: 0.4947 F1-Score: 0.6014\n",
      "akk: 652, gt: 115909\n",
      "Epoch: 020 train_loss: 0.0368 train_acc: 99.4921 Precision: 0.7699 Recall: 0.5318 F1-Score: 0.6291\n",
      "akk: 630, gt: 115931\n",
      "Epoch: 021 train_loss: 0.0379 train_acc: 99.4852 Precision: 0.7730 Recall: 0.5159 F1-Score: 0.6188\n",
      "akk: 20044, gt: 3748742\n",
      "OA: 0.9951 AA: 0.7627 Kappa: 0.6331 Precision: 0.8015 Recall: 0.5265 F1-Score: 0.6355\n",
      "*************************\n",
      "akk: 613, gt: 115948\n",
      "Epoch: 022 train_loss: 0.0365 train_acc: 99.4998 Precision: 0.7945 Recall: 0.5159 F1-Score: 0.6256\n",
      "akk: 661, gt: 115900\n",
      "Epoch: 023 train_loss: 0.0365 train_acc: 99.5033 Precision: 0.7761 Recall: 0.5434 F1-Score: 0.6393\n",
      "akk: 677, gt: 115884\n",
      "Epoch: 024 train_loss: 0.0371 train_acc: 99.5015 Precision: 0.7681 Recall: 0.5508 F1-Score: 0.6416\n",
      "akk: 668, gt: 115893\n",
      "Epoch: 025 train_loss: 0.0357 train_acc: 99.5041 Precision: 0.7740 Recall: 0.5477 F1-Score: 0.6414\n",
      "akk: 670, gt: 115891\n",
      "Epoch: 026 train_loss: 0.0355 train_acc: 99.5384 Precision: 0.8030 Recall: 0.5699 F1-Score: 0.6667\n",
      "akk: 653, gt: 115908\n",
      "Epoch: 027 train_loss: 0.0364 train_acc: 99.5033 Precision: 0.7795 Recall: 0.5392 F1-Score: 0.6374\n",
      "akk: 678, gt: 115883\n",
      "Epoch: 028 train_loss: 0.0357 train_acc: 99.5230 Precision: 0.7861 Recall: 0.5646 F1-Score: 0.6572\n",
      "akk: 669, gt: 115892\n",
      "Epoch: 029 train_loss: 0.0353 train_acc: 99.5239 Precision: 0.7907 Recall: 0.5604 F1-Score: 0.6559\n",
      "akk: 695, gt: 115866\n",
      "Epoch: 030 train_loss: 0.0357 train_acc: 99.5204 Precision: 0.7770 Recall: 0.5720 F1-Score: 0.6589\n",
      "akk: 674, gt: 115887\n",
      "Epoch: 031 train_loss: 0.0352 train_acc: 99.5127 Precision: 0.7789 Recall: 0.5561 F1-Score: 0.6489\n",
      "akk: 22387, gt: 3746399\n",
      "OA: 0.9954 AA: 0.7918 Kappa: 0.6725 Precision: 0.7972 Recall: 0.5849 F1-Score: 0.6747\n",
      "*************************\n",
      "akk: 674, gt: 115887\n",
      "Epoch: 032 train_loss: 0.0359 train_acc: 99.5247 Precision: 0.7893 Recall: 0.5636 F1-Score: 0.6576\n",
      "akk: 701, gt: 115860\n",
      "Epoch: 033 train_loss: 0.0347 train_acc: 99.5290 Precision: 0.7817 Recall: 0.5805 F1-Score: 0.6663\n",
      "akk: 692, gt: 115869\n",
      "Epoch: 034 train_loss: 0.0349 train_acc: 99.5213 Precision: 0.7789 Recall: 0.5710 F1-Score: 0.6589\n",
      "akk: 721, gt: 115840\n",
      "Epoch: 035 train_loss: 0.0348 train_acc: 99.5256 Precision: 0.7712 Recall: 0.5890 F1-Score: 0.6679\n",
      "akk: 706, gt: 115855\n",
      "Epoch: 036 train_loss: 0.0347 train_acc: 99.5127 Precision: 0.7663 Recall: 0.5731 F1-Score: 0.6558\n",
      "akk: 696, gt: 115865\n",
      "Epoch: 037 train_loss: 0.0349 train_acc: 99.5384 Precision: 0.7917 Recall: 0.5837 F1-Score: 0.6720\n",
      "akk: 694, gt: 115867\n",
      "Epoch: 038 train_loss: 0.0348 train_acc: 99.5230 Precision: 0.7795 Recall: 0.5731 F1-Score: 0.6606\n",
      "akk: 696, gt: 115865\n",
      "Epoch: 039 train_loss: 0.0344 train_acc: 99.5316 Precision: 0.7859 Recall: 0.5794 F1-Score: 0.6671\n",
      "akk: 714, gt: 115847\n",
      "Epoch: 040 train_loss: 0.0344 train_acc: 99.5178 Precision: 0.7675 Recall: 0.5805 F1-Score: 0.6610\n",
      "akk: 691, gt: 115870\n",
      "Epoch: 041 train_loss: 0.0353 train_acc: 99.5359 Precision: 0.7916 Recall: 0.5794 F1-Score: 0.6691\n",
      "akk: 25494, gt: 3743292\n",
      "OA: 0.9955 AA: 0.8200 Kappa: 0.6969 Precision: 0.7680 Recall: 0.6416 F1-Score: 0.6991\n",
      "*************************\n",
      "akk: 697, gt: 115864\n",
      "Epoch: 042 train_loss: 0.0346 train_acc: 99.5307 Precision: 0.7848 Recall: 0.5794 F1-Score: 0.6667\n",
      "akk: 696, gt: 115865\n",
      "Epoch: 043 train_loss: 0.0342 train_acc: 99.5281 Precision: 0.7830 Recall: 0.5773 F1-Score: 0.6646\n",
      "akk: 716, gt: 115845\n",
      "Epoch: 044 train_loss: 0.0339 train_acc: 99.5436 Precision: 0.7877 Recall: 0.5975 F1-Score: 0.6795\n",
      "akk: 719, gt: 115842\n",
      "Epoch: 045 train_loss: 0.0341 train_acc: 99.5376 Precision: 0.7816 Recall: 0.5953 F1-Score: 0.6759\n",
      "akk: 728, gt: 115833\n",
      "Epoch: 046 train_loss: 0.0346 train_acc: 99.5264 Precision: 0.7692 Recall: 0.5932 F1-Score: 0.6699\n",
      "akk: 722, gt: 115839\n",
      "Epoch: 047 train_loss: 0.0340 train_acc: 99.5436 Precision: 0.7853 Recall: 0.6006 F1-Score: 0.6807\n",
      "akk: 714, gt: 115847\n",
      "Epoch: 048 train_loss: 0.0340 train_acc: 99.5504 Precision: 0.7941 Recall: 0.6006 F1-Score: 0.6840\n",
      "akk: 749, gt: 115812\n",
      "Epoch: 049 train_loss: 0.0338 train_acc: 99.5547 Precision: 0.7837 Recall: 0.6218 F1-Score: 0.6934\n",
      "akk: 708, gt: 115853\n",
      "Epoch: 050 train_loss: 0.0336 train_acc: 99.5504 Precision: 0.7966 Recall: 0.5975 F1-Score: 0.6828\n",
      "akk: 744, gt: 115817\n",
      "Epoch: 051 train_loss: 0.0336 train_acc: 99.5539 Precision: 0.7849 Recall: 0.6186 F1-Score: 0.6919\n",
      "akk: 20608, gt: 3748178\n",
      "OA: 0.9949 AA: 0.7598 Kappa: 0.6192 Precision: 0.7712 Recall: 0.5208 F1-Score: 0.6217\n",
      "*************************\n",
      "akk: 736, gt: 115825\n",
      "Epoch: 052 train_loss: 0.0335 train_acc: 99.5590 Precision: 0.7921 Recall: 0.6176 F1-Score: 0.6940\n",
      "akk: 720, gt: 115841\n",
      "Epoch: 053 train_loss: 0.0337 train_acc: 99.5281 Precision: 0.7736 Recall: 0.5900 F1-Score: 0.6695\n",
      "akk: 734, gt: 115827\n",
      "Epoch: 054 train_loss: 0.0338 train_acc: 99.5573 Precision: 0.7916 Recall: 0.6155 F1-Score: 0.6925\n",
      "akk: 732, gt: 115829\n",
      "Epoch: 055 train_loss: 0.0336 train_acc: 99.5556 Precision: 0.7910 Recall: 0.6133 F1-Score: 0.6909\n",
      "akk: 748, gt: 115813\n",
      "Epoch: 056 train_loss: 0.0335 train_acc: 99.5487 Precision: 0.7794 Recall: 0.6176 F1-Score: 0.6891\n",
      "akk: 736, gt: 115825\n",
      "Epoch: 057 train_loss: 0.0338 train_acc: 99.5625 Precision: 0.7948 Recall: 0.6197 F1-Score: 0.6964\n",
      "akk: 739, gt: 115822\n",
      "Epoch: 058 train_loss: 0.0333 train_acc: 99.5530 Precision: 0.7862 Recall: 0.6155 F1-Score: 0.6904\n",
      "akk: 740, gt: 115821\n",
      "Epoch: 059 train_loss: 0.0335 train_acc: 99.5504 Precision: 0.7838 Recall: 0.6144 F1-Score: 0.6888\n",
      "akk: 740, gt: 115821\n",
      "Epoch: 060 train_loss: 0.0334 train_acc: 99.5539 Precision: 0.7865 Recall: 0.6165 F1-Score: 0.6912\n",
      "akk: 728, gt: 115833\n",
      "Epoch: 061 train_loss: 0.0333 train_acc: 99.5504 Precision: 0.7885 Recall: 0.6081 F1-Score: 0.6866\n",
      "akk: 20928, gt: 3747858\n",
      "OA: 0.9954 AA: 0.7777 Kappa: 0.6579 Precision: 0.8114 Recall: 0.5565 F1-Score: 0.6602\n",
      "*************************\n",
      "akk: 746, gt: 115815\n",
      "Epoch: 062 train_loss: 0.0331 train_acc: 99.5607 Precision: 0.7895 Recall: 0.6239 F1-Score: 0.6970\n",
      "akk: 727, gt: 115834\n",
      "Epoch: 063 train_loss: 0.0332 train_acc: 99.5565 Precision: 0.7937 Recall: 0.6112 F1-Score: 0.6906\n",
      "akk: 749, gt: 115812\n",
      "Epoch: 064 train_loss: 0.0331 train_acc: 99.5530 Precision: 0.7824 Recall: 0.6208 F1-Score: 0.6923\n",
      "akk: 747, gt: 115814\n",
      "Epoch: 065 train_loss: 0.0331 train_acc: 99.5547 Precision: 0.7845 Recall: 0.6208 F1-Score: 0.6931\n",
      "akk: 751, gt: 115810\n",
      "Epoch: 066 train_loss: 0.0330 train_acc: 99.5616 Precision: 0.7883 Recall: 0.6271 F1-Score: 0.6985\n",
      "akk: 767, gt: 115794\n",
      "Epoch: 067 train_loss: 0.0330 train_acc: 99.5616 Precision: 0.7823 Recall: 0.6356 F1-Score: 0.7013\n",
      "akk: 732, gt: 115829\n",
      "Epoch: 068 train_loss: 0.0340 train_acc: 99.5556 Precision: 0.7910 Recall: 0.6133 F1-Score: 0.6909\n",
      "akk: 739, gt: 115822\n",
      "Epoch: 069 train_loss: 0.0332 train_acc: 99.5616 Precision: 0.7930 Recall: 0.6208 F1-Score: 0.6964\n",
      "akk: 764, gt: 115797\n",
      "Epoch: 070 train_loss: 0.0328 train_acc: 99.5659 Precision: 0.7866 Recall: 0.6367 F1-Score: 0.7037\n",
      "akk: 746, gt: 115815\n",
      "Epoch: 071 train_loss: 0.0329 train_acc: 99.5693 Precision: 0.7962 Recall: 0.6292 F1-Score: 0.7030\n",
      "akk: 27515, gt: 3741271\n",
      "OA: 0.9955 AA: 0.8363 Kappa: 0.7071 Precision: 0.7480 Recall: 0.6745 F1-Score: 0.7094\n",
      "*************************\n",
      "akk: 766, gt: 115795\n",
      "Epoch: 072 train_loss: 0.0328 train_acc: 99.5796 Precision: 0.7963 Recall: 0.6462 F1-Score: 0.7135\n",
      "akk: 741, gt: 115820\n",
      "Epoch: 073 train_loss: 0.0328 train_acc: 99.5702 Precision: 0.7989 Recall: 0.6271 F1-Score: 0.7027\n",
      "akk: 748, gt: 115813\n",
      "Epoch: 074 train_loss: 0.0327 train_acc: 99.5779 Precision: 0.8021 Recall: 0.6356 F1-Score: 0.7092\n",
      "akk: 761, gt: 115800\n",
      "Epoch: 075 train_loss: 0.0333 train_acc: 99.5770 Precision: 0.7963 Recall: 0.6419 F1-Score: 0.7109\n",
      "akk: 750, gt: 115811\n",
      "Epoch: 076 train_loss: 0.0328 train_acc: 99.5676 Precision: 0.7933 Recall: 0.6303 F1-Score: 0.7025\n",
      "akk: 787, gt: 115774\n",
      "Epoch: 077 train_loss: 0.0325 train_acc: 99.5788 Precision: 0.7878 Recall: 0.6568 F1-Score: 0.7163\n",
      "akk: 763, gt: 115798\n",
      "Epoch: 078 train_loss: 0.0326 train_acc: 99.5959 Precision: 0.8100 Recall: 0.6547 F1-Score: 0.7241\n",
      "akk: 748, gt: 115813\n",
      "Epoch: 079 train_loss: 0.0329 train_acc: 99.5762 Precision: 0.8008 Recall: 0.6345 F1-Score: 0.7080\n",
      "akk: 760, gt: 115801\n",
      "Epoch: 080 train_loss: 0.0329 train_acc: 99.5779 Precision: 0.7974 Recall: 0.6419 F1-Score: 0.7113\n",
      "akk: 767, gt: 115794\n",
      "Epoch: 081 train_loss: 0.0325 train_acc: 99.5805 Precision: 0.7966 Recall: 0.6472 F1-Score: 0.7142\n",
      "akk: 25914, gt: 3742872\n",
      "OA: 0.9957 AA: 0.8283 Kappa: 0.7096 Precision: 0.7749 Recall: 0.6581 F1-Score: 0.7117\n",
      "*************************\n",
      "akk: 752, gt: 115809\n",
      "Epoch: 082 train_loss: 0.0326 train_acc: 99.5848 Precision: 0.8059 Recall: 0.6419 F1-Score: 0.7146\n",
      "akk: 765, gt: 115796\n",
      "Epoch: 083 train_loss: 0.0326 train_acc: 99.5822 Precision: 0.7987 Recall: 0.6472 F1-Score: 0.7150\n",
      "akk: 752, gt: 115809\n",
      "Epoch: 084 train_loss: 0.0326 train_acc: 99.5710 Precision: 0.7952 Recall: 0.6335 F1-Score: 0.7052\n",
      "akk: 761, gt: 115800\n",
      "Epoch: 085 train_loss: 0.0327 train_acc: 99.5753 Precision: 0.7950 Recall: 0.6409 F1-Score: 0.7097\n",
      "akk: 757, gt: 115804\n",
      "Epoch: 086 train_loss: 0.0328 train_acc: 99.5719 Precision: 0.7939 Recall: 0.6367 F1-Score: 0.7066\n",
      "akk: 759, gt: 115802\n",
      "Epoch: 087 train_loss: 0.0325 train_acc: 99.5719 Precision: 0.7931 Recall: 0.6377 F1-Score: 0.7070\n",
      "akk: 760, gt: 115801\n",
      "Epoch: 088 train_loss: 0.0325 train_acc: 99.5710 Precision: 0.7921 Recall: 0.6377 F1-Score: 0.7066\n",
      "akk: 758, gt: 115803\n",
      "Epoch: 089 train_loss: 0.0327 train_acc: 99.5796 Precision: 0.7995 Recall: 0.6419 F1-Score: 0.7121\n",
      "akk: 773, gt: 115788\n",
      "Epoch: 090 train_loss: 0.0323 train_acc: 99.5822 Precision: 0.7956 Recall: 0.6515 F1-Score: 0.7164\n",
      "akk: 760, gt: 115801\n",
      "Epoch: 091 train_loss: 0.0324 train_acc: 99.5813 Precision: 0.8000 Recall: 0.6441 F1-Score: 0.7136\n",
      "akk: 17433, gt: 3751353\n",
      "OA: 0.9951 AA: 0.7409 Kappa: 0.6120 Precision: 0.8448 Recall: 0.4826 F1-Score: 0.6143\n",
      "*************************\n",
      "akk: 774, gt: 115787\n",
      "Epoch: 092 train_loss: 0.0323 train_acc: 99.5951 Precision: 0.8049 Recall: 0.6600 F1-Score: 0.7253\n",
      "akk: 737, gt: 115824\n",
      "Epoch: 093 train_loss: 0.0324 train_acc: 99.5856 Precision: 0.8128 Recall: 0.6345 F1-Score: 0.7127\n",
      "akk: 747, gt: 115814\n",
      "Epoch: 094 train_loss: 0.0326 train_acc: 99.5805 Precision: 0.8046 Recall: 0.6367 F1-Score: 0.7108\n",
      "akk: 742, gt: 115819\n",
      "Epoch: 095 train_loss: 0.0329 train_acc: 99.5728 Precision: 0.8005 Recall: 0.6292 F1-Score: 0.7046\n",
      "akk: 758, gt: 115803\n",
      "Epoch: 096 train_loss: 0.0323 train_acc: 99.5951 Precision: 0.8113 Recall: 0.6515 F1-Score: 0.7227\n",
      "akk: 774, gt: 115787\n",
      "Epoch: 097 train_loss: 0.0321 train_acc: 99.6208 Precision: 0.8243 Recall: 0.6758 F1-Score: 0.7427\n",
      "akk: 764, gt: 115797\n",
      "Epoch: 098 train_loss: 0.0323 train_acc: 99.5831 Precision: 0.7997 Recall: 0.6472 F1-Score: 0.7155\n",
      "akk: 783, gt: 115778\n",
      "Epoch: 099 train_loss: 0.0323 train_acc: 99.6062 Precision: 0.8097 Recall: 0.6716 F1-Score: 0.7342\n",
      "akk: 793, gt: 115768\n",
      "Epoch: 100 train_loss: 0.0326 train_acc: 99.5856 Precision: 0.7907 Recall: 0.6642 F1-Score: 0.7219\n",
      "akk: 22318, gt: 3746468\n",
      "OA: 0.9956 AA: 0.7956 Kappa: 0.6822 Precision: 0.8100 Recall: 0.5924 F1-Score: 0.6843\n",
      "*************************\n",
      "Running Time: 2973.11\n",
      "**************************************************\n",
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [5:30:24<3:41:14, 3318.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "F1 score: 0.6854165520024951\n",
      "Precision: 0.8104824713641097\n",
      "Recall: 0.5937887409008551\n",
      "max_label: 1 max_pred: 1\n",
      "Accuracy: 0.9965731666981431\n",
      "Confusion Matrix:\n",
      "[[4967918    4368]\n",
      " [  12779   18680]]\n",
      "**************************************************\n",
      "Parameter:\n",
      "fix_random: True\n",
      "gpu_id: 0\n",
      "seed: 0\n",
      "dataset: BS\n",
      "flag_test: train\n",
      "model_name: conv2d_unmix\n",
      "batch_size: 128\n",
      "test_freq: 10\n",
      "patches: 5\n",
      "epoches: 100\n",
      "inference_model: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "learning_rate: 0.0001\n",
      "gamma: 0.9\n",
      "weight_decay: 0\n",
      "Train data change to the ratio of train samples: 0.03\n",
      "height=2715,width=1843,band=150\n",
      "**************************************************\n",
      "patch is : 5\n",
      "mirror_image shape : [2719,1847,150]\n",
      "**************************************************\n",
      "x_train shape = (116561, 5, 5, 150), type = float32\n",
      "x_test  shape = (3768786, 5, 5, 150), type = float32\n",
      "x_true  shape = (5003745, 5, 5, 150), type = float32\n",
      "**************************************************\n",
      "y_train: shape = (116561,) ,type = int64\n",
      "y_test: shape = (3768786,) ,type = int64\n",
      "y_true: shape = (5003745,) ,type = int64\n",
      "**************************************************\n",
      "Model Name: conv2d_unmix\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akk: 6376, gt: 110185\n",
      "Epoch: 001 train_loss: 0.5401 train_acc: 94.0821 Precision: 0.0331 Recall: 0.2235 F1-Score: 0.0577\n",
      "akk: 2596871, gt: 1171915\n",
      "OA: 0.3189 AA: 0.6513 Kappa: 0.0071 Precision: 0.0116 Recall: 0.9892 F1-Score: 0.0230\n",
      "*************************\n",
      "akk: 3925, gt: 112636\n",
      "Epoch: 002 train_loss: 0.1711 train_acc: 96.1797 Precision: 0.0530 Recall: 0.2203 F1-Score: 0.0854\n",
      "akk: 2089, gt: 114472\n",
      "Epoch: 003 train_loss: 0.0892 train_acc: 97.7960 Precision: 0.1111 Recall: 0.2458 F1-Score: 0.1530\n",
      "akk: 1383, gt: 115178\n",
      "Epoch: 004 train_loss: 0.0671 train_acc: 98.4738 Precision: 0.1981 Recall: 0.2903 F1-Score: 0.2355\n",
      "akk: 1276, gt: 115285\n",
      "Epoch: 005 train_loss: 0.0659 train_acc: 98.5141 Precision: 0.1912 Recall: 0.2585 F1-Score: 0.2198\n",
      "akk: 636, gt: 115925\n",
      "Epoch: 006 train_loss: 0.0479 train_acc: 99.2073 Precision: 0.5157 Recall: 0.3475 F1-Score: 0.4152\n",
      "akk: 573, gt: 115988\n",
      "Epoch: 007 train_loss: 0.0412 train_acc: 99.3729 Precision: 0.6859 Recall: 0.4163 F1-Score: 0.5181\n",
      "akk: 631, gt: 115930\n",
      "Epoch: 008 train_loss: 0.0396 train_acc: 99.4501 Precision: 0.7401 Recall: 0.4947 F1-Score: 0.5930\n",
      "akk: 639, gt: 115922\n",
      "Epoch: 009 train_loss: 0.0399 train_acc: 99.4432 Precision: 0.7308 Recall: 0.4947 F1-Score: 0.5900\n",
      "akk: 629, gt: 115932\n",
      "Epoch: 010 train_loss: 0.0385 train_acc: 99.4775 Precision: 0.7663 Recall: 0.5106 F1-Score: 0.6128\n",
      "akk: 635, gt: 115926\n",
      "Epoch: 011 train_loss: 0.0386 train_acc: 99.4604 Precision: 0.7480 Recall: 0.5032 F1-Score: 0.6016\n",
      "akk: 2691, gt: 3766095\n",
      "OA: 0.9925 AA: 0.5412 Kappa: 0.1502 Precision: 0.9339 Recall: 0.0824 F1-Score: 0.1514\n",
      "*************************\n",
      "akk: 650, gt: 115911\n",
      "Epoch: 012 train_loss: 0.0386 train_acc: 99.4664 Precision: 0.7477 Recall: 0.5148 F1-Score: 0.6098\n",
      "akk: 625, gt: 115936\n",
      "Epoch: 013 train_loss: 0.0378 train_acc: 99.4827 Precision: 0.7728 Recall: 0.5117 F1-Score: 0.6157\n",
      "akk: 650, gt: 115911\n",
      "Epoch: 014 train_loss: 0.0392 train_acc: 99.4801 Precision: 0.7600 Recall: 0.5233 F1-Score: 0.6198\n",
      "akk: 648, gt: 115913\n",
      "Epoch: 015 train_loss: 0.0374 train_acc: 99.4629 Precision: 0.7454 Recall: 0.5117 F1-Score: 0.6068\n",
      "akk: 644, gt: 115917\n",
      "Epoch: 016 train_loss: 0.0376 train_acc: 99.4732 Precision: 0.7562 Recall: 0.5159 F1-Score: 0.6134\n",
      "akk: 653, gt: 115908\n",
      "Epoch: 017 train_loss: 0.0375 train_acc: 99.4930 Precision: 0.7703 Recall: 0.5328 F1-Score: 0.6299\n",
      "akk: 649, gt: 115912\n",
      "Epoch: 018 train_loss: 0.0368 train_acc: 99.4964 Precision: 0.7750 Recall: 0.5328 F1-Score: 0.6315\n",
      "akk: 664, gt: 115897\n",
      "Epoch: 019 train_loss: 0.0363 train_acc: 99.4990 Precision: 0.7711 Recall: 0.5424 F1-Score: 0.6368\n",
      "akk: 664, gt: 115897\n",
      "Epoch: 020 train_loss: 0.0364 train_acc: 99.5144 Precision: 0.7846 Recall: 0.5519 F1-Score: 0.6480\n",
      "akk: 669, gt: 115892\n",
      "Epoch: 021 train_loss: 0.0362 train_acc: 99.4930 Precision: 0.7638 Recall: 0.5413 F1-Score: 0.6336\n",
      "akk: 35568, gt: 3733218\n",
      "OA: 0.9940 AA: 0.8557 Kappa: 0.6575 Precision: 0.6135 Recall: 0.7151 F1-Score: 0.6604\n",
      "*************************\n",
      "akk: 654, gt: 115907\n",
      "Epoch: 022 train_loss: 0.0362 train_acc: 99.4904 Precision: 0.7676 Recall: 0.5318 F1-Score: 0.6283\n",
      "akk: 666, gt: 115895\n",
      "Epoch: 023 train_loss: 0.0360 train_acc: 99.4955 Precision: 0.7673 Recall: 0.5413 F1-Score: 0.6348\n",
      "akk: 670, gt: 115891\n",
      "Epoch: 024 train_loss: 0.0354 train_acc: 99.5178 Precision: 0.7851 Recall: 0.5572 F1-Score: 0.6518\n",
      "akk: 667, gt: 115894\n",
      "Epoch: 025 train_loss: 0.0358 train_acc: 99.4930 Precision: 0.7646 Recall: 0.5403 F1-Score: 0.6331\n",
      "akk: 687, gt: 115874\n",
      "Epoch: 026 train_loss: 0.0357 train_acc: 99.5153 Precision: 0.7758 Recall: 0.5646 F1-Score: 0.6536\n",
      "akk: 670, gt: 115891\n",
      "Epoch: 027 train_loss: 0.0354 train_acc: 99.5076 Precision: 0.7761 Recall: 0.5508 F1-Score: 0.6444\n",
      "akk: 680, gt: 115881\n",
      "Epoch: 028 train_loss: 0.0358 train_acc: 99.5367 Precision: 0.7971 Recall: 0.5742 F1-Score: 0.6675\n",
      "akk: 680, gt: 115881\n",
      "Epoch: 029 train_loss: 0.0351 train_acc: 99.5144 Precision: 0.7779 Recall: 0.5604 F1-Score: 0.6515\n",
      "akk: 716, gt: 115845\n",
      "Epoch: 030 train_loss: 0.0347 train_acc: 99.5350 Precision: 0.7807 Recall: 0.5922 F1-Score: 0.6735\n",
      "akk: 682, gt: 115879\n",
      "Epoch: 031 train_loss: 0.0348 train_acc: 99.5058 Precision: 0.7698 Recall: 0.5561 F1-Score: 0.6458\n",
      "akk: 14262, gt: 3754524\n",
      "OA: 0.9946 AA: 0.6987 Kappa: 0.5401 Precision: 0.8516 Recall: 0.3980 F1-Score: 0.5425\n",
      "*************************\n",
      "akk: 679, gt: 115882\n",
      "Epoch: 032 train_loss: 0.0347 train_acc: 99.5256 Precision: 0.7879 Recall: 0.5667 F1-Score: 0.6593\n",
      "akk: 689, gt: 115872\n",
      "Epoch: 033 train_loss: 0.0347 train_acc: 99.5204 Precision: 0.7794 Recall: 0.5689 F1-Score: 0.6577\n",
      "akk: 693, gt: 115868\n",
      "Epoch: 034 train_loss: 0.0347 train_acc: 99.5187 Precision: 0.7763 Recall: 0.5699 F1-Score: 0.6573\n",
      "akk: 701, gt: 115860\n",
      "Epoch: 035 train_loss: 0.0346 train_acc: 99.5187 Precision: 0.7732 Recall: 0.5742 F1-Score: 0.6590\n",
      "akk: 666, gt: 115895\n",
      "Epoch: 036 train_loss: 0.0348 train_acc: 99.5196 Precision: 0.7883 Recall: 0.5561 F1-Score: 0.6522\n",
      "akk: 683, gt: 115878\n",
      "Epoch: 037 train_loss: 0.0346 train_acc: 99.5239 Precision: 0.7848 Recall: 0.5678 F1-Score: 0.6589\n",
      "akk: 693, gt: 115868\n",
      "Epoch: 038 train_loss: 0.0342 train_acc: 99.5376 Precision: 0.7922 Recall: 0.5816 F1-Score: 0.6707\n",
      "akk: 693, gt: 115868\n",
      "Epoch: 039 train_loss: 0.0343 train_acc: 99.5187 Precision: 0.7763 Recall: 0.5699 F1-Score: 0.6573\n",
      "akk: 693, gt: 115868\n",
      "Epoch: 040 train_loss: 0.0341 train_acc: 99.5410 Precision: 0.7951 Recall: 0.5837 F1-Score: 0.6732\n",
      "akk: 697, gt: 115864\n",
      "Epoch: 041 train_loss: 0.0339 train_acc: 99.5565 Precision: 0.8063 Recall: 0.5953 F1-Score: 0.6849\n",
      "akk: 19228, gt: 3749558\n",
      "OA: 0.9952 AA: 0.7593 Kappa: 0.6351 Precision: 0.8245 Recall: 0.5195 F1-Score: 0.6374\n",
      "*************************\n",
      "akk: 698, gt: 115863\n",
      "Epoch: 042 train_loss: 0.0337 train_acc: 99.5659 Precision: 0.8138 Recall: 0.6017 F1-Score: 0.6918\n",
      "akk: 695, gt: 115866\n",
      "Epoch: 043 train_loss: 0.0340 train_acc: 99.5496 Precision: 0.8014 Recall: 0.5900 F1-Score: 0.6797\n",
      "akk: 716, gt: 115845\n",
      "Epoch: 044 train_loss: 0.0340 train_acc: 99.5350 Precision: 0.7807 Recall: 0.5922 F1-Score: 0.6735\n",
      "akk: 732, gt: 115829\n",
      "Epoch: 045 train_loss: 0.0339 train_acc: 99.5384 Precision: 0.7773 Recall: 0.6028 F1-Score: 0.6790\n",
      "akk: 710, gt: 115851\n",
      "Epoch: 046 train_loss: 0.0338 train_acc: 99.5539 Precision: 0.7986 Recall: 0.6006 F1-Score: 0.6856\n",
      "akk: 723, gt: 115838\n",
      "Epoch: 047 train_loss: 0.0336 train_acc: 99.5479 Precision: 0.7884 Recall: 0.6038 F1-Score: 0.6839\n",
      "akk: 718, gt: 115843\n",
      "Epoch: 048 train_loss: 0.0337 train_acc: 99.5642 Precision: 0.8036 Recall: 0.6112 F1-Score: 0.6943\n",
      "akk: 719, gt: 115842\n",
      "Epoch: 049 train_loss: 0.0335 train_acc: 99.5599 Precision: 0.7997 Recall: 0.6091 F1-Score: 0.6915\n",
      "akk: 710, gt: 115851\n",
      "Epoch: 050 train_loss: 0.0332 train_acc: 99.5659 Precision: 0.8085 Recall: 0.6081 F1-Score: 0.6941\n",
      "akk: 722, gt: 115839\n",
      "Epoch: 051 train_loss: 0.0334 train_acc: 99.5625 Precision: 0.8006 Recall: 0.6123 F1-Score: 0.6939\n",
      "akk: 21947, gt: 3746839\n",
      "OA: 0.9951 AA: 0.7788 Kappa: 0.6479 Precision: 0.7772 Recall: 0.5590 F1-Score: 0.6503\n",
      "*************************\n",
      "akk: 706, gt: 115855\n",
      "Epoch: 052 train_loss: 0.0337 train_acc: 99.5350 Precision: 0.7847 Recall: 0.5869 F1-Score: 0.6715\n",
      "akk: 707, gt: 115854\n",
      "Epoch: 053 train_loss: 0.0334 train_acc: 99.5616 Precision: 0.8062 Recall: 0.6038 F1-Score: 0.6905\n",
      "akk: 704, gt: 115857\n",
      "Epoch: 054 train_loss: 0.0332 train_acc: 99.5642 Precision: 0.8097 Recall: 0.6038 F1-Score: 0.6917\n",
      "akk: 726, gt: 115835\n",
      "Epoch: 055 train_loss: 0.0334 train_acc: 99.5710 Precision: 0.8058 Recall: 0.6197 F1-Score: 0.7006\n",
      "akk: 715, gt: 115846\n",
      "Epoch: 056 train_loss: 0.0333 train_acc: 99.5547 Precision: 0.7972 Recall: 0.6038 F1-Score: 0.6872\n",
      "akk: 721, gt: 115840\n",
      "Epoch: 057 train_loss: 0.0332 train_acc: 99.5668 Precision: 0.8044 Recall: 0.6144 F1-Score: 0.6967\n",
      "akk: 720, gt: 115841\n",
      "Epoch: 058 train_loss: 0.0332 train_acc: 99.5676 Precision: 0.8056 Recall: 0.6144 F1-Score: 0.6971\n",
      "akk: 709, gt: 115852\n",
      "Epoch: 059 train_loss: 0.0331 train_acc: 99.5582 Precision: 0.8025 Recall: 0.6028 F1-Score: 0.6884\n",
      "akk: 719, gt: 115842\n",
      "Epoch: 060 train_loss: 0.0332 train_acc: 99.5650 Precision: 0.8039 Recall: 0.6123 F1-Score: 0.6951\n",
      "akk: 710, gt: 115851\n",
      "Epoch: 061 train_loss: 0.0331 train_acc: 99.5625 Precision: 0.8056 Recall: 0.6059 F1-Score: 0.6917\n",
      "akk: 7883, gt: 3760903\n",
      "OA: 0.9936 AA: 0.6179 Kappa: 0.3729 Precision: 0.9132 Recall: 0.2359 F1-Score: 0.3750\n",
      "*************************\n",
      "akk: 730, gt: 115831\n",
      "Epoch: 062 train_loss: 0.0329 train_acc: 99.5796 Precision: 0.8110 Recall: 0.6271 F1-Score: 0.7073\n",
      "akk: 719, gt: 115842\n",
      "Epoch: 063 train_loss: 0.0330 train_acc: 99.5685 Precision: 0.8067 Recall: 0.6144 F1-Score: 0.6975\n",
      "akk: 713, gt: 115848\n",
      "Epoch: 064 train_loss: 0.0329 train_acc: 99.5633 Precision: 0.8050 Recall: 0.6081 F1-Score: 0.6928\n",
      "akk: 712, gt: 115849\n",
      "Epoch: 065 train_loss: 0.0330 train_acc: 99.5504 Precision: 0.7949 Recall: 0.5996 F1-Score: 0.6836\n",
      "akk: 714, gt: 115847\n",
      "Epoch: 066 train_loss: 0.0328 train_acc: 99.5831 Precision: 0.8207 Recall: 0.6208 F1-Score: 0.7069\n",
      "akk: 763, gt: 115798\n",
      "Epoch: 067 train_loss: 0.0327 train_acc: 99.5788 Precision: 0.7969 Recall: 0.6441 F1-Score: 0.7124\n",
      "akk: 726, gt: 115835\n",
      "Epoch: 068 train_loss: 0.0329 train_acc: 99.5573 Precision: 0.7948 Recall: 0.6112 F1-Score: 0.6910\n",
      "akk: 709, gt: 115852\n",
      "Epoch: 069 train_loss: 0.0328 train_acc: 99.5736 Precision: 0.8152 Recall: 0.6123 F1-Score: 0.6993\n",
      "akk: 740, gt: 115821\n",
      "Epoch: 070 train_loss: 0.0325 train_acc: 99.5899 Precision: 0.8149 Recall: 0.6388 F1-Score: 0.7162\n",
      "akk: 730, gt: 115831\n",
      "Epoch: 071 train_loss: 0.0327 train_acc: 99.5865 Precision: 0.8164 Recall: 0.6314 F1-Score: 0.7121\n",
      "akk: 17698, gt: 3751088\n",
      "OA: 0.9951 AA: 0.7433 Kappa: 0.6145 Precision: 0.8402 Recall: 0.4873 F1-Score: 0.6168\n",
      "*************************\n",
      "akk: 752, gt: 115809\n",
      "Epoch: 072 train_loss: 0.0325 train_acc: 99.5882 Precision: 0.8085 Recall: 0.6441 F1-Score: 0.7170\n",
      "akk: 737, gt: 115824\n",
      "Epoch: 073 train_loss: 0.0324 train_acc: 99.5976 Precision: 0.8223 Recall: 0.6419 F1-Score: 0.7210\n",
      "akk: 764, gt: 115797\n",
      "Epoch: 074 train_loss: 0.0324 train_acc: 99.6071 Precision: 0.8181 Recall: 0.6621 F1-Score: 0.7319\n",
      "akk: 726, gt: 115835\n",
      "Epoch: 075 train_loss: 0.0324 train_acc: 99.5899 Precision: 0.8209 Recall: 0.6314 F1-Score: 0.7138\n",
      "akk: 741, gt: 115820\n",
      "Epoch: 076 train_loss: 0.0327 train_acc: 99.5873 Precision: 0.8124 Recall: 0.6377 F1-Score: 0.7145\n",
      "akk: 745, gt: 115816\n",
      "Epoch: 077 train_loss: 0.0324 train_acc: 99.5994 Precision: 0.8201 Recall: 0.6472 F1-Score: 0.7235\n",
      "akk: 768, gt: 115793\n",
      "Epoch: 078 train_loss: 0.0324 train_acc: 99.5831 Precision: 0.7982 Recall: 0.6494 F1-Score: 0.7161\n",
      "akk: 747, gt: 115814\n",
      "Epoch: 079 train_loss: 0.0323 train_acc: 99.6062 Precision: 0.8246 Recall: 0.6525 F1-Score: 0.7286\n",
      "akk: 738, gt: 115823\n",
      "Epoch: 080 train_loss: 0.0323 train_acc: 99.5882 Precision: 0.8144 Recall: 0.6367 F1-Score: 0.7146\n",
      "akk: 758, gt: 115803\n",
      "Epoch: 081 train_loss: 0.0324 train_acc: 99.6088 Precision: 0.8219 Recall: 0.6600 F1-Score: 0.7321\n",
      "akk: 18058, gt: 3750728\n",
      "OA: 0.9952 AA: 0.7482 Kappa: 0.6224 Precision: 0.8402 Recall: 0.4972 F1-Score: 0.6247\n",
      "*************************\n",
      "akk: 751, gt: 115810\n",
      "Epoch: 082 train_loss: 0.0322 train_acc: 99.6028 Precision: 0.8202 Recall: 0.6525 F1-Score: 0.7268\n",
      "akk: 746, gt: 115815\n",
      "Epoch: 083 train_loss: 0.0324 train_acc: 99.6054 Precision: 0.8244 Recall: 0.6515 F1-Score: 0.7278\n",
      "akk: 766, gt: 115795\n",
      "Epoch: 084 train_loss: 0.0330 train_acc: 99.6208 Precision: 0.8277 Recall: 0.6716 F1-Score: 0.7415\n",
      "akk: 746, gt: 115815\n",
      "Epoch: 085 train_loss: 0.0322 train_acc: 99.6191 Precision: 0.8351 Recall: 0.6600 F1-Score: 0.7373\n",
      "akk: 736, gt: 115825\n",
      "Epoch: 086 train_loss: 0.0320 train_acc: 99.5882 Precision: 0.8152 Recall: 0.6356 F1-Score: 0.7143\n",
      "akk: 744, gt: 115817\n",
      "Epoch: 087 train_loss: 0.0322 train_acc: 99.6019 Precision: 0.8226 Recall: 0.6483 F1-Score: 0.7251\n",
      "akk: 740, gt: 115821\n",
      "Epoch: 088 train_loss: 0.0322 train_acc: 99.6088 Precision: 0.8297 Recall: 0.6504 F1-Score: 0.7292\n",
      "akk: 745, gt: 115816\n",
      "Epoch: 089 train_loss: 0.0322 train_acc: 99.6251 Precision: 0.8403 Recall: 0.6631 F1-Score: 0.7413\n",
      "akk: 756, gt: 115805\n",
      "Epoch: 090 train_loss: 0.0318 train_acc: 99.6294 Precision: 0.8386 Recall: 0.6716 F1-Score: 0.7459\n",
      "akk: 759, gt: 115802\n",
      "Epoch: 091 train_loss: 0.0320 train_acc: 99.6096 Precision: 0.8221 Recall: 0.6610 F1-Score: 0.7328\n",
      "akk: 19667, gt: 3749119\n",
      "OA: 0.9954 AA: 0.7683 Kappa: 0.6514 Precision: 0.8339 Recall: 0.5374 F1-Score: 0.6536\n",
      "*************************\n",
      "akk: 763, gt: 115798\n",
      "Epoch: 092 train_loss: 0.0319 train_acc: 99.6182 Precision: 0.8270 Recall: 0.6684 F1-Score: 0.7393\n",
      "akk: 764, gt: 115797\n",
      "Epoch: 093 train_loss: 0.0318 train_acc: 99.6242 Precision: 0.8312 Recall: 0.6727 F1-Score: 0.7436\n",
      "akk: 740, gt: 115821\n",
      "Epoch: 094 train_loss: 0.0318 train_acc: 99.6174 Precision: 0.8365 Recall: 0.6557 F1-Score: 0.7352\n",
      "akk: 761, gt: 115800\n",
      "Epoch: 095 train_loss: 0.0320 train_acc: 99.6165 Precision: 0.8265 Recall: 0.6663 F1-Score: 0.7378\n",
      "akk: 753, gt: 115808\n",
      "Epoch: 096 train_loss: 0.0318 train_acc: 99.6148 Precision: 0.8287 Recall: 0.6610 F1-Score: 0.7354\n",
      "akk: 770, gt: 115791\n",
      "Epoch: 097 train_loss: 0.0318 train_acc: 99.6294 Precision: 0.8325 Recall: 0.6790 F1-Score: 0.7480\n",
      "akk: 752, gt: 115809\n",
      "Epoch: 098 train_loss: 0.0316 train_acc: 99.6242 Precision: 0.8364 Recall: 0.6663 F1-Score: 0.7417\n",
      "akk: 760, gt: 115801\n",
      "Epoch: 099 train_loss: 0.0317 train_acc: 99.6448 Precision: 0.8487 Recall: 0.6833 F1-Score: 0.7570\n",
      "akk: 781, gt: 115780\n",
      "Epoch: 100 train_loss: 0.0317 train_acc: 99.6371 Precision: 0.8335 Recall: 0.6896 F1-Score: 0.7548\n",
      "akk: 10468, gt: 3758318\n",
      "OA: 0.9942 AA: 0.6556 Kappa: 0.4617 Precision: 0.9081 Recall: 0.3115 F1-Score: 0.4639\n",
      "*************************\n",
      "Running Time: 2958.72\n",
      "**************************************************\n",
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [6:25:35<2:45:48, 3316.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "F1 score: 0.0172739162109735\n",
      "Precision: 0.00888222120521993\n",
      "Recall: 0.31278807336533265\n",
      "max_label: 1 max_pred: 1\n",
      "Accuracy: 0.7762455920515534\n",
      "Confusion Matrix:\n",
      "[[3874295 1097991]\n",
      " [  21619    9840]]\n",
      "**************************************************\n",
      "Parameter:\n",
      "fix_random: True\n",
      "gpu_id: 0\n",
      "seed: 0\n",
      "dataset: BS\n",
      "flag_test: train\n",
      "model_name: conv2d_unmix\n",
      "batch_size: 128\n",
      "test_freq: 10\n",
      "patches: 5\n",
      "epoches: 100\n",
      "inference_model: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "learning_rate: 0.0001\n",
      "gamma: 0.9\n",
      "weight_decay: 0\n",
      "Train data change to the ratio of train samples: 0.03\n",
      "height=2715,width=1843,band=150\n",
      "**************************************************\n",
      "patch is : 5\n",
      "mirror_image shape : [2719,1847,150]\n",
      "**************************************************\n",
      "x_train shape = (116561, 5, 5, 150), type = float32\n",
      "x_test  shape = (3768786, 5, 5, 150), type = float32\n",
      "x_true  shape = (5003745, 5, 5, 150), type = float32\n",
      "**************************************************\n",
      "y_train: shape = (116561,) ,type = int64\n",
      "y_test: shape = (3768786,) ,type = int64\n",
      "y_true: shape = (5003745,) ,type = int64\n",
      "**************************************************\n",
      "Model Name: conv2d_unmix\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akk: 2602, gt: 113959\n",
      "Epoch: 001 train_loss: 0.2264 train_acc: 97.0917 Precision: 0.0300 Recall: 0.0826 F1-Score: 0.0440\n",
      "akk: 8967, gt: 3759819\n",
      "OA: 0.9906 AA: 0.5316 Kappa: 0.0974 Precision: 0.2217 Recall: 0.0651 F1-Score: 0.1007\n",
      "*************************\n",
      "akk: 134, gt: 116427\n",
      "Epoch: 002 train_loss: 0.0642 train_acc: 99.1627 Precision: 0.3806 Recall: 0.0540 F1-Score: 0.0946\n",
      "akk: 307, gt: 116254\n",
      "Epoch: 003 train_loss: 0.0530 train_acc: 99.1927 Precision: 0.5049 Recall: 0.1642 F1-Score: 0.2478\n",
      "akk: 428, gt: 116133\n",
      "Epoch: 004 train_loss: 0.0471 train_acc: 99.2141 Precision: 0.5327 Recall: 0.2415 F1-Score: 0.3324\n",
      "akk: 461, gt: 116100\n",
      "Epoch: 005 train_loss: 0.0444 train_acc: 99.2768 Precision: 0.6095 Recall: 0.2977 F1-Score: 0.4000\n",
      "akk: 388, gt: 116173\n",
      "Epoch: 006 train_loss: 0.0462 train_acc: 99.2862 Precision: 0.6443 Recall: 0.2648 F1-Score: 0.3754\n",
      "akk: 489, gt: 116072\n",
      "Epoch: 007 train_loss: 0.0422 train_acc: 99.3437 Precision: 0.6830 Recall: 0.3538 F1-Score: 0.4662\n",
      "akk: 501, gt: 116060\n",
      "Epoch: 008 train_loss: 0.0412 train_acc: 99.3660 Precision: 0.7046 Recall: 0.3739 F1-Score: 0.4886\n",
      "akk: 494, gt: 116067\n",
      "Epoch: 009 train_loss: 0.0408 train_acc: 99.3669 Precision: 0.7085 Recall: 0.3708 F1-Score: 0.4868\n",
      "akk: 541, gt: 116020\n",
      "Epoch: 010 train_loss: 0.0395 train_acc: 99.3626 Precision: 0.6858 Recall: 0.3930 F1-Score: 0.4997\n",
      "akk: 560, gt: 116001\n",
      "Epoch: 011 train_loss: 0.0401 train_acc: 99.3909 Precision: 0.7089 Recall: 0.4206 F1-Score: 0.5279\n",
      "akk: 12900, gt: 3755886\n",
      "OA: 0.9943 AA: 0.6792 Kappa: 0.5021 Precision: 0.8489 Recall: 0.3589 F1-Score: 0.5045\n",
      "*************************\n",
      "akk: 588, gt: 115973\n",
      "Epoch: 012 train_loss: 0.0385 train_acc: 99.4115 Precision: 0.7194 Recall: 0.4481 F1-Score: 0.5522\n",
      "akk: 550, gt: 116011\n",
      "Epoch: 013 train_loss: 0.0389 train_acc: 99.4029 Precision: 0.7255 Recall: 0.4227 F1-Score: 0.5341\n",
      "akk: 596, gt: 115965\n",
      "Epoch: 014 train_loss: 0.0387 train_acc: 99.4286 Precision: 0.7332 Recall: 0.4629 F1-Score: 0.5675\n",
      "akk: 560, gt: 116001\n",
      "Epoch: 015 train_loss: 0.0393 train_acc: 99.4200 Precision: 0.7393 Recall: 0.4386 F1-Score: 0.5505\n",
      "akk: 645, gt: 115916\n",
      "Epoch: 016 train_loss: 0.0379 train_acc: 99.4398 Precision: 0.7256 Recall: 0.4958 F1-Score: 0.5890\n",
      "akk: 608, gt: 115953\n",
      "Epoch: 017 train_loss: 0.0383 train_acc: 99.4424 Precision: 0.7418 Recall: 0.4778 F1-Score: 0.5812\n",
      "akk: 582, gt: 115979\n",
      "Epoch: 018 train_loss: 0.0376 train_acc: 99.4200 Precision: 0.7302 Recall: 0.4502 F1-Score: 0.5570\n",
      "akk: 615, gt: 115946\n",
      "Epoch: 019 train_loss: 0.0376 train_acc: 99.4501 Precision: 0.7463 Recall: 0.4862 F1-Score: 0.5888\n",
      "akk: 634, gt: 115927\n",
      "Epoch: 020 train_loss: 0.0371 train_acc: 99.4544 Precision: 0.7429 Recall: 0.4989 F1-Score: 0.5970\n",
      "akk: 649, gt: 115912\n",
      "Epoch: 021 train_loss: 0.0368 train_acc: 99.4775 Precision: 0.7581 Recall: 0.5212 F1-Score: 0.6177\n",
      "akk: 22258, gt: 3746528\n",
      "OA: 0.9952 AA: 0.7829 Kappa: 0.6534 Precision: 0.7774 Recall: 0.5671 F1-Score: 0.6558\n",
      "*************************\n",
      "akk: 622, gt: 115939\n",
      "Epoch: 022 train_loss: 0.0373 train_acc: 99.4424 Precision: 0.7363 Recall: 0.4852 F1-Score: 0.5849\n",
      "akk: 632, gt: 115929\n",
      "Epoch: 023 train_loss: 0.0368 train_acc: 99.4578 Precision: 0.7468 Recall: 0.5000 F1-Score: 0.5990\n",
      "akk: 623, gt: 115938\n",
      "Epoch: 024 train_loss: 0.0368 train_acc: 99.4329 Precision: 0.7271 Recall: 0.4799 F1-Score: 0.5782\n",
      "akk: 644, gt: 115917\n",
      "Epoch: 025 train_loss: 0.0363 train_acc: 99.4698 Precision: 0.7531 Recall: 0.5138 F1-Score: 0.6108\n",
      "akk: 678, gt: 115883\n",
      "Epoch: 026 train_loss: 0.0359 train_acc: 99.5024 Precision: 0.7684 Recall: 0.5519 F1-Score: 0.6424\n",
      "akk: 687, gt: 115874\n",
      "Epoch: 027 train_loss: 0.0355 train_acc: 99.4878 Precision: 0.7525 Recall: 0.5477 F1-Score: 0.6340\n",
      "akk: 672, gt: 115889\n",
      "Epoch: 028 train_loss: 0.0359 train_acc: 99.4938 Precision: 0.7634 Recall: 0.5434 F1-Score: 0.6349\n",
      "akk: 663, gt: 115898\n",
      "Epoch: 029 train_loss: 0.0356 train_acc: 99.4913 Precision: 0.7647 Recall: 0.5371 F1-Score: 0.6310\n",
      "akk: 660, gt: 115901\n",
      "Epoch: 030 train_loss: 0.0355 train_acc: 99.4870 Precision: 0.7621 Recall: 0.5328 F1-Score: 0.6272\n",
      "akk: 685, gt: 115876\n",
      "Epoch: 031 train_loss: 0.0351 train_acc: 99.4964 Precision: 0.7606 Recall: 0.5519 F1-Score: 0.6397\n",
      "akk: 19269, gt: 3749517\n",
      "OA: 0.9953 AA: 0.7627 Kappa: 0.6429 Precision: 0.8335 Recall: 0.5263 F1-Score: 0.6452\n",
      "*************************\n",
      "akk: 695, gt: 115866\n",
      "Epoch: 032 train_loss: 0.0351 train_acc: 99.4998 Precision: 0.7597 Recall: 0.5593 F1-Score: 0.6443\n",
      "akk: 696, gt: 115865\n",
      "Epoch: 033 train_loss: 0.0349 train_acc: 99.5076 Precision: 0.7658 Recall: 0.5646 F1-Score: 0.6500\n",
      "akk: 678, gt: 115883\n",
      "Epoch: 034 train_loss: 0.0346 train_acc: 99.5178 Precision: 0.7817 Recall: 0.5614 F1-Score: 0.6535\n",
      "akk: 694, gt: 115867\n",
      "Epoch: 035 train_loss: 0.0347 train_acc: 99.5127 Precision: 0.7709 Recall: 0.5667 F1-Score: 0.6532\n",
      "akk: 726, gt: 115835\n",
      "Epoch: 036 train_loss: 0.0347 train_acc: 99.5110 Precision: 0.7576 Recall: 0.5826 F1-Score: 0.6587\n",
      "akk: 689, gt: 115872\n",
      "Epoch: 037 train_loss: 0.0345 train_acc: 99.5101 Precision: 0.7707 Recall: 0.5625 F1-Score: 0.6503\n",
      "akk: 726, gt: 115835\n",
      "Epoch: 038 train_loss: 0.0345 train_acc: 99.5178 Precision: 0.7631 Recall: 0.5869 F1-Score: 0.6635\n",
      "akk: 671, gt: 115890\n",
      "Epoch: 039 train_loss: 0.0342 train_acc: 99.5170 Precision: 0.7839 Recall: 0.5572 F1-Score: 0.6514\n",
      "akk: 698, gt: 115863\n",
      "Epoch: 040 train_loss: 0.0340 train_acc: 99.5333 Precision: 0.7865 Recall: 0.5816 F1-Score: 0.6687\n",
      "akk: 719, gt: 115842\n",
      "Epoch: 041 train_loss: 0.0337 train_acc: 99.5427 Precision: 0.7858 Recall: 0.5985 F1-Score: 0.6795\n",
      "akk: 22387, gt: 3746399\n",
      "OA: 0.9954 AA: 0.7903 Kappa: 0.6689 Precision: 0.7930 Recall: 0.5818 F1-Score: 0.6712\n",
      "*************************\n",
      "akk: 694, gt: 115867\n",
      "Epoch: 042 train_loss: 0.0339 train_acc: 99.5247 Precision: 0.7810 Recall: 0.5742 F1-Score: 0.6618\n",
      "akk: 725, gt: 115836\n",
      "Epoch: 043 train_loss: 0.0341 train_acc: 99.5204 Precision: 0.7655 Recall: 0.5879 F1-Score: 0.6651\n",
      "akk: 729, gt: 115832\n",
      "Epoch: 044 train_loss: 0.0339 train_acc: 99.5153 Precision: 0.7599 Recall: 0.5869 F1-Score: 0.6623\n",
      "akk: 707, gt: 115854\n",
      "Epoch: 045 train_loss: 0.0336 train_acc: 99.5496 Precision: 0.7963 Recall: 0.5964 F1-Score: 0.6820\n",
      "akk: 750, gt: 115811\n",
      "Epoch: 046 train_loss: 0.0337 train_acc: 99.5539 Precision: 0.7827 Recall: 0.6218 F1-Score: 0.6930\n",
      "akk: 709, gt: 115852\n",
      "Epoch: 047 train_loss: 0.0334 train_acc: 99.5565 Precision: 0.8011 Recall: 0.6017 F1-Score: 0.6872\n",
      "akk: 709, gt: 115852\n",
      "Epoch: 048 train_loss: 0.0334 train_acc: 99.5444 Precision: 0.7913 Recall: 0.5943 F1-Score: 0.6788\n",
      "akk: 743, gt: 115818\n",
      "Epoch: 049 train_loss: 0.0330 train_acc: 99.5582 Precision: 0.7887 Recall: 0.6208 F1-Score: 0.6947\n",
      "akk: 709, gt: 115852\n",
      "Epoch: 050 train_loss: 0.0334 train_acc: 99.5530 Precision: 0.7983 Recall: 0.5996 F1-Score: 0.6848\n",
      "akk: 745, gt: 115816\n",
      "Epoch: 051 train_loss: 0.0328 train_acc: 99.5582 Precision: 0.7879 Recall: 0.6218 F1-Score: 0.6951\n",
      "akk: 25056, gt: 3743730\n",
      "OA: 0.9955 AA: 0.8153 Kappa: 0.6920 Precision: 0.7699 Recall: 0.6321 F1-Score: 0.6942\n",
      "*************************\n",
      "akk: 736, gt: 115825\n",
      "Epoch: 052 train_loss: 0.0329 train_acc: 99.5882 Precision: 0.8152 Recall: 0.6356 F1-Score: 0.7143\n",
      "akk: 747, gt: 115814\n",
      "Epoch: 053 train_loss: 0.0327 train_acc: 99.5547 Precision: 0.7845 Recall: 0.6208 F1-Score: 0.6931\n",
      "akk: 750, gt: 115811\n",
      "Epoch: 054 train_loss: 0.0326 train_acc: 99.5916 Precision: 0.8120 Recall: 0.6451 F1-Score: 0.7190\n",
      "akk: 740, gt: 115821\n",
      "Epoch: 055 train_loss: 0.0329 train_acc: 99.5590 Precision: 0.7905 Recall: 0.6197 F1-Score: 0.6948\n",
      "akk: 742, gt: 115819\n",
      "Epoch: 056 train_loss: 0.0325 train_acc: 99.5728 Precision: 0.8005 Recall: 0.6292 F1-Score: 0.7046\n",
      "akk: 749, gt: 115812\n",
      "Epoch: 057 train_loss: 0.0325 train_acc: 99.6011 Precision: 0.8198 Recall: 0.6504 F1-Score: 0.7253\n",
      "akk: 759, gt: 115802\n",
      "Epoch: 058 train_loss: 0.0324 train_acc: 99.5856 Precision: 0.8037 Recall: 0.6462 F1-Score: 0.7164\n",
      "akk: 745, gt: 115816\n",
      "Epoch: 059 train_loss: 0.0325 train_acc: 99.5839 Precision: 0.8081 Recall: 0.6377 F1-Score: 0.7128\n",
      "akk: 754, gt: 115807\n",
      "Epoch: 060 train_loss: 0.0323 train_acc: 99.5848 Precision: 0.8050 Recall: 0.6430 F1-Score: 0.7150\n",
      "akk: 730, gt: 115831\n",
      "Epoch: 061 train_loss: 0.0321 train_acc: 99.5951 Precision: 0.8233 Recall: 0.6367 F1-Score: 0.7180\n",
      "akk: 13982, gt: 3754804\n",
      "OA: 0.9947 AA: 0.6992 Kappa: 0.5449 Precision: 0.8707 Recall: 0.3990 F1-Score: 0.5472\n",
      "*************************\n",
      "akk: 751, gt: 115810\n",
      "Epoch: 062 train_loss: 0.0320 train_acc: 99.6045 Precision: 0.8216 Recall: 0.6536 F1-Score: 0.7280\n",
      "akk: 774, gt: 115787\n",
      "Epoch: 063 train_loss: 0.0320 train_acc: 99.5968 Precision: 0.8062 Recall: 0.6610 F1-Score: 0.7264\n",
      "akk: 754, gt: 115807\n",
      "Epoch: 064 train_loss: 0.0320 train_acc: 99.6088 Precision: 0.8236 Recall: 0.6578 F1-Score: 0.7314\n",
      "akk: 759, gt: 115802\n",
      "Epoch: 065 train_loss: 0.0319 train_acc: 99.6045 Precision: 0.8182 Recall: 0.6578 F1-Score: 0.7293\n",
      "akk: 741, gt: 115820\n",
      "Epoch: 066 train_loss: 0.0319 train_acc: 99.5976 Precision: 0.8205 Recall: 0.6441 F1-Score: 0.7217\n",
      "akk: 759, gt: 115802\n",
      "Epoch: 067 train_loss: 0.0318 train_acc: 99.6148 Precision: 0.8261 Recall: 0.6642 F1-Score: 0.7363\n",
      "akk: 776, gt: 115785\n",
      "Epoch: 068 train_loss: 0.0316 train_acc: 99.6208 Precision: 0.8235 Recall: 0.6769 F1-Score: 0.7430\n",
      "akk: 758, gt: 115803\n",
      "Epoch: 069 train_loss: 0.0317 train_acc: 99.6294 Precision: 0.8377 Recall: 0.6727 F1-Score: 0.7462\n",
      "akk: 781, gt: 115780\n",
      "Epoch: 070 train_loss: 0.0315 train_acc: 99.6217 Precision: 0.8220 Recall: 0.6801 F1-Score: 0.7443\n",
      "akk: 781, gt: 115780\n",
      "Epoch: 071 train_loss: 0.0316 train_acc: 99.6217 Precision: 0.8220 Recall: 0.6801 F1-Score: 0.7443\n",
      "akk: 25695, gt: 3743091\n",
      "OA: 0.9953 AA: 0.8144 Kappa: 0.6822 Precision: 0.7487 Recall: 0.6305 F1-Score: 0.6845\n",
      "*************************\n",
      "akk: 779, gt: 115782\n",
      "Epoch: 072 train_loss: 0.0314 train_acc: 99.6508 Precision: 0.8447 Recall: 0.6970 F1-Score: 0.7638\n",
      "akk: 784, gt: 115777\n",
      "Epoch: 073 train_loss: 0.0313 train_acc: 99.6294 Precision: 0.8265 Recall: 0.6864 F1-Score: 0.7500\n",
      "akk: 772, gt: 115789\n",
      "Epoch: 074 train_loss: 0.0313 train_acc: 99.6465 Precision: 0.8446 Recall: 0.6907 F1-Score: 0.7599\n",
      "akk: 782, gt: 115779\n",
      "Epoch: 075 train_loss: 0.0311 train_acc: 99.6671 Precision: 0.8555 Recall: 0.7087 F1-Score: 0.7752\n",
      "akk: 776, gt: 115785\n",
      "Epoch: 076 train_loss: 0.0312 train_acc: 99.6551 Precision: 0.8492 Recall: 0.6981 F1-Score: 0.7663\n",
      "akk: 797, gt: 115764\n",
      "Epoch: 077 train_loss: 0.0310 train_acc: 99.6422 Precision: 0.8306 Recall: 0.7013 F1-Score: 0.7605\n",
      "akk: 774, gt: 115787\n",
      "Epoch: 078 train_loss: 0.0312 train_acc: 99.6328 Precision: 0.8333 Recall: 0.6833 F1-Score: 0.7509\n",
      "akk: 784, gt: 115777\n",
      "Epoch: 079 train_loss: 0.0313 train_acc: 99.6465 Precision: 0.8393 Recall: 0.6970 F1-Score: 0.7616\n",
      "akk: 809, gt: 115752\n",
      "Epoch: 080 train_loss: 0.0309 train_acc: 99.6474 Precision: 0.8294 Recall: 0.7108 F1-Score: 0.7655\n",
      "akk: 810, gt: 115751\n",
      "Epoch: 081 train_loss: 0.0308 train_acc: 99.6551 Precision: 0.8346 Recall: 0.7161 F1-Score: 0.7708\n",
      "akk: 26427, gt: 3742359\n",
      "OA: 0.9952 AA: 0.8171 Kappa: 0.6793 Precision: 0.7345 Recall: 0.6361 F1-Score: 0.6817\n",
      "*************************\n",
      "akk: 789, gt: 115772\n",
      "Epoch: 082 train_loss: 0.0308 train_acc: 99.6783 Precision: 0.8606 Recall: 0.7193 F1-Score: 0.7836\n",
      "akk: 789, gt: 115772\n",
      "Epoch: 083 train_loss: 0.0309 train_acc: 99.6663 Precision: 0.8517 Recall: 0.7119 F1-Score: 0.7755\n",
      "akk: 765, gt: 115796\n",
      "Epoch: 084 train_loss: 0.0307 train_acc: 99.6731 Precision: 0.8680 Recall: 0.7034 F1-Score: 0.7771\n",
      "akk: 827, gt: 115734\n",
      "Epoch: 085 train_loss: 0.0307 train_acc: 99.6731 Precision: 0.8404 Recall: 0.7362 F1-Score: 0.7849\n",
      "akk: 785, gt: 115776\n",
      "Epoch: 086 train_loss: 0.0307 train_acc: 99.6697 Precision: 0.8561 Recall: 0.7119 F1-Score: 0.7773\n",
      "akk: 791, gt: 115770\n",
      "Epoch: 087 train_loss: 0.0306 train_acc: 99.6680 Precision: 0.8521 Recall: 0.7140 F1-Score: 0.7769\n",
      "akk: 815, gt: 115746\n",
      "Epoch: 088 train_loss: 0.0305 train_acc: 99.6766 Precision: 0.8479 Recall: 0.7320 F1-Score: 0.7857\n",
      "akk: 793, gt: 115768\n",
      "Epoch: 089 train_loss: 0.0305 train_acc: 99.6817 Precision: 0.8613 Recall: 0.7235 F1-Score: 0.7864\n",
      "akk: 807, gt: 115754\n",
      "Epoch: 090 train_loss: 0.0301 train_acc: 99.6989 Precision: 0.8674 Recall: 0.7415 F1-Score: 0.7995\n",
      "akk: 825, gt: 115736\n",
      "Epoch: 091 train_loss: 0.0301 train_acc: 99.6937 Precision: 0.8558 Recall: 0.7479 F1-Score: 0.7982\n",
      "akk: 24198, gt: 3744588\n",
      "OA: 0.9952 AA: 0.7977 Kappa: 0.6636 Precision: 0.7530 Recall: 0.5971 F1-Score: 0.6660\n",
      "*************************\n",
      "akk: 803, gt: 115758\n",
      "Epoch: 092 train_loss: 0.0302 train_acc: 99.7040 Precision: 0.8730 Recall: 0.7426 F1-Score: 0.8025\n",
      "akk: 806, gt: 115755\n",
      "Epoch: 093 train_loss: 0.0303 train_acc: 99.6860 Precision: 0.8586 Recall: 0.7331 F1-Score: 0.7909\n",
      "akk: 836, gt: 115725\n",
      "Epoch: 094 train_loss: 0.0300 train_acc: 99.7049 Precision: 0.8589 Recall: 0.7606 F1-Score: 0.8067\n",
      "akk: 824, gt: 115737\n",
      "Epoch: 095 train_loss: 0.0300 train_acc: 99.7083 Precision: 0.8665 Recall: 0.7564 F1-Score: 0.8077\n",
      "akk: 822, gt: 115739\n",
      "Epoch: 096 train_loss: 0.0305 train_acc: 99.6946 Precision: 0.8577 Recall: 0.7468 F1-Score: 0.7984\n",
      "akk: 819, gt: 115742\n",
      "Epoch: 097 train_loss: 0.0298 train_acc: 99.7177 Precision: 0.8755 Recall: 0.7595 F1-Score: 0.8134\n",
      "akk: 824, gt: 115737\n",
      "Epoch: 098 train_loss: 0.0299 train_acc: 99.7032 Precision: 0.8629 Recall: 0.7532 F1-Score: 0.8043\n",
      "akk: 823, gt: 115738\n",
      "Epoch: 099 train_loss: 0.0297 train_acc: 99.7298 Precision: 0.8821 Recall: 0.7691 F1-Score: 0.8217\n",
      "akk: 809, gt: 115752\n",
      "Epoch: 100 train_loss: 0.0298 train_acc: 99.7040 Precision: 0.8702 Recall: 0.7458 F1-Score: 0.8032\n",
      "akk: 19525, gt: 3749261\n",
      "OA: 0.9950 AA: 0.7564 Kappa: 0.6244 Precision: 0.8031 Recall: 0.5139 F1-Score: 0.6267\n",
      "*************************\n",
      "Running Time: 2962.36\n",
      "**************************************************\n",
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [7:20:48<1:50:30, 3315.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "F1 score: 0.6319332933506162\n",
      "Precision: 0.80731586752348\n",
      "Recall: 0.5191519120124607\n",
      "max_label: 1 max_pred: 1\n",
      "Accuracy: 0.9961978478119888\n",
      "Confusion Matrix:\n",
      "[[4968388    3898]\n",
      " [  15127   16332]]\n",
      "**************************************************\n",
      "Parameter:\n",
      "fix_random: True\n",
      "gpu_id: 0\n",
      "seed: 0\n",
      "dataset: BS\n",
      "flag_test: train\n",
      "model_name: conv2d_unmix\n",
      "batch_size: 128\n",
      "test_freq: 10\n",
      "patches: 5\n",
      "epoches: 100\n",
      "inference_model: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "learning_rate: 0.0001\n",
      "gamma: 0.9\n",
      "weight_decay: 0\n",
      "Train data change to the ratio of train samples: 0.03\n",
      "height=2715,width=1843,band=150\n",
      "**************************************************\n",
      "patch is : 5\n",
      "mirror_image shape : [2719,1847,150]\n",
      "**************************************************\n",
      "x_train shape = (116561, 5, 5, 150), type = float32\n",
      "x_test  shape = (3768786, 5, 5, 150), type = float32\n",
      "x_true  shape = (5003745, 5, 5, 150), type = float32\n",
      "**************************************************\n",
      "y_train: shape = (116561,) ,type = int64\n",
      "y_test: shape = (3768786,) ,type = int64\n",
      "y_true: shape = (5003745,) ,type = int64\n",
      "**************************************************\n",
      "Model Name: conv2d_unmix\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akk: 7299, gt: 109262\n",
      "Epoch: 001 train_loss: 0.5883 train_acc: 93.1752 Precision: 0.0197 Recall: 0.1525 F1-Score: 0.0349\n",
      "akk: 266, gt: 3768520\n",
      "OA: 0.9918 AA: 0.5000 Kappa: -0.0000 Precision: 0.0075 Recall: 0.0001 F1-Score: 0.0001\n",
      "*************************\n",
      "akk: 4646, gt: 111915\n",
      "Epoch: 002 train_loss: 0.1602 train_acc: 95.5337 Precision: 0.0413 Recall: 0.2034 F1-Score: 0.0687\n",
      "akk: 2493, gt: 114068\n",
      "Epoch: 003 train_loss: 0.0889 train_acc: 97.4031 Precision: 0.0822 Recall: 0.2172 F1-Score: 0.1193\n",
      "akk: 1134, gt: 115427\n",
      "Epoch: 004 train_loss: 0.0651 train_acc: 98.5312 Precision: 0.1614 Recall: 0.1939 F1-Score: 0.1761\n",
      "akk: 642, gt: 115919\n",
      "Epoch: 005 train_loss: 0.0493 train_acc: 99.0906 Precision: 0.4097 Recall: 0.2786 F1-Score: 0.3317\n",
      "akk: 476, gt: 116085\n",
      "Epoch: 006 train_loss: 0.0425 train_acc: 99.3548 Precision: 0.7017 Recall: 0.3538 F1-Score: 0.4704\n",
      "akk: 500, gt: 116061\n",
      "Epoch: 007 train_loss: 0.0408 train_acc: 99.3737 Precision: 0.7140 Recall: 0.3782 F1-Score: 0.4945\n",
      "akk: 559, gt: 116002\n",
      "Epoch: 008 train_loss: 0.0399 train_acc: 99.4123 Precision: 0.7317 Recall: 0.4333 F1-Score: 0.5442\n",
      "akk: 569, gt: 115992\n",
      "Epoch: 009 train_loss: 0.0386 train_acc: 99.4346 Precision: 0.7504 Recall: 0.4523 F1-Score: 0.5644\n",
      "akk: 615, gt: 115946\n",
      "Epoch: 010 train_loss: 0.0383 train_acc: 99.4501 Precision: 0.7463 Recall: 0.4862 F1-Score: 0.5888\n",
      "akk: 620, gt: 115941\n",
      "Epoch: 011 train_loss: 0.0382 train_acc: 99.4458 Precision: 0.7403 Recall: 0.4862 F1-Score: 0.5870\n",
      "akk: 23279, gt: 3745507\n",
      "OA: 0.9953 AA: 0.7939 Kappa: 0.6661 Precision: 0.7723 Recall: 0.5892 F1-Score: 0.6684\n",
      "*************************\n",
      "akk: 604, gt: 115957\n",
      "Epoch: 012 train_loss: 0.0375 train_acc: 99.4715 Precision: 0.7715 Recall: 0.4936 F1-Score: 0.6021\n",
      "akk: 621, gt: 115940\n",
      "Epoch: 013 train_loss: 0.0374 train_acc: 99.4878 Precision: 0.7794 Recall: 0.5127 F1-Score: 0.6185\n",
      "akk: 613, gt: 115948\n",
      "Epoch: 014 train_loss: 0.0377 train_acc: 99.4741 Precision: 0.7700 Recall: 0.5000 F1-Score: 0.6063\n",
      "akk: 667, gt: 115894\n",
      "Epoch: 015 train_loss: 0.0368 train_acc: 99.4775 Precision: 0.7511 Recall: 0.5307 F1-Score: 0.6220\n",
      "akk: 645, gt: 115916\n",
      "Epoch: 016 train_loss: 0.0364 train_acc: 99.4998 Precision: 0.7798 Recall: 0.5328 F1-Score: 0.6331\n",
      "akk: 607, gt: 115954\n",
      "Epoch: 017 train_loss: 0.0373 train_acc: 99.4827 Precision: 0.7809 Recall: 0.5021 F1-Score: 0.6112\n",
      "akk: 654, gt: 115907\n",
      "Epoch: 018 train_loss: 0.0363 train_acc: 99.4955 Precision: 0.7722 Recall: 0.5350 F1-Score: 0.6320\n",
      "akk: 647, gt: 115914\n",
      "Epoch: 019 train_loss: 0.0364 train_acc: 99.4981 Precision: 0.7774 Recall: 0.5328 F1-Score: 0.6323\n",
      "akk: 660, gt: 115901\n",
      "Epoch: 020 train_loss: 0.0360 train_acc: 99.5076 Precision: 0.7803 Recall: 0.5456 F1-Score: 0.6421\n",
      "akk: 664, gt: 115897\n",
      "Epoch: 021 train_loss: 0.0367 train_acc: 99.4904 Precision: 0.7636 Recall: 0.5371 F1-Score: 0.6306\n",
      "akk: 18275, gt: 3750511\n",
      "OA: 0.9950 AA: 0.7455 Kappa: 0.6129 Precision: 0.8213 Recall: 0.4919 F1-Score: 0.6153\n",
      "*************************\n",
      "akk: 685, gt: 115876\n",
      "Epoch: 022 train_loss: 0.0358 train_acc: 99.5136 Precision: 0.7752 Recall: 0.5625 F1-Score: 0.6519\n",
      "akk: 670, gt: 115891\n",
      "Epoch: 023 train_loss: 0.0359 train_acc: 99.4801 Precision: 0.7522 Recall: 0.5339 F1-Score: 0.6245\n",
      "akk: 674, gt: 115887\n",
      "Epoch: 024 train_loss: 0.0360 train_acc: 99.5024 Precision: 0.7700 Recall: 0.5498 F1-Score: 0.6415\n",
      "akk: 684, gt: 115877\n",
      "Epoch: 025 train_loss: 0.0356 train_acc: 99.5230 Precision: 0.7836 Recall: 0.5678 F1-Score: 0.6585\n",
      "akk: 674, gt: 115887\n",
      "Epoch: 026 train_loss: 0.0354 train_acc: 99.5093 Precision: 0.7760 Recall: 0.5540 F1-Score: 0.6465\n",
      "akk: 690, gt: 115871\n",
      "Epoch: 027 train_loss: 0.0359 train_acc: 99.5127 Precision: 0.7725 Recall: 0.5646 F1-Score: 0.6524\n",
      "akk: 680, gt: 115881\n",
      "Epoch: 028 train_loss: 0.0351 train_acc: 99.5093 Precision: 0.7735 Recall: 0.5572 F1-Score: 0.6478\n",
      "akk: 667, gt: 115894\n",
      "Epoch: 029 train_loss: 0.0352 train_acc: 99.5221 Precision: 0.7901 Recall: 0.5583 F1-Score: 0.6543\n",
      "akk: 716, gt: 115845\n",
      "Epoch: 030 train_loss: 0.0348 train_acc: 99.5333 Precision: 0.7793 Recall: 0.5911 F1-Score: 0.6723\n",
      "akk: 707, gt: 115854\n",
      "Epoch: 031 train_loss: 0.0348 train_acc: 99.5376 Precision: 0.7864 Recall: 0.5890 F1-Score: 0.6735\n",
      "akk: 24157, gt: 3744629\n",
      "OA: 0.9955 AA: 0.8081 Kappa: 0.6873 Precision: 0.7803 Recall: 0.6177 F1-Score: 0.6895\n",
      "*************************\n",
      "akk: 703, gt: 115858\n",
      "Epoch: 032 train_loss: 0.0344 train_acc: 99.5239 Precision: 0.7767 Recall: 0.5784 F1-Score: 0.6630\n",
      "akk: 665, gt: 115896\n",
      "Epoch: 033 train_loss: 0.0346 train_acc: 99.5410 Precision: 0.8075 Recall: 0.5689 F1-Score: 0.6675\n",
      "akk: 689, gt: 115872\n",
      "Epoch: 034 train_loss: 0.0345 train_acc: 99.5444 Precision: 0.7997 Recall: 0.5837 F1-Score: 0.6748\n",
      "akk: 688, gt: 115873\n",
      "Epoch: 035 train_loss: 0.0346 train_acc: 99.5384 Precision: 0.7951 Recall: 0.5794 F1-Score: 0.6703\n",
      "akk: 677, gt: 115884\n",
      "Epoch: 036 train_loss: 0.0345 train_acc: 99.5444 Precision: 0.8050 Recall: 0.5773 F1-Score: 0.6724\n",
      "akk: 686, gt: 115875\n",
      "Epoch: 037 train_loss: 0.0344 train_acc: 99.5453 Precision: 0.8017 Recall: 0.5826 F1-Score: 0.6748\n",
      "akk: 689, gt: 115872\n",
      "Epoch: 038 train_loss: 0.0343 train_acc: 99.5341 Precision: 0.7910 Recall: 0.5773 F1-Score: 0.6675\n",
      "akk: 688, gt: 115873\n",
      "Epoch: 039 train_loss: 0.0341 train_acc: 99.5350 Precision: 0.7922 Recall: 0.5773 F1-Score: 0.6679\n",
      "akk: 719, gt: 115842\n",
      "Epoch: 040 train_loss: 0.0341 train_acc: 99.5427 Precision: 0.7858 Recall: 0.5985 F1-Score: 0.6795\n",
      "akk: 691, gt: 115870\n",
      "Epoch: 041 train_loss: 0.0340 train_acc: 99.5496 Precision: 0.8032 Recall: 0.5879 F1-Score: 0.6789\n",
      "akk: 24384, gt: 3744402\n",
      "OA: 0.9957 AA: 0.8149 Kappa: 0.6994 Precision: 0.7898 Recall: 0.6311 F1-Score: 0.7016\n",
      "*************************\n",
      "akk: 693, gt: 115868\n",
      "Epoch: 042 train_loss: 0.0343 train_acc: 99.5479 Precision: 0.8009 Recall: 0.5879 F1-Score: 0.6781\n",
      "akk: 691, gt: 115870\n",
      "Epoch: 043 train_loss: 0.0339 train_acc: 99.5462 Precision: 0.8003 Recall: 0.5858 F1-Score: 0.6765\n",
      "akk: 687, gt: 115874\n",
      "Epoch: 044 train_loss: 0.0342 train_acc: 99.5376 Precision: 0.7948 Recall: 0.5784 F1-Score: 0.6695\n",
      "akk: 726, gt: 115835\n",
      "Epoch: 045 train_loss: 0.0338 train_acc: 99.5436 Precision: 0.7837 Recall: 0.6028 F1-Score: 0.6814\n",
      "akk: 703, gt: 115858\n",
      "Epoch: 046 train_loss: 0.0337 train_acc: 99.5530 Precision: 0.8009 Recall: 0.5964 F1-Score: 0.6837\n",
      "akk: 699, gt: 115862\n",
      "Epoch: 047 train_loss: 0.0338 train_acc: 99.5393 Precision: 0.7911 Recall: 0.5858 F1-Score: 0.6732\n",
      "akk: 715, gt: 115846\n",
      "Epoch: 048 train_loss: 0.0338 train_acc: 99.5376 Precision: 0.7832 Recall: 0.5932 F1-Score: 0.6751\n",
      "akk: 711, gt: 115850\n",
      "Epoch: 049 train_loss: 0.0334 train_acc: 99.5770 Precision: 0.8172 Recall: 0.6155 F1-Score: 0.7021\n",
      "akk: 693, gt: 115868\n",
      "Epoch: 050 train_loss: 0.0331 train_acc: 99.5496 Precision: 0.8023 Recall: 0.5890 F1-Score: 0.6793\n",
      "akk: 723, gt: 115838\n",
      "Epoch: 051 train_loss: 0.0333 train_acc: 99.5822 Precision: 0.8160 Recall: 0.6250 F1-Score: 0.7079\n",
      "akk: 20333, gt: 3748453\n",
      "OA: 0.9951 AA: 0.7653 Kappa: 0.6359 Precision: 0.7980 Recall: 0.5317 F1-Score: 0.6382\n",
      "*************************\n",
      "akk: 739, gt: 115822\n",
      "Epoch: 052 train_loss: 0.0334 train_acc: 99.5650 Precision: 0.7957 Recall: 0.6229 F1-Score: 0.6988\n",
      "akk: 715, gt: 115846\n",
      "Epoch: 053 train_loss: 0.0332 train_acc: 99.5719 Precision: 0.8112 Recall: 0.6144 F1-Score: 0.6992\n",
      "akk: 721, gt: 115840\n",
      "Epoch: 054 train_loss: 0.0333 train_acc: 99.5702 Precision: 0.8072 Recall: 0.6165 F1-Score: 0.6991\n",
      "akk: 725, gt: 115836\n",
      "Epoch: 055 train_loss: 0.0332 train_acc: 99.5616 Precision: 0.7986 Recall: 0.6133 F1-Score: 0.6938\n",
      "akk: 735, gt: 115826\n",
      "Epoch: 056 train_loss: 0.0331 train_acc: 99.5702 Precision: 0.8014 Recall: 0.6239 F1-Score: 0.7016\n",
      "akk: 738, gt: 115823\n",
      "Epoch: 057 train_loss: 0.0334 train_acc: 99.5607 Precision: 0.7927 Recall: 0.6197 F1-Score: 0.6956\n",
      "akk: 735, gt: 115826\n",
      "Epoch: 058 train_loss: 0.0330 train_acc: 99.5685 Precision: 0.8000 Recall: 0.6229 F1-Score: 0.7004\n",
      "akk: 732, gt: 115829\n",
      "Epoch: 059 train_loss: 0.0329 train_acc: 99.5779 Precision: 0.8087 Recall: 0.6271 F1-Score: 0.7064\n",
      "akk: 740, gt: 115821\n",
      "Epoch: 060 train_loss: 0.0331 train_acc: 99.5607 Precision: 0.7919 Recall: 0.6208 F1-Score: 0.6960\n",
      "akk: 725, gt: 115836\n",
      "Epoch: 061 train_loss: 0.0328 train_acc: 99.5788 Precision: 0.8124 Recall: 0.6239 F1-Score: 0.7058\n",
      "akk: 20163, gt: 3748623\n",
      "OA: 0.9952 AA: 0.7679 Kappa: 0.6442 Precision: 0.8125 Recall: 0.5369 F1-Score: 0.6465\n",
      "*************************\n",
      "akk: 740, gt: 115821\n",
      "Epoch: 062 train_loss: 0.0327 train_acc: 99.5813 Precision: 0.8081 Recall: 0.6335 F1-Score: 0.7102\n",
      "akk: 716, gt: 115845\n",
      "Epoch: 063 train_loss: 0.0329 train_acc: 99.5762 Precision: 0.8142 Recall: 0.6176 F1-Score: 0.7024\n",
      "akk: 716, gt: 115845\n",
      "Epoch: 064 train_loss: 0.0327 train_acc: 99.5728 Precision: 0.8115 Recall: 0.6155 F1-Score: 0.7000\n",
      "akk: 738, gt: 115823\n",
      "Epoch: 065 train_loss: 0.0328 train_acc: 99.5710 Precision: 0.8008 Recall: 0.6261 F1-Score: 0.7027\n",
      "akk: 746, gt: 115815\n",
      "Epoch: 066 train_loss: 0.0327 train_acc: 99.5796 Precision: 0.8043 Recall: 0.6356 F1-Score: 0.7101\n",
      "akk: 725, gt: 115836\n",
      "Epoch: 067 train_loss: 0.0328 train_acc: 99.5839 Precision: 0.8166 Recall: 0.6271 F1-Score: 0.7094\n",
      "akk: 749, gt: 115812\n",
      "Epoch: 068 train_loss: 0.0327 train_acc: 99.5856 Precision: 0.8077 Recall: 0.6409 F1-Score: 0.7147\n",
      "akk: 743, gt: 115818\n",
      "Epoch: 069 train_loss: 0.0324 train_acc: 99.5942 Precision: 0.8170 Recall: 0.6430 F1-Score: 0.7196\n",
      "akk: 742, gt: 115819\n",
      "Epoch: 070 train_loss: 0.0324 train_acc: 99.5865 Precision: 0.8113 Recall: 0.6377 F1-Score: 0.7141\n",
      "akk: 755, gt: 115806\n",
      "Epoch: 071 train_loss: 0.0323 train_acc: 99.6045 Precision: 0.8199 Recall: 0.6557 F1-Score: 0.7287\n",
      "akk: 31032, gt: 3737754\n",
      "OA: 0.9945 AA: 0.8315 Kappa: 0.6575 Precision: 0.6548 Recall: 0.6659 F1-Score: 0.6603\n",
      "*************************\n",
      "akk: 766, gt: 115795\n",
      "Epoch: 072 train_loss: 0.0324 train_acc: 99.5985 Precision: 0.8107 Recall: 0.6578 F1-Score: 0.7263\n",
      "akk: 742, gt: 115819\n",
      "Epoch: 073 train_loss: 0.0323 train_acc: 99.5933 Precision: 0.8167 Recall: 0.6419 F1-Score: 0.7189\n",
      "akk: 758, gt: 115803\n",
      "Epoch: 074 train_loss: 0.0324 train_acc: 99.5968 Precision: 0.8127 Recall: 0.6525 F1-Score: 0.7239\n",
      "akk: 749, gt: 115812\n",
      "Epoch: 075 train_loss: 0.0323 train_acc: 99.5891 Precision: 0.8104 Recall: 0.6430 F1-Score: 0.7171\n",
      "akk: 747, gt: 115814\n",
      "Epoch: 076 train_loss: 0.0322 train_acc: 99.6062 Precision: 0.8246 Recall: 0.6525 F1-Score: 0.7286\n",
      "akk: 753, gt: 115808\n",
      "Epoch: 077 train_loss: 0.0321 train_acc: 99.6131 Precision: 0.8274 Recall: 0.6600 F1-Score: 0.7342\n",
      "akk: 749, gt: 115812\n",
      "Epoch: 078 train_loss: 0.0321 train_acc: 99.6199 Precision: 0.8344 Recall: 0.6621 F1-Score: 0.7383\n",
      "akk: 745, gt: 115816\n",
      "Epoch: 079 train_loss: 0.0321 train_acc: 99.6148 Precision: 0.8322 Recall: 0.6568 F1-Score: 0.7342\n",
      "akk: 754, gt: 115807\n",
      "Epoch: 080 train_loss: 0.0320 train_acc: 99.6225 Precision: 0.8342 Recall: 0.6663 F1-Score: 0.7409\n",
      "akk: 777, gt: 115784\n",
      "Epoch: 081 train_loss: 0.0319 train_acc: 99.6182 Precision: 0.8211 Recall: 0.6758 F1-Score: 0.7414\n",
      "akk: 23575, gt: 3745211\n",
      "OA: 0.9954 AA: 0.8002 Kappa: 0.6768 Precision: 0.7790 Recall: 0.6018 F1-Score: 0.6791\n",
      "*************************\n",
      "akk: 752, gt: 115809\n",
      "Epoch: 082 train_loss: 0.0319 train_acc: 99.6191 Precision: 0.8324 Recall: 0.6631 F1-Score: 0.7382\n",
      "akk: 749, gt: 115812\n",
      "Epoch: 083 train_loss: 0.0320 train_acc: 99.6182 Precision: 0.8331 Recall: 0.6610 F1-Score: 0.7372\n",
      "akk: 751, gt: 115810\n",
      "Epoch: 084 train_loss: 0.0319 train_acc: 99.6199 Precision: 0.8336 Recall: 0.6631 F1-Score: 0.7386\n",
      "akk: 779, gt: 115782\n",
      "Epoch: 085 train_loss: 0.0318 train_acc: 99.6371 Precision: 0.8344 Recall: 0.6886 F1-Score: 0.7545\n",
      "akk: 741, gt: 115820\n",
      "Epoch: 086 train_loss: 0.0318 train_acc: 99.6131 Precision: 0.8327 Recall: 0.6536 F1-Score: 0.7323\n",
      "akk: 761, gt: 115800\n",
      "Epoch: 087 train_loss: 0.0316 train_acc: 99.6131 Precision: 0.8239 Recall: 0.6642 F1-Score: 0.7355\n",
      "akk: 773, gt: 115788\n",
      "Epoch: 088 train_loss: 0.0318 train_acc: 99.6182 Precision: 0.8228 Recall: 0.6737 F1-Score: 0.7408\n",
      "akk: 774, gt: 115787\n",
      "Epoch: 089 train_loss: 0.0317 train_acc: 99.6139 Precision: 0.8191 Recall: 0.6716 F1-Score: 0.7381\n",
      "akk: 766, gt: 115795\n",
      "Epoch: 090 train_loss: 0.0317 train_acc: 99.6242 Precision: 0.8303 Recall: 0.6737 F1-Score: 0.7439\n",
      "akk: 762, gt: 115799\n",
      "Epoch: 091 train_loss: 0.0316 train_acc: 99.6414 Precision: 0.8451 Recall: 0.6822 F1-Score: 0.7550\n",
      "akk: 31043, gt: 3737743\n",
      "OA: 0.9949 AA: 0.8463 Kappa: 0.6868 Precision: 0.6835 Recall: 0.6953 F1-Score: 0.6893\n",
      "*************************\n",
      "akk: 769, gt: 115792\n",
      "Epoch: 092 train_loss: 0.0315 train_acc: 99.6268 Precision: 0.8309 Recall: 0.6769 F1-Score: 0.7461\n",
      "akk: 764, gt: 115797\n",
      "Epoch: 093 train_loss: 0.0314 train_acc: 99.6500 Precision: 0.8508 Recall: 0.6886 F1-Score: 0.7611\n",
      "akk: 765, gt: 115796\n",
      "Epoch: 094 train_loss: 0.0314 train_acc: 99.6457 Precision: 0.8471 Recall: 0.6864 F1-Score: 0.7583\n",
      "akk: 760, gt: 115801\n",
      "Epoch: 095 train_loss: 0.0314 train_acc: 99.6328 Precision: 0.8395 Recall: 0.6758 F1-Score: 0.7488\n",
      "akk: 768, gt: 115793\n",
      "Epoch: 096 train_loss: 0.0313 train_acc: 99.6465 Precision: 0.8464 Recall: 0.6886 F1-Score: 0.7593\n",
      "akk: 780, gt: 115781\n",
      "Epoch: 097 train_loss: 0.0313 train_acc: 99.6397 Precision: 0.8359 Recall: 0.6907 F1-Score: 0.7564\n",
      "akk: 761, gt: 115800\n",
      "Epoch: 098 train_loss: 0.0313 train_acc: 99.6525 Precision: 0.8541 Recall: 0.6886 F1-Score: 0.7625\n",
      "akk: 773, gt: 115788\n",
      "Epoch: 099 train_loss: 0.0314 train_acc: 99.6474 Precision: 0.8448 Recall: 0.6917 F1-Score: 0.7606\n",
      "akk: 778, gt: 115783\n",
      "Epoch: 100 train_loss: 0.0312 train_acc: 99.6500 Precision: 0.8445 Recall: 0.6960 F1-Score: 0.7631\n",
      "akk: 31533, gt: 3737253\n",
      "OA: 0.9948 AA: 0.8459 Kappa: 0.6805 Precision: 0.6721 Recall: 0.6945 F1-Score: 0.6831\n",
      "*************************\n",
      "Running Time: 2978.78\n",
      "**************************************************\n",
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [8:16:31<55:23, 3323.89s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "F1 score: 0.6863884982028442\n",
      "Precision: 0.6750791552672835\n",
      "Recall: 0.6980832194284624\n",
      "max_label: 1 max_pred: 1\n",
      "Accuracy: 0.9959894039364516\n",
      "Confusion Matrix:\n",
      "[[4961716   10570]\n",
      " [   9498   21961]]\n",
      "**************************************************\n",
      "Parameter:\n",
      "fix_random: True\n",
      "gpu_id: 0\n",
      "seed: 0\n",
      "dataset: BS\n",
      "flag_test: train\n",
      "model_name: conv2d_unmix\n",
      "batch_size: 128\n",
      "test_freq: 10\n",
      "patches: 5\n",
      "epoches: 100\n",
      "inference_model: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "learning_rate: 0.0001\n",
      "gamma: 0.9\n",
      "weight_decay: 0\n",
      "Train data change to the ratio of train samples: 0.03\n",
      "height=2715,width=1843,band=150\n",
      "**************************************************\n",
      "patch is : 5\n",
      "mirror_image shape : [2719,1847,150]\n",
      "**************************************************\n",
      "x_train shape = (116561, 5, 5, 150), type = float32\n",
      "x_test  shape = (3768786, 5, 5, 150), type = float32\n",
      "x_true  shape = (5003745, 5, 5, 150), type = float32\n",
      "**************************************************\n",
      "y_train: shape = (116561,) ,type = int64\n",
      "y_test: shape = (3768786,) ,type = int64\n",
      "y_true: shape = (5003745,) ,type = int64\n",
      "**************************************************\n",
      "Model Name: conv2d_unmix\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akk: 5849, gt: 110712\n",
      "Epoch: 001 train_loss: 0.7264 train_acc: 94.4741 Precision: 0.0301 Recall: 0.1864 F1-Score: 0.0518\n",
      "akk: 1358780, gt: 2410006\n",
      "OA: 0.6461 AA: 0.7775 Kappa: 0.0246 Precision: 0.0205 Recall: 0.9111 F1-Score: 0.0400\n",
      "*************************\n",
      "akk: 3121, gt: 113440\n",
      "Epoch: 002 train_loss: 0.1633 train_acc: 96.8935 Precision: 0.0711 Recall: 0.2352 F1-Score: 0.1092\n",
      "akk: 2389, gt: 114172\n",
      "Epoch: 003 train_loss: 0.0986 train_acc: 97.5815 Precision: 0.1076 Recall: 0.2722 F1-Score: 0.1542\n",
      "akk: 1671, gt: 114890\n",
      "Epoch: 004 train_loss: 0.0784 train_acc: 98.1580 Precision: 0.1400 Recall: 0.2479 F1-Score: 0.1790\n",
      "akk: 1438, gt: 115123\n",
      "Epoch: 005 train_loss: 0.0662 train_acc: 98.3923 Precision: 0.1766 Recall: 0.2691 F1-Score: 0.2133\n",
      "akk: 650, gt: 115911\n",
      "Epoch: 006 train_loss: 0.0519 train_acc: 99.0065 Precision: 0.3354 Recall: 0.2309 F1-Score: 0.2735\n",
      "akk: 522, gt: 116039\n",
      "Epoch: 007 train_loss: 0.0428 train_acc: 99.3137 Precision: 0.6379 Recall: 0.3528 F1-Score: 0.4543\n",
      "akk: 545, gt: 116016\n",
      "Epoch: 008 train_loss: 0.0405 train_acc: 99.3557 Precision: 0.6771 Recall: 0.3909 F1-Score: 0.4956\n",
      "akk: 611, gt: 115950\n",
      "Epoch: 009 train_loss: 0.0396 train_acc: 99.3694 Precision: 0.6710 Recall: 0.4343 F1-Score: 0.5273\n",
      "akk: 607, gt: 115954\n",
      "Epoch: 010 train_loss: 0.0390 train_acc: 99.4072 Precision: 0.7084 Recall: 0.4555 F1-Score: 0.5545\n",
      "akk: 625, gt: 115936\n",
      "Epoch: 011 train_loss: 0.0392 train_acc: 99.4140 Precision: 0.7088 Recall: 0.4693 F1-Score: 0.5647\n",
      "akk: 19942, gt: 3748844\n",
      "OA: 0.9946 AA: 0.7445 Kappa: 0.5905 Precision: 0.7503 Recall: 0.4903 F1-Score: 0.5931\n",
      "*************************\n",
      "akk: 652, gt: 115909\n",
      "Epoch: 012 train_loss: 0.0382 train_acc: 99.4286 Precision: 0.7132 Recall: 0.4926 F1-Score: 0.5827\n",
      "akk: 642, gt: 115919\n",
      "Epoch: 013 train_loss: 0.0380 train_acc: 99.4441 Precision: 0.7305 Recall: 0.4968 F1-Score: 0.5914\n",
      "akk: 667, gt: 115894\n",
      "Epoch: 014 train_loss: 0.0379 train_acc: 99.4501 Precision: 0.7271 Recall: 0.5138 F1-Score: 0.6021\n",
      "akk: 642, gt: 115919\n",
      "Epoch: 015 train_loss: 0.0375 train_acc: 99.4664 Precision: 0.7508 Recall: 0.5106 F1-Score: 0.6078\n",
      "akk: 664, gt: 115897\n",
      "Epoch: 016 train_loss: 0.0372 train_acc: 99.4526 Precision: 0.7304 Recall: 0.5138 F1-Score: 0.6032\n",
      "akk: 656, gt: 115905\n",
      "Epoch: 017 train_loss: 0.0371 train_acc: 99.4681 Precision: 0.7470 Recall: 0.5191 F1-Score: 0.6125\n",
      "akk: 630, gt: 115931\n",
      "Epoch: 018 train_loss: 0.0369 train_acc: 99.4767 Precision: 0.7651 Recall: 0.5106 F1-Score: 0.6125\n",
      "akk: 668, gt: 115893\n",
      "Epoch: 019 train_loss: 0.0366 train_acc: 99.4732 Precision: 0.7470 Recall: 0.5286 F1-Score: 0.6191\n",
      "akk: 680, gt: 115881\n",
      "Epoch: 020 train_loss: 0.0362 train_acc: 99.4870 Precision: 0.7544 Recall: 0.5434 F1-Score: 0.6318\n",
      "akk: 674, gt: 115887\n",
      "Epoch: 021 train_loss: 0.0363 train_acc: 99.5007 Precision: 0.7685 Recall: 0.5487 F1-Score: 0.6403\n",
      "akk: 47018, gt: 3721768\n",
      "OA: 0.9911 AA: 0.8556 Kappa: 0.5608 Precision: 0.4659 Recall: 0.7179 F1-Score: 0.5651\n",
      "*************************\n",
      "akk: 683, gt: 115878\n",
      "Epoch: 022 train_loss: 0.0361 train_acc: 99.5136 Precision: 0.7760 Recall: 0.5614 F1-Score: 0.6515\n",
      "akk: 683, gt: 115878\n",
      "Epoch: 023 train_loss: 0.0362 train_acc: 99.4895 Precision: 0.7555 Recall: 0.5466 F1-Score: 0.6343\n",
      "akk: 661, gt: 115900\n",
      "Epoch: 024 train_loss: 0.0357 train_acc: 99.4998 Precision: 0.7731 Recall: 0.5413 F1-Score: 0.6368\n",
      "akk: 676, gt: 115885\n",
      "Epoch: 025 train_loss: 0.0358 train_acc: 99.5058 Precision: 0.7722 Recall: 0.5530 F1-Score: 0.6444\n",
      "akk: 687, gt: 115874\n",
      "Epoch: 026 train_loss: 0.0357 train_acc: 99.5067 Precision: 0.7686 Recall: 0.5593 F1-Score: 0.6475\n",
      "akk: 683, gt: 115878\n",
      "Epoch: 027 train_loss: 0.0357 train_acc: 99.4895 Precision: 0.7555 Recall: 0.5466 F1-Score: 0.6343\n",
      "akk: 695, gt: 115866\n",
      "Epoch: 028 train_loss: 0.0355 train_acc: 99.5136 Precision: 0.7712 Recall: 0.5678 F1-Score: 0.6541\n",
      "akk: 695, gt: 115866\n",
      "Epoch: 029 train_loss: 0.0355 train_acc: 99.5101 Precision: 0.7683 Recall: 0.5657 F1-Score: 0.6516\n",
      "akk: 698, gt: 115863\n",
      "Epoch: 030 train_loss: 0.0352 train_acc: 99.5076 Precision: 0.7650 Recall: 0.5657 F1-Score: 0.6504\n",
      "akk: 684, gt: 115877\n",
      "Epoch: 031 train_loss: 0.0353 train_acc: 99.5007 Precision: 0.7646 Recall: 0.5540 F1-Score: 0.6425\n",
      "akk: 10940, gt: 3757846\n",
      "OA: 0.9940 AA: 0.6553 Kappa: 0.4556 Precision: 0.8676 Recall: 0.3111 F1-Score: 0.4579\n",
      "*************************\n",
      "akk: 677, gt: 115884\n",
      "Epoch: 032 train_loss: 0.0354 train_acc: 99.5050 Precision: 0.7710 Recall: 0.5530 F1-Score: 0.6440\n",
      "akk: 684, gt: 115877\n",
      "Epoch: 033 train_loss: 0.0351 train_acc: 99.5264 Precision: 0.7865 Recall: 0.5699 F1-Score: 0.6609\n",
      "akk: 672, gt: 115889\n",
      "Epoch: 034 train_loss: 0.0350 train_acc: 99.5058 Precision: 0.7738 Recall: 0.5508 F1-Score: 0.6436\n",
      "akk: 687, gt: 115874\n",
      "Epoch: 035 train_loss: 0.0347 train_acc: 99.5256 Precision: 0.7846 Recall: 0.5710 F1-Score: 0.6609\n",
      "akk: 705, gt: 115856\n",
      "Epoch: 036 train_loss: 0.0353 train_acc: 99.5033 Precision: 0.7589 Recall: 0.5667 F1-Score: 0.6489\n",
      "akk: 711, gt: 115850\n",
      "Epoch: 037 train_loss: 0.0347 train_acc: 99.5359 Precision: 0.7834 Recall: 0.5900 F1-Score: 0.6731\n",
      "akk: 692, gt: 115869\n",
      "Epoch: 038 train_loss: 0.0346 train_acc: 99.5213 Precision: 0.7789 Recall: 0.5710 F1-Score: 0.6589\n",
      "akk: 706, gt: 115855\n",
      "Epoch: 039 train_loss: 0.0346 train_acc: 99.5299 Precision: 0.7805 Recall: 0.5837 F1-Score: 0.6679\n",
      "akk: 695, gt: 115866\n",
      "Epoch: 040 train_loss: 0.0345 train_acc: 99.5256 Precision: 0.7813 Recall: 0.5752 F1-Score: 0.6626\n",
      "akk: 694, gt: 115867\n",
      "Epoch: 041 train_loss: 0.0344 train_acc: 99.5281 Precision: 0.7839 Recall: 0.5763 F1-Score: 0.6642\n",
      "akk: 32908, gt: 3735878\n",
      "OA: 0.9949 AA: 0.8604 Kappa: 0.6938 Precision: 0.6711 Recall: 0.7237 F1-Score: 0.6964\n",
      "*************************\n",
      "akk: 683, gt: 115878\n",
      "Epoch: 042 train_loss: 0.0344 train_acc: 99.5324 Precision: 0.7921 Recall: 0.5731 F1-Score: 0.6650\n",
      "akk: 696, gt: 115865\n",
      "Epoch: 043 train_loss: 0.0342 train_acc: 99.5367 Precision: 0.7902 Recall: 0.5826 F1-Score: 0.6707\n",
      "akk: 720, gt: 115841\n",
      "Epoch: 044 train_loss: 0.0341 train_acc: 99.5316 Precision: 0.7764 Recall: 0.5922 F1-Score: 0.6719\n",
      "akk: 679, gt: 115882\n",
      "Epoch: 045 train_loss: 0.0342 train_acc: 99.5324 Precision: 0.7938 Recall: 0.5710 F1-Score: 0.6642\n",
      "akk: 709, gt: 115852\n",
      "Epoch: 046 train_loss: 0.0341 train_acc: 99.5410 Precision: 0.7884 Recall: 0.5922 F1-Score: 0.6763\n",
      "akk: 699, gt: 115862\n",
      "Epoch: 047 train_loss: 0.0340 train_acc: 99.5462 Precision: 0.7969 Recall: 0.5900 F1-Score: 0.6780\n",
      "akk: 719, gt: 115842\n",
      "Epoch: 048 train_loss: 0.0341 train_acc: 99.5462 Precision: 0.7886 Recall: 0.6006 F1-Score: 0.6819\n",
      "akk: 690, gt: 115871\n",
      "Epoch: 049 train_loss: 0.0340 train_acc: 99.5556 Precision: 0.8087 Recall: 0.5911 F1-Score: 0.6830\n",
      "akk: 702, gt: 115859\n",
      "Epoch: 050 train_loss: 0.0338 train_acc: 99.5573 Precision: 0.8048 Recall: 0.5985 F1-Score: 0.6865\n",
      "akk: 730, gt: 115831\n",
      "Epoch: 051 train_loss: 0.0338 train_acc: 99.5487 Precision: 0.7863 Recall: 0.6081 F1-Score: 0.6858\n",
      "akk: 18987, gt: 3749799\n",
      "OA: 0.9953 AA: 0.7599 Kappa: 0.6396 Precision: 0.8367 Recall: 0.5206 F1-Score: 0.6418\n",
      "*************************\n",
      "akk: 701, gt: 115860\n",
      "Epoch: 052 train_loss: 0.0338 train_acc: 99.5496 Precision: 0.7989 Recall: 0.5932 F1-Score: 0.6809\n",
      "akk: 686, gt: 115875\n",
      "Epoch: 053 train_loss: 0.0336 train_acc: 99.5487 Precision: 0.8047 Recall: 0.5847 F1-Score: 0.6773\n",
      "akk: 706, gt: 115855\n",
      "Epoch: 054 train_loss: 0.0337 train_acc: 99.5522 Precision: 0.7989 Recall: 0.5975 F1-Score: 0.6836\n",
      "akk: 705, gt: 115856\n",
      "Epoch: 055 train_loss: 0.0335 train_acc: 99.5616 Precision: 0.8071 Recall: 0.6028 F1-Score: 0.6901\n",
      "akk: 710, gt: 115851\n",
      "Epoch: 056 train_loss: 0.0336 train_acc: 99.5436 Precision: 0.7901 Recall: 0.5943 F1-Score: 0.6784\n",
      "akk: 732, gt: 115829\n",
      "Epoch: 057 train_loss: 0.0336 train_acc: 99.5504 Precision: 0.7869 Recall: 0.6102 F1-Score: 0.6874\n",
      "akk: 706, gt: 115855\n",
      "Epoch: 058 train_loss: 0.0335 train_acc: 99.5522 Precision: 0.7989 Recall: 0.5975 F1-Score: 0.6836\n",
      "akk: 723, gt: 115838\n",
      "Epoch: 059 train_loss: 0.0339 train_acc: 99.5341 Precision: 0.7773 Recall: 0.5953 F1-Score: 0.6743\n",
      "akk: 731, gt: 115830\n",
      "Epoch: 060 train_loss: 0.0334 train_acc: 99.5547 Precision: 0.7907 Recall: 0.6123 F1-Score: 0.6901\n",
      "akk: 702, gt: 115859\n",
      "Epoch: 061 train_loss: 0.0334 train_acc: 99.5625 Precision: 0.8091 Recall: 0.6017 F1-Score: 0.6902\n",
      "akk: 19829, gt: 3748957\n",
      "OA: 0.9954 AA: 0.7700 Kappa: 0.6534 Precision: 0.8323 Recall: 0.5408 F1-Score: 0.6556\n",
      "*************************\n",
      "akk: 718, gt: 115843\n",
      "Epoch: 062 train_loss: 0.0333 train_acc: 99.5676 Precision: 0.8064 Recall: 0.6133 F1-Score: 0.6968\n",
      "akk: 716, gt: 115845\n",
      "Epoch: 063 train_loss: 0.0333 train_acc: 99.5693 Precision: 0.8087 Recall: 0.6133 F1-Score: 0.6976\n",
      "akk: 725, gt: 115836\n",
      "Epoch: 064 train_loss: 0.0333 train_acc: 99.5685 Precision: 0.8041 Recall: 0.6176 F1-Score: 0.6986\n",
      "akk: 740, gt: 115821\n",
      "Epoch: 065 train_loss: 0.0331 train_acc: 99.5813 Precision: 0.8081 Recall: 0.6335 F1-Score: 0.7102\n",
      "akk: 718, gt: 115843\n",
      "Epoch: 066 train_loss: 0.0332 train_acc: 99.5693 Precision: 0.8078 Recall: 0.6144 F1-Score: 0.6980\n",
      "akk: 744, gt: 115817\n",
      "Epoch: 067 train_loss: 0.0331 train_acc: 99.5848 Precision: 0.8091 Recall: 0.6377 F1-Score: 0.7133\n",
      "akk: 702, gt: 115859\n",
      "Epoch: 068 train_loss: 0.0333 train_acc: 99.5728 Precision: 0.8177 Recall: 0.6081 F1-Score: 0.6974\n",
      "akk: 706, gt: 115855\n",
      "Epoch: 069 train_loss: 0.0333 train_acc: 99.5607 Precision: 0.8059 Recall: 0.6028 F1-Score: 0.6897\n",
      "akk: 698, gt: 115863\n",
      "Epoch: 070 train_loss: 0.0331 train_acc: 99.5625 Precision: 0.8109 Recall: 0.5996 F1-Score: 0.6894\n",
      "akk: 718, gt: 115843\n",
      "Epoch: 071 train_loss: 0.0329 train_acc: 99.5813 Precision: 0.8175 Recall: 0.6218 F1-Score: 0.7064\n",
      "akk: 26021, gt: 3742765\n",
      "OA: 0.9955 AA: 0.8236 Kappa: 0.6982 Precision: 0.7610 Recall: 0.6489 F1-Score: 0.7005\n",
      "*************************\n",
      "akk: 709, gt: 115852\n",
      "Epoch: 072 train_loss: 0.0329 train_acc: 99.5805 Precision: 0.8209 Recall: 0.6165 F1-Score: 0.7042\n",
      "akk: 729, gt: 115832\n",
      "Epoch: 073 train_loss: 0.0330 train_acc: 99.5788 Precision: 0.8107 Recall: 0.6261 F1-Score: 0.7065\n",
      "akk: 721, gt: 115840\n",
      "Epoch: 074 train_loss: 0.0329 train_acc: 99.5856 Precision: 0.8197 Recall: 0.6261 F1-Score: 0.7099\n",
      "akk: 709, gt: 115852\n",
      "Epoch: 075 train_loss: 0.0329 train_acc: 99.5770 Precision: 0.8181 Recall: 0.6144 F1-Score: 0.7018\n",
      "akk: 737, gt: 115824\n",
      "Epoch: 076 train_loss: 0.0328 train_acc: 99.6062 Precision: 0.8290 Recall: 0.6472 F1-Score: 0.7269\n",
      "akk: 727, gt: 115834\n",
      "Epoch: 077 train_loss: 0.0328 train_acc: 99.5942 Precision: 0.8239 Recall: 0.6345 F1-Score: 0.7169\n",
      "akk: 738, gt: 115823\n",
      "Epoch: 078 train_loss: 0.0328 train_acc: 99.5951 Precision: 0.8198 Recall: 0.6409 F1-Score: 0.7194\n",
      "akk: 723, gt: 115838\n",
      "Epoch: 079 train_loss: 0.0328 train_acc: 99.5822 Precision: 0.8160 Recall: 0.6250 F1-Score: 0.7079\n",
      "akk: 736, gt: 115825\n",
      "Epoch: 080 train_loss: 0.0325 train_acc: 99.5985 Precision: 0.8234 Recall: 0.6419 F1-Score: 0.7214\n",
      "akk: 731, gt: 115830\n",
      "Epoch: 081 train_loss: 0.0326 train_acc: 99.5925 Precision: 0.8208 Recall: 0.6356 F1-Score: 0.7164\n",
      "akk: 21979, gt: 3746807\n",
      "OA: 0.9955 AA: 0.7912 Kappa: 0.6761 Precision: 0.8100 Recall: 0.5834 F1-Score: 0.6783\n",
      "*************************\n",
      "akk: 735, gt: 115826\n",
      "Epoch: 082 train_loss: 0.0326 train_acc: 99.5959 Precision: 0.8218 Recall: 0.6398 F1-Score: 0.7195\n",
      "akk: 715, gt: 115846\n",
      "Epoch: 083 train_loss: 0.0327 train_acc: 99.5942 Precision: 0.8294 Recall: 0.6282 F1-Score: 0.7149\n",
      "akk: 726, gt: 115835\n",
      "Epoch: 084 train_loss: 0.0325 train_acc: 99.6036 Precision: 0.8320 Recall: 0.6398 F1-Score: 0.7234\n",
      "akk: 723, gt: 115838\n",
      "Epoch: 085 train_loss: 0.0324 train_acc: 99.5753 Precision: 0.8105 Recall: 0.6208 F1-Score: 0.7031\n",
      "akk: 727, gt: 115834\n",
      "Epoch: 086 train_loss: 0.0327 train_acc: 99.5891 Precision: 0.8198 Recall: 0.6314 F1-Score: 0.7133\n",
      "akk: 742, gt: 115819\n",
      "Epoch: 087 train_loss: 0.0323 train_acc: 99.5882 Precision: 0.8127 Recall: 0.6388 F1-Score: 0.7153\n",
      "akk: 741, gt: 115820\n",
      "Epoch: 088 train_loss: 0.0325 train_acc: 99.6079 Precision: 0.8286 Recall: 0.6504 F1-Score: 0.7288\n",
      "akk: 752, gt: 115809\n",
      "Epoch: 089 train_loss: 0.0324 train_acc: 99.6019 Precision: 0.8191 Recall: 0.6525 F1-Score: 0.7264\n",
      "akk: 747, gt: 115814\n",
      "Epoch: 090 train_loss: 0.0324 train_acc: 99.6165 Precision: 0.8327 Recall: 0.6589 F1-Score: 0.7357\n",
      "akk: 736, gt: 115825\n",
      "Epoch: 091 train_loss: 0.0323 train_acc: 99.6036 Precision: 0.8274 Recall: 0.6451 F1-Score: 0.7250\n",
      "akk: 29120, gt: 3739666\n",
      "OA: 0.9954 AA: 0.8456 Kappa: 0.7073 Precision: 0.7266 Recall: 0.6934 F1-Score: 0.7096\n",
      "*************************\n",
      "akk: 732, gt: 115829\n",
      "Epoch: 092 train_loss: 0.0322 train_acc: 99.6105 Precision: 0.8347 Recall: 0.6472 F1-Score: 0.7291\n",
      "akk: 746, gt: 115815\n",
      "Epoch: 093 train_loss: 0.0322 train_acc: 99.6139 Precision: 0.8311 Recall: 0.6568 F1-Score: 0.7337\n",
      "akk: 748, gt: 115813\n",
      "Epoch: 094 train_loss: 0.0324 train_acc: 99.6071 Precision: 0.8249 Recall: 0.6536 F1-Score: 0.7293\n",
      "akk: 737, gt: 115824\n",
      "Epoch: 095 train_loss: 0.0323 train_acc: 99.6028 Precision: 0.8263 Recall: 0.6451 F1-Score: 0.7246\n",
      "akk: 717, gt: 115844\n",
      "Epoch: 096 train_loss: 0.0321 train_acc: 99.6199 Precision: 0.8494 Recall: 0.6451 F1-Score: 0.7333\n",
      "akk: 735, gt: 115826\n",
      "Epoch: 097 train_loss: 0.0323 train_acc: 99.6028 Precision: 0.8272 Recall: 0.6441 F1-Score: 0.7242\n",
      "akk: 730, gt: 115831\n",
      "Epoch: 098 train_loss: 0.0321 train_acc: 99.6311 Precision: 0.8521 Recall: 0.6589 F1-Score: 0.7431\n",
      "akk: 758, gt: 115803\n",
      "Epoch: 099 train_loss: 0.0323 train_acc: 99.6071 Precision: 0.8206 Recall: 0.6589 F1-Score: 0.7309\n",
      "akk: 741, gt: 115820\n",
      "Epoch: 100 train_loss: 0.0322 train_acc: 99.6079 Precision: 0.8286 Recall: 0.6504 F1-Score: 0.7288\n",
      "akk: 26037, gt: 3742749\n",
      "OA: 0.9955 AA: 0.8242 Kappa: 0.6992 Precision: 0.7618 Recall: 0.6500 F1-Score: 0.7015\n",
      "*************************\n",
      "Running Time: 2968.39\n",
      "**************************************************\n",
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [9:11:51<00:00, 3311.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "F1 score: 0.7037957330134521\n",
      "Precision: 0.7634964306960142\n",
      "Recall: 0.6527543787151531\n",
      "max_label: 1 max_pred: 1\n",
      "Accuracy: 0.996545587355071\n",
      "Confusion Matrix:\n",
      "[[4965925    6361]\n",
      " [  10924   20535]]\n",
      "**************************************************\n",
      "Parameter:\n",
      "fix_random: True\n",
      "gpu_id: 0\n",
      "seed: 0\n",
      "dataset: BS\n",
      "flag_test: train\n",
      "model_name: conv2d_unmix\n",
      "batch_size: 128\n",
      "test_freq: 10\n",
      "patches: 5\n",
      "epoches: 100\n",
      "inference_model: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "learning_rate: 0.0001\n",
      "gamma: 0.9\n",
      "weight_decay: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
    "seeds = [20250401, 20250402,20250403,20250404,20250405,20250406,20250407,20250408,20250409,20250410]\n",
    "if args.fix_random:\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "else:\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "for i in tqdm(seeds):\n",
    "    ## prepare dataset\n",
    "    label_train_loader, label_test_loader, label_true_loader, band, height, width, num_classes, label, total_pos_true = prepare_dataset(args,train_ratio=0.03)\n",
    "    # create model\n",
    "    if args.model_name == 'conv2d_unmix':\n",
    "        model = DSNet(band, num_classes, args.patches, args.model_name)\n",
    "    else:\n",
    "        raise KeyError(\"{} model is unknown.\".format(args.model_name))\n",
    "    model = model.cuda()\n",
    "    print(\"Model Name: {}\".format(args.model_name))\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    # Set the optimizer\n",
    "    if 'unmix' in args.model_name:\n",
    "        params = map(id, model.unmix_decoder.parameters())\n",
    "        ignored_params = list(set(params))\n",
    "        base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n",
    "        optimizer = torch.optim.Adam([{'params': base_params},{'params': model.unmix_decoder.parameters(), 'lr': 3e-4}],\n",
    "                                    lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    \n",
    "    apply_nonegative = NonZeroClipper()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.epoches//10, gamma=args.gamma)\n",
    "    #-------------------------------------------------------------------------------\n",
    "    print(\"start training\")\n",
    "    tic = time.time()\n",
    "    min_val_obj, best_OA = 0.5, 0\n",
    "    for epoch in range(args.epoches):\n",
    "        scheduler.step()\n",
    "\n",
    "        # train model\n",
    "        model.train()\n",
    "        train_acc, train_obj, tar_t, pre_t = train_epoch(model, label_train_loader, criterion, optimizer)\n",
    "        OA1, AA_mean1, Kappa1, AA1, precision1, recall1, f1_1 = output_metric(tar_t, pre_t)\n",
    "        print(\"Epoch: {:03d} train_loss: {:.4f} train_acc: {:.4f} Precision: {:.4f} Recall: {:.4f} F1-Score: {:.4f}\".format(epoch+1, train_obj, train_acc, precision1, recall1, f1_1))\n",
    "\n",
    "        if 'unmix' in args.model_name: # regularize unmix decoder\n",
    "            model.unmix_decoder.apply(apply_nonegative)\n",
    "\n",
    "        if (epoch % args.test_freq == 0) | (epoch == args.epoches - 1):\n",
    "            model.eval()\n",
    "            tar_v, pre_v = valid_epoch(model, label_test_loader, criterion, optimizer)\n",
    "            OA2, AA_mean2, Kappa2, AA2, precision2, recall2, f1_2 = output_metric(tar_v, pre_v)\n",
    "            print(\"OA: {:.4f} AA: {:.4f} Kappa: {:.4f} Precision: {:.4f} Recall: {:.4f} F1-Score: {:.4f}\".format(OA2, AA_mean2, Kappa2, precision2, recall2, f1_2))\n",
    "            print(\"*************************\")\n",
    "\n",
    "            if f1_2 > min_val_obj and epoch > 10:\n",
    "                model_save_path = os.path.join('./results/', args.dataset+'_07_ratio_'+str(i)+'_'+args.model_name+'_p'+str(args.patches)+\n",
    "                                                '_'+str(round(OA2*100, 2))+'_epoch'+str(epoch)+\"_seed_\"+str(i)+'.pkl')\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "                min_val_obj = OA2\n",
    "                best_epoch = epoch\n",
    "                best_OA = OA2\n",
    "                best_AA = AA_mean2\n",
    "                best_Kappa = Kappa2\n",
    "                best_each_AA = AA2\n",
    "\n",
    "    toc = time.time()\n",
    "    print(\"Running Time: {:.2f}\".format(toc-tic))\n",
    "    print(\"**************************************************\")\n",
    "    if best_OA == 0:\n",
    "        model_save_path = os.path.join('./results/', args.dataset+'_07_ratio_'+str(i)+'_'+args.model_name+'_p'+str(args.patches)+\n",
    "                                        '_'+str(round(OA2*100, 2))+'_epoch'+str(epoch)+\"_seed_\"+str(i)+'_.pkl')\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        wandb.save(model_save_path)\n",
    "\n",
    "    model.eval()\n",
    "    pre_u = test_epoch(model, label_true_loader)\n",
    "\n",
    "    prediction_matrix = np.zeros((height, width), dtype=float)\n",
    "    for i in range(total_pos_true.shape[0]):\n",
    "        prediction_matrix[total_pos_true[i,0], total_pos_true[i,1]] = pre_u[i] + 1\n",
    "\n",
    "    # 예측 및 레이블 데이터 플래트닝 및 타입 변환\n",
    "    flattened_label = label.flatten().astype(int)\n",
    "    flattened_prediction = prediction_matrix.flatten().astype(int)\n",
    "    print(f'flattened_label shape: {flattened_label.shape}')\n",
    "    print(f'flattened_prediction shape: {flattened_prediction.shape}')\n",
    "    flattened_label[flattened_label==2]=0\n",
    "    flattened_prediction[flattened_prediction==2]=0\n",
    "\n",
    "    # F1 score, precision, and recall calculation\n",
    "    try:\n",
    "        f1 = f1_score(flattened_label, flattened_prediction, average='binary')\n",
    "        precision = precision_score(flattened_label, flattened_prediction, average='binary')\n",
    "        recall = recall_score(flattened_label, flattened_prediction, average='binary')\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}. Switching to 'macro' average.\")\n",
    "        f1 = f1_score(flattened_label, flattened_prediction, average='macro')\n",
    "        precision = precision_score(flattened_label, flattened_prediction, average='macro')\n",
    "        recall = recall_score(flattened_label, flattened_prediction, average='macro')\n",
    "\n",
    "    conf_matrix = confusion_matrix(flattened_label, flattened_prediction)\n",
    "    print(\"**************************************************\")\n",
    "    print(f'F1 score: {f1}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'max_label: {np.max(flattened_label)} max_pred: {np.max(flattened_prediction)}')\n",
    "    print(f'Accuracy: {np.sum(flattened_label == flattened_prediction) / flattened_label.size}')\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"**************************************************\")\n",
    "    print(\"Parameter:\") \n",
    "    print_args(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f416be2-8839-47e5-b129-97b4fe067fb8",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "408cdca0-458b-495f-8331-38e050c1ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_path = '/home1/jmt30269/DSNet/results/'\n",
    "list_dir = os.listdir(base_path)\n",
    "list_dir.sort()\n",
    "\n",
    "filtered_files = [f for f in list_dir if 'BS_07_ratio_202504' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87ea0cec-0b7a-441e-a2a0-ba41adac2a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BS_07_ratio_20250401_conv2d_unmix_p5_99.51_epoch20_seed_20250401.pkl',\n",
       " 'BS_07_ratio_20250402_conv2d_unmix_p5_99.44_epoch40_seed_20250402.pkl',\n",
       " 'BS_07_ratio_20250403_conv2d_unmix_p5_99.43_epoch20_seed_20250403.pkl',\n",
       " 'BS_07_ratio_20250404_conv2d_unmix_p5_99.41_epoch30_seed_20250404.pkl',\n",
       " 'BS_07_ratio_20250405_conv2d_unmix_p5_99.29_epoch20_seed_20250405.pkl',\n",
       " 'BS_07_ratio_20250406_conv2d_unmix_p5_99.51_epoch20_seed_20250406.pkl',\n",
       " 'BS_07_ratio_20250407_conv2d_unmix_p5_99.4_epoch20_seed_20250407.pkl',\n",
       " 'BS_07_ratio_20250408_conv2d_unmix_p5_99.52_epoch20_seed_20250408.pkl',\n",
       " 'BS_07_ratio_20250409_conv2d_unmix_p5_99.5_epoch20_seed_20250409.pkl',\n",
       " 'BS_07_ratio_20250410_conv2d_unmix_p5_99.11_epoch20_seed_20250410.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8258eeae-2508-4fd5-b96a-0f3ae6bd1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def test_epoch(model, test_loader):\n",
    "    pre = np.array([])  # 예측 레이블 저장\n",
    "    probs = []  # 각 클래스별 확률 저장\n",
    "\n",
    "    for batch_idx, (batch_data, batch_target) in enumerate(test_loader):\n",
    "        batch_data = batch_data.cuda()\n",
    "        batch_target = batch_target.cuda()\n",
    "\n",
    "        if 'unmix' in args.model_name:\n",
    "            re_unmix_nonlinear, re_unmix, batch_pred = model(batch_data)\n",
    "        else:\n",
    "            batch_pred = model(batch_data)\n",
    "\n",
    "        # Softmax 적용하여 확률 계산\n",
    "        batch_probs = F.softmax(batch_pred, dim=1)  # (배치 크기, 클래스 수)\n",
    "\n",
    "        _, pred = batch_pred.topk(1, 1, True, True)  # 최상위 클래스 예측\n",
    "        pp = pred.squeeze()\n",
    "\n",
    "        pre = np.append(pre, pp.data.cpu().numpy())\n",
    "        probs.append(batch_probs.data.cpu().numpy())  # 확률 저장\n",
    "\n",
    "    return pre, np.vstack(probs)  # 예측 레이블과 확률 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "761156f1-db9e-46bc-9e38-fa52e90b4547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train data change to the ratio of train samples: 0.015\n",
      "height=2715,width=1843,band=150\n",
      "**************************************************\n",
      "patch is : 5\n",
      "mirror_image shape : [2719,1847,150]\n",
      "**************************************************\n",
      "x_train shape = (58281, 5, 5, 150), type = float32\n",
      "x_test  shape = (3827066, 5, 5, 150), type = float32\n",
      "x_true  shape = (5003745, 5, 5, 150), type = float32\n",
      "**************************************************\n",
      "y_train: shape = (58281,) ,type = int64\n",
      "y_test: shape = (3827066,) ,type = int64\n",
      "y_true: shape = (5003745,) ,type = int64\n",
      "**************************************************\n",
      "Model Name: conv2d_unmix\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "if args.fix_random:\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "else:\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "## prepare dataset\n",
    "label_train_loader, label_test_loader, label_true_loader, band, height, width, num_classes, label, total_pos_true = prepare_dataset(args,train_ratio=0.015)\n",
    "# create model\n",
    "if args.model_name == 'conv2d_unmix':\n",
    "    model = DSNet(band, num_classes, args.patches, args.model_name)\n",
    "else:\n",
    "    raise KeyError(\"{} model is unknown.\".format(args.model_name))\n",
    "model = model.cuda()\n",
    "print(\"Model Name: {}\".format(args.model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61a34060-d033-47c9-9e39-7a8e0862022f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1899721/186435780.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(run))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n",
      "Error: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].. Switching to 'macro' average.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Model Output: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "F1 score: 0.498088582481903\n",
      "Precision: 0.5317067479726861\n",
      "Recall: 0.5001350546622082\n",
      "Accuracy: 0.7726728680218516\n",
      "20250402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1899721/186435780.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(run))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n",
      "Error: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].. Switching to 'macro' average.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Model Output: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "F1 score: 0.33845396295084734\n",
      "Precision: 0.3336629865731022\n",
      "Recall: 0.4569929662075991\n",
      "Accuracy: 0.7721354705325711\n",
      "20250403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1899721/186435780.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(run))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n",
      "Error: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].. Switching to 'macro' average.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Model Output: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "F1 score: 0.5000277182275105\n",
      "Precision: 0.4770165625194378\n",
      "Recall: 0.5333730267040432\n",
      "Accuracy: 0.7720589278630305\n",
      "20250404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1899721/186435780.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(run))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n",
      "Error: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].. Switching to 'macro' average.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Model Output: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "F1 score: 0.47861466452768936\n",
      "Precision: 0.444772194676655\n",
      "Recall: 0.5219438319074355\n",
      "Accuracy: 0.7718760648274442\n",
      "20250405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1899721/186435780.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(run))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n",
      "Error: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].. Switching to 'macro' average.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Model Output: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "F1 score: 0.4740271539424119\n",
      "Precision: 0.4392788174180908\n",
      "Recall: 0.5182826732122637\n",
      "Accuracy: 0.7709685445601244\n",
      "20250406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1899721/186435780.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(run))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n",
      "Error: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].. Switching to 'macro' average.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Model Output: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "F1 score: 0.5025045401911653\n",
      "Precision: 0.5247430943560998\n",
      "Recall: 0.5087736631395312\n",
      "Accuracy: 0.7726970499096177\n",
      "20250407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1899721/186435780.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(run))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n",
      "Error: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].. Switching to 'macro' average.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Model Output: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "F1 score: 0.34429172667142294\n",
      "Precision: 0.3373389638977244\n",
      "Recall: 0.5706183680210816\n",
      "Accuracy: 0.7718654727609021\n",
      "20250408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1899721/186435780.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(run))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n",
      "Error: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].. Switching to 'macro' average.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Model Output: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "F1 score: 0.5091697805881644\n",
      "Precision: 0.5167090305044614\n",
      "Recall: 0.5221320321670335\n",
      "Accuracy: 0.7727480117392074\n",
      "20250409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1899721/186435780.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(run))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n",
      "Error: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].. Switching to 'macro' average.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Model Output: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "F1 score: 0.49572706790578475\n",
      "Precision: 0.5313172594169809\n",
      "Recall: 0.49724525499212024\n",
      "Accuracy: 0.7726249039469437\n",
      "20250410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1899721/186435780.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(run))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_label shape: (5003745,)\n",
      "flattened_prediction shape: (5003745,)\n",
      "Error: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].. Switching to 'macro' average.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/jmt30269/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Model Output: ./results/BS_06_conv2d_unmix_p5_97.02_epoch20.pkl\n",
      "F1 score: 0.4782528280138905\n",
      "Precision: 0.41296192276864346\n",
      "Recall: 0.5706479917832067\n",
      "Accuracy: 0.7695542039012779\n"
     ]
    }
   ],
   "source": [
    "# criterion\n",
    "# create model\n",
    "import re\n",
    "for run in filtered_files:\n",
    "    # 정규표현식으로 8자리 숫자 추출\n",
    "    match = re.search(r'202504\\d{2}', run)\n",
    "    if match:\n",
    "        date_str = match.group()\n",
    "        print(date_str)\n",
    "    if args.model_name == 'conv2d_unmix':\n",
    "        model = DSNet(band, num_classes, args.patches, args.model_name)\n",
    "    else:\n",
    "        raise KeyError(\"{} model is unknown.\".format(args.model_name))\n",
    "    model = model.cuda()\n",
    "    run=os.path.join(base_path,run)\n",
    "    from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    # Set the optimizer\n",
    "    if 'unmix' in args.model_name:\n",
    "        params = map(id, model.unmix_decoder.parameters())\n",
    "        ignored_params = list(set(params))\n",
    "        base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n",
    "        optimizer = torch.optim.Adam([{'params': base_params},{'params': model.unmix_decoder.parameters(), 'lr': 3e-4}],\n",
    "                                    lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    \n",
    "    apply_nonegative = NonZeroClipper()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.epoches//10, gamma=args.gamma)\n",
    "    \n",
    "    #-------------------------------------------------------------------------------\n",
    "    model.eval()\n",
    "    model.load_state_dict(torch.load(run))\n",
    "    pre_u,prob = test_epoch(model, label_true_loader)\n",
    "    \n",
    "    prediction_matrix = np.zeros((height, width), dtype=float)\n",
    "    for i in range(total_pos_true.shape[0]):\n",
    "        prediction_matrix[total_pos_true[i,0], total_pos_true[i,1]] = pre_u[i] + 1\n",
    "    # savemat('07_matrix.mat',{'P':prediction_matrix, 'label':label})\n",
    "    \n",
    "    # 예측 및 레이블 데이터 플래트닝 및 타입 변환\n",
    "    flattened_label = label.flatten().astype(int)\n",
    "    flattened_prediction = prediction_matrix.flatten().astype(int)\n",
    "    print(f'flattened_label shape: {flattened_label.shape}')\n",
    "    print(f'flattened_prediction shape: {flattened_prediction.shape}')\n",
    "    data111 = list(zip(flattened_label, prob, flattened_prediction))\n",
    "    # 문자열에서 대괄호와 공백 제거 후 숫자 추출\n",
    "    df=pd.DataFrame(data111,columns=['gt','prob','pred'])\n",
    "    back=[]\n",
    "    akk=[]\n",
    "    for i in df['prob']:\n",
    "        akk.append(i[0])\n",
    "        back.append(i[1])\n",
    "    df['prob_akk']=akk\n",
    "    df['prob_back']=back\n",
    "    # prob 칼럼 삭제 (선택사항)\n",
    "    df = df.drop('prob', axis=1)\n",
    "    df.to_csv(f\"bs_dsnet_{date_str}_result.csv\",index=False)\n",
    "    \n",
    "    # F1 score, precision, and recall calculation\n",
    "    try:\n",
    "        f1 = f1_score(flattened_label, flattened_prediction, average='binary')\n",
    "        precision = precision_score(flattened_label, flattened_prediction, average='binary')\n",
    "        recall = recall_score(flattened_label, flattened_prediction, average='binary')\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}. Switching to 'macro' average.\")\n",
    "        f1 = f1_score(flattened_label, flattened_prediction, average='macro')\n",
    "        precision = precision_score(flattened_label, flattened_prediction, average='macro')\n",
    "        recall = recall_score(flattened_label, flattened_prediction, average='macro')\n",
    "    \n",
    "    print(\"**************************************************\")\n",
    "    print(f'Model Output: {args.inference_model}')\n",
    "    print(f'F1 score: {f1}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Accuracy: {np.sum(flattened_label == flattened_prediction) / flattened_label.size}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
